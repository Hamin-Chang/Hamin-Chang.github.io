---
layout: single
title:  "[IC/개념] 이미지 분류 - WRN 🛣️"
toc: true
toc_sticky: true
categories:
  - cv-imageclassification
---

## WRN 논문 리뷰

이번 글에서는 [**<U>WRN 논문</U>**](https://arxiv.org/pdf/1605.07146.pdf)(Wide Residual Networks)을 리뷰해본다. WRN은 residual network의 넓이를 증가시키고 깊이를 감소시킨 모델이다. 여기서 **신경망의 너비를 증가시킨다는 것은 filter수를 증가시킨다는 의미**이다. 즉, **WRN은 residual block을 구성하는 conv layer의 filter 수를 증가시켜서 신경망의 넓이를 증가**시켰다. 

### 1. 등장배경
지금까지 CNN은 모델의 깊이를 증가시키는 방법으로 발전해왔다. 하지만 모델의 깊이가 깊어질수록 vanishing gradient, gradient explosion과 같은 문제들이 발생했다. 이러한 문제를 해결하기 위해서 등장한 ResNet은 residual block 개념을 제안해서 뛰어난 성능을 낼 수 있었다. 하지만 **ResNet의 핵심 아이디어인 identity mapping을 허용하는 residual block은 학습시에 약점이 되기도** 한다.

Gradient가 네트워크를 통해서 흐르기 때문에 residual block weight를 거치도록 강제할 수 있는 것이 없기 때문에 훈련 중에 어떤 것도 학습하지 않을 수 있으므로 오직 몇 개의 block만 유용한 표현을 학습하거나, 혹은 많은 block들은 최종 목표에 적은 기여를 하는 매우 적은 정보만 공유할 수 있다는 약점이 있다. 이 문제를 **Diminishing feature reuse**라고 칭한다. 본 논문의 저자들은 이 문제를 해결하기 위해 residual block을 무작위로 비활성화하는 방법을 선택했다.

본 논문의 저자들은 network의 depth를 증가시키는 것과 비교했을 때 **ResNet block의 widening이 적절하게 이루어진다면 성능을 향상하는 더욱 효과적인 방법**을 제공하는 것임을 증명한다. 저자들은 이전 연구에 비해 상당히 향상되고 50배 더 적은 layer를 가지고 2배 더 빠른 wider deep residual network를 제시했는데, 이를 wide residual networks라고 부른다. 즉, 더 넓은 residual을 사용함으로써 성능이 향상되는 것을 확인했다.

#### 1.1 Dropout 적용하기

Dropout은 한 때 인기있는 기법이었으나 batch normalization(BN)으로 주로 대체되었다. BN도 Dropout처럼 regularizer의 역할을 수행하고 실험적으로 BN을 가지는 network가 더 나은 accuracy를 가진다는 것이 보였기 때문이다. 

하지만 본 논문에서는 **WRN의 경우 더 넓은 residual block을 사용하기 때문에 parameter의 수가 증가하기 때문에 Dropout을 사용**한다. 이전에 ResNet에서의 dropout은 identity part에 투입되어서 부정적인 효과를 보였기 때문에, **본 논문에서는 dropout을 conv layers 사이에 투입**시킨다고 한다.

WRN에 대한 실험적인 결과는 dropout이 일관성 있게 모델의 성능을 향상시킨다는 것을 보여주며, 심지어 새로운 SOTA result를 만들어내는 것을 보였다고 한다.


#### 1.2 논문의 contribution 요약
따라서 **본 논문의 contribution을 요약**하면
* Residual 구조에 대한 자세한 실험적인 연구를 제시
* ResNet이 상당히 향상된 성능을 낼 수 있도록 하는 widened architecture를 제시
* overfitting을 피하기 위해 deep residual network 내에서 dropout을 활용하는 새로운 방법 제시
* 새롭게 제안된 ResNet architecture가 여러가지 dataset에서 향상된 accuracy와 속도를 보여주며 SOTA result 달성


### 2. Wide Residual Networks

Identity mapping을 가지는 기본적인 residual block은 다음 식으로 표현된다. 

![3](https://user-images.githubusercontent.com/77332628/236479512-945cbdf4-260d-4b4e-9b60-e4489d888c79.png)

위 식에서 $x_{l+1}$과 $x_l$은 $l$번째 unit의 input과 output을 의미하고, $F$는 residual function이고 $W_l$은 block의 parameter를 나타낸다.

Residual network는 연속적으로 쌓인 residual block으로 구성된다. 

<img width="996" alt="4" src="https://user-images.githubusercontent.com/77332628/236479517-a626feb0-136b-47fa-b1d1-ebf906416cd4.png">

Original Residual block은 다음의 2종류이다. 

(위 이미지에서 (a),(b) 구조)
* Basic : 연속된 3x3 conv로 구성되며, batch normalization과 ReLU가 앞선 conv에 적용되는 구조
* Bottleneck : [$conv$ 1x1] - [$conv$ 3x3] - [$conv$ 1x1] 구조

Original architecture과 비교해서 residual block에서의 batch normalization, activation과 convolution의 순서는 conv-BN-ReLu에서 BN-ReLU-conv로 변경한다. 순서를 바꾼 것이 더 빠르고 나은 성능을 보이기 때문에 본 논문에서는 original architecture는 고려하지 않는다.

또한 Bottleneck block은 layer 수를 늘리기 위해서 block의 연산량을 감소시키고자 사용되었는데, 본 논문에서는 widening의 효과를 연구하기 위해서 network를 얇게 만드는 bottleneck 구조는 고려하지 않는다.

Residual block의 representational power를 증가시키는 근본적인 간단한 방법은 다음과 같다.

* Block당 더 많은 conv layer 추가
* 더 많은 feature planes를 추가해서 conv layer를 넓히기
* Conv layer의 filter size를 증가

여러 연구들에서 작은 filter를 쓰는 것이 좋다는 것이 확인되었기 때문에 3x3 보다 더 큰 filter는 고려하지 않는다.


WRN에서는 기존 ResNet에 2개의 추가적인 factor가 존재한다.

* block deepening factor $l$ : block에 포함된 conv 개수
* widening factor $k$ : conv layer 내에서 feature 수의 배수
예를 들어서 $l=2, k=1$은 basic block을 의미한다.

본 논문에서 사용하는 WRN의 구조는 다음과 같다.

![5](https://user-images.githubusercontent.com/77332628/236479525-1e7f1c87-8efb-41d0-bd48-c01b2fa35f97.png)

처음의 convolutional layer conv1 다음에 3개의 group의 residual blocks conv2, conv3, conv4 다음에 average pooling, final classification layer가 뒤따르는 구조다. B(3,3)은 conv layer의 kernel size를 의미하고, 기존 conv layer의 필터 수에 k배를 해주고, N이 WRN의 깊이를 결정한다.(N은 몇개의 residual block이 묶여있는지를 의미한다.)

본 논문의 저자들은 residual block의 representational power의 효과를 연구하길 기대하고, 이를 위해 기본 architecture에 몇가지 수정을 수행하고 실험을 진행한다.

### 3. WRN Experiments

#### 3.1 Types of convolutions in a block

$B(M)$은 residual block structure를 나타내며, $M$은 block 내부에 있는 convolutional laers의 kernel size list를 나타낸다. 예를 들어, $B(3,1)$은 3x3 conv layer와 1x1 conv layer를 가지고 있는 residual block인 것이다.

본 논문에서는 bottleneck block 구조를 고려하지 않으므로 block마다 feature planes의 수는 항상 똑같이 유지된다.

다음과 같은 조합들의 residual block structure로 실험을 수행했는데, 이는 저자들이 basic residual architecture에서 3x3 conv layer 각각이 얼마나 중요한지에 알고 싶었고, 이것이 1x1 layer나 1x1과 3x3 convolutional layer의 조합으로 대체될 수 있는지 궁금했기 때문이다.

![6](https://user-images.githubusercontent.com/77332628/236479531-2431e6dd-c414-41a0-9a0c-93ae081ee744.png)

참고로 논문 내에서 WRN의 표기 방식은 다음과 같다.

* WRN - 28 - 2 - $B(3,3)$
=> 28 depth, k=2, 3x3 conv 2개

다른 block types $B$를 가지고 실험한 결과는 다음과 같다.

WRN-40-2를 사용해서 $B(1,3,1), B(3,1), B(1,3), B(3,1,1)$ block types를 실험했다.

파라미터 수를 유지하기 위해 $B(3,3), B(3,1,3)$은 더 얕은 depth를 자기는 네트워크를 사용했다.

![7](https://user-images.githubusercontent.com/77332628/236479533-d701c95a-4186-495b-8f3b-247ce4706cb5.png)

성능은 다들 비슷하다 그 중에서도 $B(3,3)$이 간소한 차이로 가장 좋은 성능을 보였기 때문에 저자들은 이후 3x3 conv를 가지는 WRN만 사용한다.



논문의 저자들은 block deepening factor $l$이 성능에 어떻게 영향을 미치는지 보기 위해 실험을 진행했다.

### 3.2 Block Deepening factor $l$

deepening factor $l$(block당 conv layer 개수)를 달리해서 최적은 $l$을 찾기 위한 실험도 진행됐다.

WRN-40-2 with 3x3 convolution을 사용해서 실험을 진행한 결과는 다음과 같다.

![8](https://user-images.githubusercontent.com/77332628/236479537-3c2d3d6e-4805-4a18-8674-20665de8b973.png)

이를 통해서 $B(3,3)$이 $B(3), B(3,3,3), B(3,3,3,3)$보다 좋은 성능을 내기 때문에 $B(3,3)$이 최적의 block당 convolution 수라는 것이다. 따라서 남은 실험에서 저자들은 $B(3,3)$을 가진 WRN만을 실험에 사용한다.



### 3.3 Widening factor $k$

파라미터 수는 $l$과 $d$(ResNet block 수)에 선형적으로 증가하지만, 파라미터 수와 computational complexity는 $k$에 대해서는 2차로 증가한다. 하지만 GPU가 large tensor의 병렬 계산에서 훨씬 더 효율적이기 때문에 수천개의 small kernel을 가지는 것보다 layer를 넓히는 것이 계산적으로 더 효과적이기 때문에 저자들은 최적읜 $d$ to $k$ ratio에 대해 관심을 가졌다. 

$k$를 증가시키려면 layer의 총 개수 $d$는 감소한다. 최적의 비율을 찾기 위해서 저자들은 $k$를 2부터 12까지, depth는 16부터 40까지 실험했다. 결과는 다음과 같다.

![9](https://user-images.githubusercontent.com/77332628/236479542-9664a165-3ff4-40cf-8882-9f851c4200d2.png)

위 표에서 볼 수 있듯이, 동일한 depth에서는 $k$가 클수록 성능이 우수했고, 동일한 $k$에서는 depth가 클수록 성능이 좋았다. 이제 다른 모델들과 성능을 비교해보자. WRN 40-4와 thin ResNet-1001를 비교하면 WRN이 CIFAR-10과 CIFAR-100에서 모두 더 뛰어난 성능을 보인다. 또한 WRN-28-10은 thin ResNet-1001보다 36배 더 적은 layer를 가지고도 CIFAR-10과 CIFAR-100에서 훨씬 뛰어난 성능을 보였다.

다음은 WRN-28-10과 ResNet-164를 비교한 그래프인데, 파라미터가 더 많은 WRN-28-10이 ResNet-164보다 더 학습이 잘 된것을 볼 수 있다.

<img width="1000" alt="10" src="https://user-images.githubusercontent.com/77332628/236479547-397e4bb6-9a14-41c5-abb4-83dc391f76d8.png">


지금까지의 실험 내용을 요약하자면,
* Widening은 다른 depth를 가지는 residual 사이에서 일관적으로 성능을 향상시킨다.
* Depth와 width를 모두 증가시키는 것은 parameter가 너무 많아지거나, 더 강력한 regularization이 요구될 때까지 도움이 된다.
* wide network는 thin network보다 2배 혹은 그 이상의 parameter를 성공적으로 학습할 수 있다.



### 3.4 Dropout in residual blocks

layer를 widening하는 것이 parameter 수를 증가시키기 때문에 논문의 저자들을 regularization의 방법을 연구했다. Residual network에는 regularization을 하는 batch normalization이 있지만, 이는 heavy data augmentation을 요구하기 때문에 저자들은 이를 지양했다. 따라서 다음 이미지의 (d)처럼 dropout layer를 residual block 안에 있는 convolution 사이에 추가했다.

![11](https://user-images.githubusercontent.com/77332628/236479550-18902c9a-c141-4596-90e1-b7d40aeb6586.png)

다음 표에서 볼 수 있듯이 대부분의 실험에서 dropout을 적용한 것이 안했을 때보다 성능이 향상 된 것을 볼 수 있다.


![12](https://user-images.githubusercontent.com/77332628/236479553-8e453856-2b55-4635-89a0-4bdeb706d695.png)

### 3.5 Computational efficiency

Widening은 효과적으로 computation의 균형을 맞추는데에 더욱 최적의 방식으로 도움을 주기 때문에 wide network가 thin network보다 훨씬 효율적이다.

cudnn v5와 Titan X를 사용해서 여러가지 모델에 대해 forward + backward update time을 측정했으며 결과는 다음과 같다.

![13](https://user-images.githubusercontent.com/77332628/236479555-63ae7e72-bdfe-4213-b202-cc911d5555d9.png)

CIFAR에서 최고의 성능을 낸 WRN-28-10가 thin ResNet-1001보다 1.6배 더 빠르고, wide WRN-40-4는 ResNet-1001와 거의 유사한 accuracy를 가지지만, 8배 더 빠르다.

Conclusion

본 논문에서는 residual network에서 conv layer를 넓혀서 모델의 성능을 향상시키는 방법을 제시했다. 성능이 향상되었다는 점에서도 의미가 있지만 나는 많은 parameter를 안정적으로 학습할 수 있다는 점에서 인상 깊었다.

출처 및 참고문헌:

1. https://arxiv.org/pdf/1605.07146.pdf
2. https://norman3.github.io/papers/docs/wide_resnet.html
3. https://deep-learning-study.tistory.com/519
4. https://cumulu-s.tistory.com/35
