---
title : '[DL/CV] 객체 탐지 - YOLO v1 🤟'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## YOLO (you only look once) v1 논문 읽어보기 

### 0. YOLO v1 개요
이번 글에서는 YOLO v1 논문(https://arxiv.org/pdf/1506.02640.pdf)을 리뷰해보도록 하겠다. 이전 글들에서 다룬 R-CNN 등의 2-stage detector는 localization과 classification을 수행하는 network가 분리되어 있었다. 이는 각 task가 순차적으로 수행되는 것이기 때문에, detection 속도가 느려지게 된다. 하지만 1-stage detector는 하나의 통합된 network가 두 task를 동시에 수행한다. YOLO v1은 대표적인 1-stage detector로, real-time에 가까운 detection 속도를 가진다.

이미지1

YOLO v1은 localization과 classification을 동시에 수행하기 위해서 이미지를 지정한 grid로 나누고, 각 grid cell이 한번에 bounding box와 class 정보라는 2가지 정답을 도출하도록 만들었다. 또한 각 grid cell에서 얻은 정보를 feature map이 잘 encode 할 수 있도록 독자적인 DarkNet CNN을 사용하고, 이를 통해서 얻은 feature map을 활용해서 자체적으로 정의한 손실함수를 통해서 전체 모델을 학습시킨다.

### 1. 1-stage detector

YOLO v1은 별도의 region proposals를 사용하지 않고 전체 이미지를 입력으로 사용한다. 먼저 전체 이미지를 SxS 크기의 grid로 나눠준다. 여기서 객체의 중심이 특정 grid cell에 위치하면, 해당 특정 gird cell은 그 객체를 detect하도록 **할당(responsible for)**된다.

이미지2

위 이미지처럼 4행 3열의 grid cell이 왼쪽 개를 예측하도록 할당되었고, 4행 4열의 grid cell이 오른쪽 개를 예측하도록 할당되었다. **이는 나머지 grid cell은 객체를 예측하는데 참여할 수 없음을 의미한다.**

각각의 grid cell은 **B**개의 bounding box와 해당 bounding box에 대한 **confidence score**를 예측한다. confidence score는 해당 bounding box에 객체가 포함되어 있는지 여부와, box가 얼마나 ground truth box를 예측했는지를 나타내는 수치다. confidence score는 $Pr(Object) * IoU(truthpred)$로 정의하는데, grid cell 내에 객체가 존재하지 않으면 $Pr(Object)$가 0이 돼서 confidence score은 0이 되고, 객체가 존재하면 $Pr(Object)$가 1이 되어 confidence score는 IoU와 값이 같아진다.

이미지3

위의 이미지처럼 각각의 bounding box는 box의 좌표 정보(x,y,w,h)와 confidence score라는 5개의 예측값을 가진다. 여기서 (x,y)는 grid cell의 경계에 비례한 box의 중심 좌표를 나타내고, 높이와 너비는 box의 크기를 grid cell에 비례해서 나타낸 값이다. x,y는 grid cell내에 위치하기 때문에 0 ~ 1 사이의 값만 가지지만 w,h는 1 이상의 값을 가질 수 있다. 

**하나의 bounding box는 하나의 객체만을 예측하며, 하나의 grid cell은 하나의 bounding box를 학습에 사용한다.** 예를 들어 grid cell별로 B개의 bounding box를 예측한다고 하면, confidence score가 가장 높은 1개의 bounding box만 학습에 사용한다.

이미지4

각 grid cell은 C개의 conditional class probabilities인 $Pr(Class i|Object)$를 예측한다. 이는 특정 grid cell에 객체가 존재하고 가정했을 때, 특정 class i일 확률인 조건부 확률값이다. bounding box 수와 상관없이 하나의 grid cell마다 하나의 조건부 확률을 예측한다. 주의할 점은 bounding box별로 class probabilities를 예측하는 것이 아니라 grid cell 별로 예측한다는 것이다.

이미지5

참고로 논문에서는 S=7, B=2, C=20으로 설정했다. 논문에서는 PASCAL VOC 데이터셋을 사용해서 학습했기 때문에 class수가 20개여서 C=20으로 설정했다. 즉, 이미지를 7x7 grid로 나누고 각 grid cell은 2개의 bounding box와 해당 box의 confidence score, 그리고 C개의 class probabilities를 예측한다. 이미지별 예측값의 크기는 7x7x(2x5+20)이다. 이와 같은 과정을 통해 bounding box의 위치와 크기, 그리고 class에 대한 정보를 동시에 예측할 수 있다.

위의 이미지를 보면 각각의 grid cell은 1x30의 벡터인 것을 알 수 있다. 

### 2. DarkNet
YOLO v1은 최종 예측값의 크기인 7x7x30에 맞는 feature map을 생성하기 위해 DarkNet이라는 독자적인 CNN을 설계했다. network의 전체적인 구조는 다음과 같다.

이미지6

DarkNet은 이미지 분류를 위한 GooLeNet 구조에 영감을 받아 구조를 설계했다. 기존의GoogLeNet은 Inception module을 사용한 반면에 YOLO에서는 Inception module을 일자로 이어둔 모델을 사용했다. 이후 모델이 detection task를 수행할 수 있도록 4개의 conv layer와 2개의 fc layer를 추가했다. 또한 이미지 분류를 위해 학습 시켰을 때는 224x224 크기의 이미지를 사용한 반면, detection task를 위한 학습을 위해서는 이미지의 크기를 키워 448x448 크기의 이미지를 사용한다. 논문에서는 이를 detection task는 결이 고운 시각 정보를 필요로 하기 때문이라고 설명한다.



### 3. Loss function

이미지7

기존 R-CNN 계열의 모델이 classification과 localizaion task에 맞게 서로 다른 손실 함수를 사용했던 것과 달리 YOLO v1은 regression에 주로 사용되는 **SSE(Sum of Squared Error)**를 사용한다. 위 이미지에서 볼 수 있듯이 Localization loss, Confidence loss, Classification loss의 합으로 구성되어 있다.

**Localization Loss**

이미지8

* $λ_{coord}$ : 대부분의 grid cell은 객체를 포함하지 않아서 confidence score가 0이 되어 객체를 포함하는 grid cell의 gradient를 압도해서 모델이 불안정해질 수 있기 때문에 $λ_{coord}$는 객체를 포함하는 grid cell에 가중치를 두는 역할을 한다. 논문에서는 $λ_{coord} = 5$로 설정했다.
* $S^2$ : grid cell의 개수 (논문에서는 7x7 = 49)
* B : grid cell별 bounding box (=2)
* $1^{obj}_{i,j}$ : $i$번째 grid cell의 $j$번째 bounding box가 객체를 예측하도록 할당 되었을 때는 1, 그렇지 않을 경우 0인 index parameter이다. 앞서 설명했듯이 grid cell에서는 B개의 bounding box를 예측하지만 그 confidence score가 높은 오직 1개의 bounding box만을 학습에 사용한다.
* $x_i,y_i,w_i,h_i$ : ground truth box의 x,y 좌표와 w,h. 특징은 크기가 큰 bounding box의 오류가 크기가 작은 bounding box의 오류보다 덜 중요하다는 것을 반영하기 위해 w,h 값에 루트를 씌운다는 것이다.
* $\hat{x}_i,\hat{y}_i,\hat{w}_i,\hat{h}_i$ : 예측 bounding box의 x,y 좌표와 w,h.

**Confidence Loss**

이미지9

* $λ_{noobj}$ : 위에서 언급한 객체를 포함하지 않는 grid cell에 곱해주는 가중치 파라미터이다. 논문에서는 0.5로 설정해서 $λ_{coord} = 5$로 설정한 값보다 상당히 작게 해서 객체를 포함하지 않은 grid cell의 영향력을 줄였다.
* $1^{noobj}_{i,j}$ : 위에서와는 반대로 $i$번째 grid cell의 $j$번째 bounding box가 객체를 예측하도록 할당(responsible)받지 않았을 때 1, 그렇지 않을 경우 0인 index parameter이다.
* $C_i$ : 객체가 포함되어 있을 경우 1, 아닐 경우 0.
* $\hat{C}_i$ : 예측한 bounding box의 confidence score.

**Classification Loss**

이미지10

* $p_i(c)$ : 실제 class probabilities
* $\hat{p}_i(c)$ : 예측 class probabilities


### 4. YOLO v1 훈련하기

이미지11

YOLO v1의 학습과정은 위 이미지처럼 굉장히 단순하다. DarkNet에 이미지를 입력해서 7x7x30 크기의 feature map을 정의한 손실함수를 이용해서 학습시킨다.

논문에서는 다음과 같은 방법으로 모델을 학습시켰다.

* Epoch = 135, batch_size = 64, momentum = 0.9, decay = 0.0005
* learning_rate scheduling : 첫 epoch에서 $10^{-3}$으로 시작해서 75번째 epoch까지 $10^{-2}$으로 학습시킨 후, 이후 30 epochs 동안 $10^{-3}$으로 학습하고 마지막 30 epochs 동안 $10^{-4}$로 학습시킨다.
* 과적합을 막기 위해 dropout과 data augmentation을 활용한다.

### 5. 추론하기
학습을 마친 YOLO 모델은 PASCAL VOC의 이미지에 대해 각각 98개의 bounding box를 출력하고 이렇게 나온 98개의 bounding boxes에 대해 NMS(Non-Maximum Supression)을 적용한다.

### 6. YOLO v1의 장단점
* YOLO v1 모델은 base network의 경우 45fps, 경량화한 fast version의 network는 150fps의 결과를 보여 매우 빠른 detection 속도를 보인다. 실시간으로 0.0025 이하의 지연시간(latency)를 가지며 객체를 detect하는 것이 가능하다고 한다.
*  sliding window 방식이나 region proposal 기반의 모델과는 달리 YOLO v1 모델은 전체 이미지를 인지하여 맥락 정보(contextual information)을 학습한다고 하는데, 이를 통해 배경 영역을 객체로 인식하는 False Positive 오류를 Fast R-CNN 모델보다 상대적으로 덜 범한다고 한다.
*  일반화 가능한 표현(representations)를 학습하여 새로운 도메인이나 예상치 못한 입력 이미지에 대해 상대적으로 강건한 모습을 보인다.
* SOTA 모델보다 정확도 측면에서 약간은 부족한 성능을 보이기도 하고, 특히 작은 객체를 제대로 탐지하지 못하는 단점이 있다.
* 또한 각 grid cell의 bounding box는 하나의 객체만을 예측하기 때문에 같은 grid cell 내에 있는 여러 객체를 탐지하지 못한다.

참고 자료:

개인 블로그1 (https://velog.io/@skhim520/YOLO-v1-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EB%B0%8F-%EC%BD%94%EB%93%9C-%EA%B5%AC%ED%98%84)

개인블로그2 (https://herbwood.tistory.com/13)

YOLO v1 논문 (https://arxiv.org/pdf/1506.02640.pdf)
