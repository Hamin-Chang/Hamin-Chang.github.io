---
toc : true
title : '[IC/개념] 이미지 분류 - ResNet ⏭️'
layout : single
toc: true
toc_sticky: true
categories:
  - cv-imageclassification
---

## ResNet 논문 리뷰

이번 글에서는 [**<U>ResNet 논문</U>**](https://arxiv.org/pdf/1512.03385.pdf)(Deep Residual Learning for Image Recognition)을 리뷰해보도록 하겠다. 

### 0. Introduction

역대 ILSVRC 대회 결과를 보면, 모델의 depth가 모델의 성능에 큰 영향을 준다는 것을 알 수 있다. 즉, **visual recognition task에서 depth는 굉장히 중요한 요소**임을 알 수 있다. 하지만 모델의 깊이가 깊어짐에 따라 필연적으로 overfitting, gradient vanishing, 연산량 등의 문제들이 발생한다. 이러한 이유로 심층 신경망을 안정적으로 학습시키기 상당히 까다로운데, 이에 논문에서 마이크로소프트 팀은 **residual learning framework**를 이용해서 이전보다 훨씬 깊은 네트워크를 더 쉽게 학습시킬 수 있었다고 한다.





### 1. Deep Residual learning

다음 이미지의 기존 네트워크는 입력 $x$를 받고 layer를 거쳐 $H(x)$를 출력하는데, 이는 입력값 $x$를 타겟값 $y$로 mapping하는 함수 $H(x)$를 얻는 것이 목적이다. 

![0](https://user-images.githubusercontent.com/77332628/233089589-e5db9b43-a833-47d4-90cf-b515b2c8457d.png)

하지만 다음 이미지의 Residual learning은 $H(x)$가 아니라 **Residual Function인 $F(x)=H(x)-x$를 얻도록 목표를 수정**한다.

![1](https://user-images.githubusercontent.com/77332628/233089598-389f57dd-aa4e-4230-b334-8421ffa18055.png)

Residual Function $F(x)$는 출력과 입력의 차이로, $F(x)$를 최소화시키는 것으로 목적을 바꾼다. 즉, **입력과 출력의 차이를 줄이는 것을 목표**로 한다는 것이다. 즉, $H(x)$를 x로 mapping하는 것이 학습의 목표가 된다. 결과적으로 $H(x)=F(x)+x$이기 때문에 단순히 입력에서 출력으로 바로 연결되는 **shortcut connection(skip connection)만 추가**하면 된다.

위 이미지에서 **shortcut mapping하는 $h$를 identitiy**라고 하고 **ReLU를 통과하는 $f$를 after-adding mapping**이라는 이름을 붙인다. 직전 layer까지 학습된 $x_l$의 출력값을 $x_{l+1}$이라고 했을 때,  $x_{l+1}=f(h(x_l)+F(x_l))$이 된다. 따라서 **$F(x)+x$는 residual block에 속하는 layer들에 대한 학습 결과와 그 전까지 학습된 결과를 더해준 값**이다. 

residual block의 장점은 한 block의 입력과 같은 $x$가 그대로 출력에 연결되는 것이기 때문에 **파라미터 수에 변화가 없으며**, 덧셈이 늘어나는 것을 빼면 shortct connection을 통한 **연산량 증가 또한 없다**. 또한 다음 이미지처럼 layer의 **연산이 곱셈에서 덧셈으로 변형**되어 몇개의 layer를 건너뛰는 효과가 있어서 이 덕에 f**orward와 backward path가 단순해지는 효과**가 있었으며, **gradient vanishing 문제를 해결**할 수 있다고 한다.

![2](https://user-images.githubusercontent.com/77332628/233089605-39cf5cee-51d4-45f4-b116-74618f1dc942.png)


위에서 언급한 $H(x) = F(x)+x$ 식에서 **$x$와 $F(x)$을 더할 때 Element-wise addition**을 사용하는데 이는 이 둘의 차원이 같을 때 가능하다. 만약 $x$와 $F(x)$의 **차원이 다르다면 다음 식과 같이 linear projection($W_s$)을 추가해서 차원을 일치**시킨다.

![3](https://user-images.githubusercontent.com/77332628/233089611-b87594c0-b164-43c2-a13c-30492702df66.png)



### 2. Architectures

#### 2.1 Plain Network architecture
Baseline 모델로 사용한 plain net은 VGGNet에서 영감 받았다고 한다. 즉, conv filter 사이즈가 3x3이고, 다음 2가지 규칙에 기반해 설계했다고 한다.

1. Output feature map의 size가 같은 layer들은 모두 같은 수의 conv filter를 사용한다.
2. Output feature map의 size가 반으로 줄어들면 time complexity를 동일하게 유지하기 위해 필터 수를 2배로 늘려준다.

추갖거으로, downsampling을 수행한다면 pooling대신 stride=2인 conv filter를 사용하고 모델 끝단에는 GAP를 사용하고, 사이즈가 1,000인 FC layer와 Softmax를 사용한다. 다음 이미지처럼 전체 layer 수는 34인데 VGGNet과 비교해서 더 적은 필터와 복잡성을 가진다.

![4](https://user-images.githubusercontent.com/77332628/233089614-37d452a1-acde-49d3-ae07-8dea93b2193c.png)

#### 2.2 ResNet architecture

ResNet 모델은 plain net에 기반해 shortcut connection(skip connection)을 추가하여 구성한다. 이때, 위에서 언급한 것처럼, input과 output의 차원이 같다면 identitiy shortcut을 바로 사용하고, 차원이 서로 다르다면 다음의 두가지 선택권이 있다.

1. zero padding을 적용해 차원을 키워준다.
2. 앞서 언급한 1x1 conv를 이용한 projection shortcut을 사용한다.

![5](https://user-images.githubusercontent.com/77332628/233089624-80ecaa1d-ef28-4748-9747-b9a7b19be1dd.png)

이때, shortuct이 feature map을 2 size씩 건너뛰기 때문에 stride=2로 설정한다.

다음 이미지의 윗 부분이 ResNet의 구조이다.

![6](https://user-images.githubusercontent.com/77332628/233089626-9ce1a08e-dcc8-4093-87bd-d7e9b838a73e.png)

#### 2.3. ResNet Implementation
 
논문에서 ResNet 구현은 다음 조건들로 진행했다.

1. 이미지의 짧은 쪽이 [256,480] 사이가 되도록 random하게 resize
2. horizontal flip 부분적으로 적용 & per-pixel mean을 빼줌
3. 224x224 size로 random하게 crop
4. standard color augmentation 적용
5. conv와 activation function 사이에 Batch normalization 적용
6. He 초기화 방법으로 가중치 초기화
7. Optimizer : SGD (mini-batch size:256, momentum:0.9, Weight decay : $10^{-4}$)
8. Learning rate : 0.1에서 시작하고 학습이 정체될 때 $10^{-1}$씩 곱해줌

9. $60 * 10^4$ 반복(iteration) 학습
10. dropout 미사용
11. test 시에는 10-cross validation 적용, multiple scale을 적용해 짧은 쪽이 {224, 256, 384, 480, 640} 중 하나가 되도록 resize 한 후, 평균 score을 산출

### 3. Experiments

ImagNet Classification

이제 plain net과 ResNet을 대상으로 ImageNet 데이터셋을 이용해서 수행한 실험의 결과와 그 특징에 대해 다룬다. ImageNet을 대상으로 한 layer 깊이에 따른 모델 구조는 다음 표와 같다.

![7](https://user-images.githubusercontent.com/77332628/233089633-5c4f2d6d-e97c-4f1a-bfdc-52b60bec537d.png)

#### 3.1 18 layers vs 34 layers

먼저 plain 모델로 실험한 결과를 살펴본다. 다음 이미지에서와 같이 1**8 layer의 얕은 plain 모델에 비해 34 layer의 더 깊은 plain 모델에서 더 높은 Validation error**가 나타났다고 한다. 논문에서는 34 layer plain net에서 degradation 문제가 있다고 판단했다.

![8](https://user-images.githubusercontent.com/77332628/233089641-84c4570c-6f09-4ae4-b729-c0c9a3590300.png)

이러한 최적화 문제는 **Vanishing gradient 때문인 것은 아니**라고 판단했는데, plain net은 Batch Normalization이 적용되어 순전파 과정에서 모든 신호의 variance는 0이 아니며, 역전파 과정에서 gradient 또한 healthy norm을 보였기 때문이다. 따라서 순전파, 역전파 신호 모두 사라지지 않았기 때문에 **deep plain model은 exponentially low convergence rate를 가지기 때문에 training error가 덜 감소**했을 것이라고 마이크로소프트 팀은 추측했다.

다음으로 18 layer과 34 layer ResNet 모델을 비교해본다. 이때 모든 shortcut connection은 identitiy mapping을 사용하고, 입력과 출력의 차원을 맞출 때는 zero padding을 사용해서 파라미터 수의 증가는 없다. 실험 결과 다음 3가지 특징을 알 수 있다.

1) plain net에서와 다르게 34 layer가 18 layer보다 약 2.8% 우수한 성능을 보였다. 특히 34 layer에서 낮은 training error와 높은 validation 성능을 보였다. 이는 d**egradation 문제가 잘 해결**되고, **plain net과 달리 모델이 깊어지더라도 좋은 성능**을 얻을 수 있음을 의미한다.

![9](https://user-images.githubusercontent.com/77332628/233089643-995b512b-8284-4e56-98c4-3eed29ebac17.png)

2) 34 layer일 때 ResNet의 top-1 error는 plain net보다 약 3.5% 줄었고, 이는 **residual learning이 extremely deep system에서 매우 효과적**임을 알 수 있다.

![10](https://user-images.githubusercontent.com/77332628/233089646-f94b91b1-320c-4a62-b90c-0e85cd050b42.png)

3) 위 표에서 18 layer일 때는 ResNet과 plain net의 성능이 비슷했는데, ResNet이 더 빨리 수렴했다고 한다. 이는 **과도하게 깊지 않은 경우, 같은 상황에서 ResNet이 더 빨리 수렴**한다는 것을 알 수 있다.



#### 3.2 Identity vs Projection Shortcuts

shortcut conncection의 3가지 옵션에 대해 비교한다.

A. **zero-padding shortcut**을 (dimension matching)에 사용한 경우 : 모든 shortcut은 **parameter-free**하다.

B. **projection shorcut을 (dimension을 키워줄 때만)** 사용한 경우, 다른 모든 shortcut은 identitiy하다.

C. **모든 shortcut으로 projection shortcut**을 사용한 경우

A 방식은 zero-padded 차원이 residual learning을 수행하지 않기 때문에 B보다 낮은 성능을 보였고, C 방식에서 projection shortcut에 의해 파라미터가 추가되었기 때문에 B보다 좋은 성능을 보였다. **A < B < C의 성능**을 보였다.

하지만 세 방식의 성능차가 미미했기 때문에 **projection shortcut이 degradation을 해결하는데 필수적이지는 않다**는 것을 확인할 수 있다. 논문에서는 memory / time complexity와 모델의 사이즈를 줄이기 위해 C 옵션을 사용하지 않는다. 특히 **Identity shortcut은 bottleneck 구조의 복잡성을 높이지 않는 데에 매우 중요**하기 때문이다.

#### 3.3 Deeper Bottleneck Architecture

ImageNet에 대해 학습을 진행할 때 **training time을 줄이기 위해**서 다음 이미지의 왼쪽의 기존 building block을 오른쪽의** bottleneck design으로 수정**했다고 한다. 각 Residual Function마다 2개의 layer대신 3개의 layer를 사용했고, 이는 각각 1x1, 3x3, 1x1 conv로 이루어져있다. 1x1 conv는 dimension을 줄였다가 늘리는 역할을 한다. 이는 **3x3 conv layer의 input과 output dimension을 작게 만들기 위해**서이다. 

![11](https://user-images.githubusercontent.com/77332628/233089651-f4a1069c-8dfc-453d-9247-95766185a418.png)

여기서 **identity shortcut은 bottleneck 구조에서 특히 중요**하다. **identitiy shortcut이 projection shortcut으로 바뀌면 time complexity와 모델의 size는 두배**가 된다. 따라서 parameter free한 identitiy shortuct은 효율적인 bottleneck 구조를 위해 꼭 필요하다.

#### 3.4 Deeper ResNets

![7](https://user-images.githubusercontent.com/77332628/233089633-5c4f2d6d-e97c-4f1a-bfdc-52b60bec537d.png)

1) 50-layer ResNet

**34-layer ResNet의 2-layer block을 3-layer bottleneck block으로 대체해서 50-layer ResNet을 구성**한다. 이때, **dimension matching은 위의 B 옵션**을 사용한다.

2) 101- and 152-layer ResNets

더 많은 3-layer block을 사용해서 101-과 152-layer ResNet을 구성한다. depth가 상당히 증가했음에도 **VGG-16/19 모델보다 더 낮은 model complexity**를 가지며, 다음 표와 같이 **degradation 문제없이 상당히 높은 정확도**를 보인다.

![12](https://user-images.githubusercontent.com/77332628/233089657-4bd0d972-f898-47b3-bc21-82c94b213d0b.png)

3) Comparsion with SOTA methods

다음 표와 같이 ResNet의 single 모델(앙상블 적용 x)은 앙상블이 적용된 이전의 다른 모델을 능가했고, 앙상블을 적용했을 경우 top-5 error 3.57%를 달성했다.

![13](https://user-images.githubusercontent.com/77332628/233089661-27a8b7bf-2e05-4f1c-a10b-57dc0958c4e6.png)

4) Exploring over 1000 layers

모델이 깊어질수록 좋은 성능을 내는 걸까? 이러한 의문을 품고 논문의 저자들도 1202 layers를 가진 모델을 실험했다. **Optimization과정에는 문제가 없었다**. 아래 그래프처럼 두 모델의 training error는 비슷했지만, **1202 resnet이 110 resnet보다 성능이 좋진 않은데, 이는 Overfitting 때문이**라고 논문에서는 얘기한다.

![14](https://user-images.githubusercontent.com/77332628/233089666-e85a2a94-b3c0-4d39-9d31-39099b982c68.png)

결론적으로 ResNet은 모델이 깊어지면서 생기는 문제들을 해결했다는 것에 큰 의미가 있다. ResNet은 기본중의 기본적인 모델이라고 한다. 이제 이를 기반으로 더 많은 모델들을 탐구해보려한다.

출처 및 참고 문헌 :

1. https://arxiv.org/pdf/1512.03385.pdf
2. https://phil-baek.tistory.com/entry/ResNet-Deep-Residual-Learning-for-Image-Recognition-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0
3. https://deep-math.tistory.com/18
4. https://velog.io/@cha-suyeon/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Deep-Residual-Learning-for-Image-Recognition-ResNet

