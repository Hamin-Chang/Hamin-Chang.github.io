---
title : '[IC/Pytorch] 파이토치로 WRN 구현하기 🛣️' 
layout: single
toc: true
toc_sticky: true
categories:
  - ICCode
---

## Pytorch로 WRN 구현하기

이번 글에서는 실제 파이토치 코드로 WRN 모델을 구현해본다. WRN 모델에 대한 설명은 [**<U>WRN 논문 리뷰</U>**](https://hamin-chang.github.io/)를 참고하길 바란다. 이번 글에서 사용하는 코드는 [**<U>weiaicunzai의 repository</U>**](https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/wideresidual.py)의 코드를 사용했다.



먼저 residual unit을 구현한다.

WRN에서 사용하는 residual unit은 다음과 같다.

![1](https://user-images.githubusercontent.com/77332628/236633668-f9d77970-d8fc-45cb-8c07-f30e7ff04703.png)


```python
import torch
import torch.nn as nn

class WiderBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super().__init__()
        
        self.residual = nn.Sequential(nn.BatchNorm2d(in_channels),
                                     nn.ReLU(),
                                     nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
                                     nn.BatchNorm2d(out_channels),
                                     nn.ReLU(),
                                     nn.Dropout(),
                                     nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1))
        
        self.shortcut = nn.Sequential()
    
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=stride, padding=0)
    
    def forward(self, x):
        x_shortcut = self.shortcut(x)
        x_res = self.residual(x)
        return x_shortcut + x_res
```

이제 residual block을 이용해서 전체 모델을 구축하는데, depth=40, $k$=10으로 설정한다.

전체적인 구조는 다음 표와 같다.

![2](https://user-images.githubusercontent.com/77332628/236633669-22b3a8b4-158d-42bd-b8ff-e4a5bbb365ab.png)



```python
class WRN(nn.Module):
    def __init__(self, depth=40, k=10, num_classes=10, init_weights = True):
        super().__init__()
        N = int((depth-4)/6)
        self.in_channels = 16
        
        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)
        self.conv2 = self._make_layer(16*k, N, 1)
        self.conv3 = self._make_layer(32*k, N, 2)
        self.conv4 = self._make_layer(64*k, N, 2)
        self.bn = nn.BatchNorm2d(64*k)
        self.relu = nn.ReLU()
        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(64*k, num_classes)
        
        # 가중치 초기화
        if init_weights:
            self._weights_initialize()
            
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x
    
    def _make_layer(self, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks-1)
        layers = []
        
        for stride in strides:
            layers.append(WiderBlock(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)
    
    # 가중치 초기화
    def _weights_initialize(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
                
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight,0,0.01)
                nn.init.constant_(m.bias, 0)
                
def WRN_40_10():
    return WRN(40, 10)
```

이제 모델이 잘 만들어졌는지 확인해보자. 입력 데이터로 랜덤한 값을 넣어서 확인해보자.



```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.randn(3,3,64,64).to(device)
model = WRN_40_10().to(device)
output = model(x)
print(output.size())
```

    torch.Size([3, 10])


마지막으로 모델 summary를 출력하고 글을 마친다.

참고한 블로그에는 STL10 데이터셋으로 실제 모델 훈련까지 시키지만 이번 글의 목적은 WRN을 파이토치로 구현하는 것이기 때문에 나머지 과정은 생략한다.


```python
!pip install torchsummary
import torchsummary
torchsummary.summary(model, (3,64,64), device=device.type)
```

    Collecting torchsummary
      Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)
    Installing collected packages: torchsummary
    Successfully installed torchsummary-1.5.1
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
    [0m----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1           [-1, 16, 64, 64]             448
                Conv2d-2          [-1, 160, 64, 64]           2,720
           BatchNorm2d-3           [-1, 16, 64, 64]              32
                  ReLU-4           [-1, 16, 64, 64]               0
                Conv2d-5          [-1, 160, 64, 64]          23,200
           BatchNorm2d-6          [-1, 160, 64, 64]             320
                  ReLU-7          [-1, 160, 64, 64]               0
               Dropout-8          [-1, 160, 64, 64]               0
                Conv2d-9          [-1, 160, 64, 64]         230,560
           WiderBlock-10          [-1, 160, 64, 64]               0
          BatchNorm2d-11          [-1, 160, 64, 64]             320
                 ReLU-12          [-1, 160, 64, 64]               0
               Conv2d-13          [-1, 160, 64, 64]         230,560
          BatchNorm2d-14          [-1, 160, 64, 64]             320
                 ReLU-15          [-1, 160, 64, 64]               0
              Dropout-16          [-1, 160, 64, 64]               0
               Conv2d-17          [-1, 160, 64, 64]         230,560
           WiderBlock-18          [-1, 160, 64, 64]               0
          BatchNorm2d-19          [-1, 160, 64, 64]             320
                 ReLU-20          [-1, 160, 64, 64]               0
               Conv2d-21          [-1, 160, 64, 64]         230,560
          BatchNorm2d-22          [-1, 160, 64, 64]             320
                 ReLU-23          [-1, 160, 64, 64]               0
              Dropout-24          [-1, 160, 64, 64]               0
               Conv2d-25          [-1, 160, 64, 64]         230,560
           WiderBlock-26          [-1, 160, 64, 64]               0
          BatchNorm2d-27          [-1, 160, 64, 64]             320
                 ReLU-28          [-1, 160, 64, 64]               0
               Conv2d-29          [-1, 160, 64, 64]         230,560
          BatchNorm2d-30          [-1, 160, 64, 64]             320
                 ReLU-31          [-1, 160, 64, 64]               0
              Dropout-32          [-1, 160, 64, 64]               0
               Conv2d-33          [-1, 160, 64, 64]         230,560
           WiderBlock-34          [-1, 160, 64, 64]               0
          BatchNorm2d-35          [-1, 160, 64, 64]             320
                 ReLU-36          [-1, 160, 64, 64]               0
               Conv2d-37          [-1, 160, 64, 64]         230,560
          BatchNorm2d-38          [-1, 160, 64, 64]             320
                 ReLU-39          [-1, 160, 64, 64]               0
              Dropout-40          [-1, 160, 64, 64]               0
               Conv2d-41          [-1, 160, 64, 64]         230,560
           WiderBlock-42          [-1, 160, 64, 64]               0
          BatchNorm2d-43          [-1, 160, 64, 64]             320
                 ReLU-44          [-1, 160, 64, 64]               0
               Conv2d-45          [-1, 160, 64, 64]         230,560
          BatchNorm2d-46          [-1, 160, 64, 64]             320
                 ReLU-47          [-1, 160, 64, 64]               0
              Dropout-48          [-1, 160, 64, 64]               0
               Conv2d-49          [-1, 160, 64, 64]         230,560
           WiderBlock-50          [-1, 160, 64, 64]               0
               Conv2d-51          [-1, 320, 32, 32]          51,520
          BatchNorm2d-52          [-1, 160, 64, 64]             320
                 ReLU-53          [-1, 160, 64, 64]               0
               Conv2d-54          [-1, 320, 32, 32]         461,120
          BatchNorm2d-55          [-1, 320, 32, 32]             640
                 ReLU-56          [-1, 320, 32, 32]               0
              Dropout-57          [-1, 320, 32, 32]               0
               Conv2d-58          [-1, 320, 32, 32]         921,920
           WiderBlock-59          [-1, 320, 32, 32]               0
          BatchNorm2d-60          [-1, 320, 32, 32]             640
                 ReLU-61          [-1, 320, 32, 32]               0
               Conv2d-62          [-1, 320, 32, 32]         921,920
          BatchNorm2d-63          [-1, 320, 32, 32]             640
                 ReLU-64          [-1, 320, 32, 32]               0
              Dropout-65          [-1, 320, 32, 32]               0
               Conv2d-66          [-1, 320, 32, 32]         921,920
           WiderBlock-67          [-1, 320, 32, 32]               0
          BatchNorm2d-68          [-1, 320, 32, 32]             640
                 ReLU-69          [-1, 320, 32, 32]               0
               Conv2d-70          [-1, 320, 32, 32]         921,920
          BatchNorm2d-71          [-1, 320, 32, 32]             640
                 ReLU-72          [-1, 320, 32, 32]               0
              Dropout-73          [-1, 320, 32, 32]               0
               Conv2d-74          [-1, 320, 32, 32]         921,920
           WiderBlock-75          [-1, 320, 32, 32]               0
          BatchNorm2d-76          [-1, 320, 32, 32]             640
                 ReLU-77          [-1, 320, 32, 32]               0
               Conv2d-78          [-1, 320, 32, 32]         921,920
          BatchNorm2d-79          [-1, 320, 32, 32]             640
                 ReLU-80          [-1, 320, 32, 32]               0
              Dropout-81          [-1, 320, 32, 32]               0
               Conv2d-82          [-1, 320, 32, 32]         921,920
           WiderBlock-83          [-1, 320, 32, 32]               0
          BatchNorm2d-84          [-1, 320, 32, 32]             640
                 ReLU-85          [-1, 320, 32, 32]               0
               Conv2d-86          [-1, 320, 32, 32]         921,920
          BatchNorm2d-87          [-1, 320, 32, 32]             640
                 ReLU-88          [-1, 320, 32, 32]               0
              Dropout-89          [-1, 320, 32, 32]               0
               Conv2d-90          [-1, 320, 32, 32]         921,920
           WiderBlock-91          [-1, 320, 32, 32]               0
          BatchNorm2d-92          [-1, 320, 32, 32]             640
                 ReLU-93          [-1, 320, 32, 32]               0
               Conv2d-94          [-1, 320, 32, 32]         921,920
          BatchNorm2d-95          [-1, 320, 32, 32]             640
                 ReLU-96          [-1, 320, 32, 32]               0
              Dropout-97          [-1, 320, 32, 32]               0
               Conv2d-98          [-1, 320, 32, 32]         921,920
           WiderBlock-99          [-1, 320, 32, 32]               0
              Conv2d-100          [-1, 640, 16, 16]         205,440
         BatchNorm2d-101          [-1, 320, 32, 32]             640
                ReLU-102          [-1, 320, 32, 32]               0
              Conv2d-103          [-1, 640, 16, 16]       1,843,840
         BatchNorm2d-104          [-1, 640, 16, 16]           1,280
                ReLU-105          [-1, 640, 16, 16]               0
             Dropout-106          [-1, 640, 16, 16]               0
              Conv2d-107          [-1, 640, 16, 16]       3,687,040
          WiderBlock-108          [-1, 640, 16, 16]               0
         BatchNorm2d-109          [-1, 640, 16, 16]           1,280
                ReLU-110          [-1, 640, 16, 16]               0
              Conv2d-111          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-112          [-1, 640, 16, 16]           1,280
                ReLU-113          [-1, 640, 16, 16]               0
             Dropout-114          [-1, 640, 16, 16]               0
              Conv2d-115          [-1, 640, 16, 16]       3,687,040
          WiderBlock-116          [-1, 640, 16, 16]               0
         BatchNorm2d-117          [-1, 640, 16, 16]           1,280
                ReLU-118          [-1, 640, 16, 16]               0
              Conv2d-119          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-120          [-1, 640, 16, 16]           1,280
                ReLU-121          [-1, 640, 16, 16]               0
             Dropout-122          [-1, 640, 16, 16]               0
              Conv2d-123          [-1, 640, 16, 16]       3,687,040
          WiderBlock-124          [-1, 640, 16, 16]               0
         BatchNorm2d-125          [-1, 640, 16, 16]           1,280
                ReLU-126          [-1, 640, 16, 16]               0
              Conv2d-127          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-128          [-1, 640, 16, 16]           1,280
                ReLU-129          [-1, 640, 16, 16]               0
             Dropout-130          [-1, 640, 16, 16]               0
              Conv2d-131          [-1, 640, 16, 16]       3,687,040
          WiderBlock-132          [-1, 640, 16, 16]               0
         BatchNorm2d-133          [-1, 640, 16, 16]           1,280
                ReLU-134          [-1, 640, 16, 16]               0
              Conv2d-135          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-136          [-1, 640, 16, 16]           1,280
                ReLU-137          [-1, 640, 16, 16]               0
             Dropout-138          [-1, 640, 16, 16]               0
              Conv2d-139          [-1, 640, 16, 16]       3,687,040
          WiderBlock-140          [-1, 640, 16, 16]               0
         BatchNorm2d-141          [-1, 640, 16, 16]           1,280
                ReLU-142          [-1, 640, 16, 16]               0
              Conv2d-143          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-144          [-1, 640, 16, 16]           1,280
                ReLU-145          [-1, 640, 16, 16]               0
             Dropout-146          [-1, 640, 16, 16]               0
              Conv2d-147          [-1, 640, 16, 16]       3,687,040
          WiderBlock-148          [-1, 640, 16, 16]               0
         BatchNorm2d-149          [-1, 640, 16, 16]           1,280
                ReLU-150          [-1, 640, 16, 16]               0
    AdaptiveAvgPool2d-151            [-1, 640, 1, 1]               0
              Linear-152                   [-1, 10]           6,410
    ================================================================
    Total params: 55,856,330
    Trainable params: 55,856,330
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.05
    Forward/backward pass size (MB): 430.25
    Params size (MB): 213.07
    Estimated Total Size (MB): 643.38
    ----------------------------------------------------------------


출처 및 참고문헌 :

1. https://deep-learning-study.tistory.com/542
2. https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/wideresidual.py
