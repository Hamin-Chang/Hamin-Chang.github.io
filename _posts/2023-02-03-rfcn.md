---
title : '[OD/개념] 객체 탐지 - R-FCN 📌'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## R-FCN 논문 (Object Detection via Region-based Fully Convolutional Networks) 읽어보기

### 1.  Translation invariance Dilemma

일반적으로 2-stage detector는 서로 다른 task를 수행하는 두 sub-network 간에 주로 학습하는 속성에서 차이가 발생하는데 이를 translation invariance dilemma라고 한다.

![1](https://user-images.githubusercontent.com/77332628/216543585-b3e433b4-894b-46d9-a7c8-e203e68422e2.png)

먼저 **Translation invariance**는 입력값의 위치가 변해도 출력값은 동일한 경우에 해당하는 함수의 속성이다. 위의 이미치처럼 석상의 위치가 달라져도 특정 모델이 동일하게 석상이라고 인식하면 해당 모델은 translation invariance한 속성을 가지고 할 수 있다. 반대로 입력값의 위치가 변하면 출력값이 달라질 경우에는 이를 **translation variance**한 속성을 가지고 있다고 할 수 있다.

일반적으로 Image classification 모델은 translation invariance 속성을 선호하는 반면, Object detection 모델은 객체의 위치가 변하면 이러한 변화를 잘 포착하는 것이 바람직하기 때문에 translation variance 속성을 중요시한다.

2-stage detector은 feature를 추출하는 역할을 수행하는 backbone network와 detection을 수행하는 network로 구성되어 있다. 그 중 backbone network는 image classification task를 위해 pre-trained 되어있다. 즉 원본 이미지를 backbone network에 입력해서 얻은 feature map은 translation invariance한 속성을 띄고 있다. 반면 detection을 수행하는 network는 translation variance한 속성을 가진다. 하지만  원본 이미지를 backbone network에 입력해서 얻은 feature map은 위치 정보가 소실된 채로 detection network로 입력된다. 당연하게도 detection network는 객체에 대한 위치 정보가 없는 feature map이 입력되어서 학습이 제대로 이뤄지지 않는다.

이처럼 두 network간에 충돌이 발생하는 경우를 **translation invariance dilemma**라고 하며, 이로 인해 mAP 값이 하락하게 된다.

### 2. Main Ideas

ResNet 논문의 저자는 translation invariance dilemma를 해결하기 위해 위 이미지처럼 ResNet + Faster R-CNN 모델의 두 conv layer 사이에 RoI pooling을 삽입해서 region specific한 연산을 추가했다.  하지만 본 논문의 저자는 ResNet + Faster R-CNN 모델과 같은 방법을 사용하면 모든 RoI를 개별적으로 conv, fc layer에 입력하기 때문에 학습과 추론의 속도가 느려진다는 점을 지적하고 이를 해결하기 위해 **R-FCN 모델은 RPN을 통해 추출한 RoI끼리 연산을 공유하면서 객체의 위치에 대한 정보를 포함한 feature map을 사용하는 구조**를 사용한다.




### 2.1 Backbone Network

R-FCN 모델은 backbone network로 ResNet-101 network를 사용한다. 논문의 저자는 pre-trained된 ResNet-101 모델의 average pooling layer와 fc layer를 제거하고 오직 conv layer만으로 feature map을 연산하도록 학습시킨다. 마지막 feature map의 channel은 2048-d이며, 1x1 conv 연산을 적용해서 channel 수를 1024-d로 줄인다.

### 2.2 Position sensitive score maps & Position sensitive RoI pooling
![3](https://user-images.githubusercontent.com/77332628/216543598-b2b3e16d-d186-407f-90ec-400d41df7f51.jpeg)

먼저 RPN을 통해서 얻은 각각의 RoI에 대해서 class별로 위치 정보를 encode하기 위해서 $k * k$로 grid로 나눠준다. RoI의 크기가 $w$ x $h$이면, 각 구간의 크기는 $\frac{w}k$ x $\frac{h}k$이다. 논문에서는 $k=3$으로 지정했다.

![4](https://user-images.githubusercontent.com/77332628/216543601-dc1ab6b6-a35f-40fe-87d9-78b05c2f8f8c.png)

Backbone network에서 얻은 feature map의 channel 수가 $k^2(C+1)$이  되도록 마지막 conv 연산을 적용해서 **Position-sensitive score map**를 생성한다. 여기서 $C$는 class수 이고 $k=3$로 지정했기 때문에 position-sensitive score map은 class별로 위치정보인 {top-left, top-center, top-right, ..., bottom-right}에 해당하는 정보를 encode하고 있다. Position-sensitive score map과 RoI를 활용해서 $(i,j)$번째 구간에서 오직  $(i,j)$번째 score map만 pooling하는 **Position-sensitive RoI Pooling**을 수행한다. pooling의 결과는 다음과 같다.

![5](https://user-images.githubusercontent.com/77332628/216543605-dbd56bb9-75c3-4982-b4d8-7ef6b13a346e.png)

* $r_c(i,j)$ : $C$번째 class의 $(i,j)$번째 구간의 값이 pooling된 결과
* $z_{i,j.c}$ : $k^2(C+1)$ score map중 하나의 score map
* $(x_0,y_0)$ : RoI의 top-left corner
* $n$ : 구간 내의 픽셀 수
* $θ$ : 네트워크 내의 모든 학습 가능한 파라미터

![6](https://user-images.githubusercontent.com/77332628/216543607-8a4185ac-a086-4deb-bc1c-b7a04c7ca4d5.png)

pooling의 과정을 간단하게 생각하면 class 별로 $\frac{w}k$ x $\frac{h}k$만큼의 RoI grid에 대해서 average poolin을 수행한 것이라고 볼 수 있다. (논문에서는 max pooling도 적용할 수 있다고 한다.) 이를 통해서 RoI별로 크기가 $k*k$이며 채널 수가 $(C+1)$인 feature map(위 이미지의 최우측 그림)이 생성된다. 

이후 class별로 $k*k$ 크기의 feature map의 각 요소들의 평균을 구한다. 논문에서는 이 과정을 **voting**이라고 한다. $k=3$일 경우, 채널 별로 9개의 요소의 합의 평균을 구하면 된다. 이를 통해서 $(C+1)$ 크기의 feature map을 얻을 수 있고, softmax function을 통해 loss를 계산한다. 다음은 position-sensitive RoI pooling과 voting의 예시다.

![7](https://user-images.githubusercontent.com/77332628/216543610-15b061d1-2466-470f-857b-85c7883adbbb.png)

↑ 'person' 클래스에 대해서 position-sensitive RoI pooling과 voting 수행

![8](https://user-images.githubusercontent.com/77332628/216543613-23810fdb-bd89-4792-b6dd-ce6711502aea.png)

↑ RoI가 object와 정확히 overlap 되지 않았을 때의  position-sensitive RoI pooling과 voting

논문에서는 bounding box regression 역시 비슷한 방법으로 수행하는데, $k^2(C+1)$-d feature map외에도 $4k^2$-d feature map을 추가해서 bounding box regression을 수행한다. 이는 글 뒷부분 training 파트에서 다룬다.



### 3. Loss function

![9](https://user-images.githubusercontent.com/77332628/216543617-e6e9742a-d5a2-4d66-bf3d-c07546dca19d.png)

손실함수는 Fast R-CNN 모델과 같이 cross-entropy loss와 bounding box regression loss의 합으로 구성한다. 손실함수에서 $c^*$은 RoI의 ground truth label에 해당하며, IoU를 기준으로 0.5 이상일 경우, $c^*=1$, 그렇지 않으면 $c^*=0$이다. 두 loss 사이의 가중치를 조절하는 balancing parameter $λ=1$로 설정한다. 손실함수에 대한 자세한 설명은 Fast R-CNN 논문 리뷰[**링크**](https://hamin-chang.github.io/cv-objectdetection/frcnn/#2-multi-task-loss)를 참고 바란다. 

### 4. Training

![10](https://user-images.githubusercontent.com/77332628/216543619-05eab8c1-25a5-49e9-ad51-7686eba49a4f.png)

1) feature extraction

* Input : 원본 이미지
* Process : feature extraction by pre-trained ResNet-101
* Output : feature map

2) Position-sensitive score maps 생성

* Input : feature map
* Process : 3x3x$k^2(C+1)$ conv layer, 3x3x$(4k^2)$ conv layer
* Output : $k^2(C+1)$-d feature map(=position sensitive score map), $4k^2$-d feature map

여기서 $4k^2$-d feature map은 bounding box regression을 위한건데, RoI의 각 구간별로 bounding box의 offset이 encode 된 feature map이다.

3) Region proposal by RPN

RPN 정확한 동작 과정은 Fast R-CNN 논문 리뷰[**링크**](https://hamin-chang.github.io/cv-objectdetection/frcnn/)을 참고바란다.

* Input : feature map by ResNet-101
* Process : region proposals
* Output : RoIs

4) Position-sensitive RoI pooling

* Input : $k^2(C+1)$-d feature map(=position sensitive score map), $4k^2$-d feature map, RoIs
* Process : position-sensitive pooling (average pooling)
* Output : $k ×k ×(C+1)$ feature map, $k×k×4$ feature map

5) Voting

voting이란 feature map에 대해서 다음 이미지처럼 각 채널의 요소들의 평균을 구하는 과정이다.

![11](https://user-images.githubusercontent.com/77332628/216543622-95c83f2a-e731-4db6-862b-39e513f6e8bf.png)


* Input : $k ×k ×(C+1)$ feature map, $k×k×4$ feature map
* Process : Voting
* Output : $(C+1)$-d feature **vector**, 4-d feature **vector**

6) Train R-FCN network by loss function

마지막으로 5)에서 얻은 feature vector를 사용해서 각각 cross-entropy loss와 smooth L1 loss를 구한 후 backward pass를 통해서 network를 학습시킨다. 실제 학습시에는 RPN과 R-FCN을 번갈아가며 학습하는 4-step alternating training 방식을 사용했다.

### 5. Inference & 결론

최종적으로 얻은 예측값에 **Non maximum suppression**을 수행한다. 이 때 NMS threshold = 0.7, IoU threshold = 0.5로 설정한다.

R-FCN 모델은 class별로 객체의 위치 정보를 encode한 position sensitive score & pooling을 통해서 translation invariance dilemma를 효과적으로 해결했다. 이를 통해서 ASCAL VOC 2007 데이터셋을 사용했을 때, 83.6%라는 높은 mAP값을 보였다고 한다. R-FCN 모델은 이름 그대로 fully convolutional network이며, 오직 conv layer로만 구성되어 있다. 또한 position-sensitive pooling 이후 학습 가능한 layer가 없기 때문에 region-wise 연산량이 많지 않아(cost free) 학습 및 추론 속도가 빠르다는 장점이 있다. detection 시 이미지 한 장당 ResNet + Faster R-CNN 모델보다 0.5~20배 이상 빠른 속도를 보인다고 한다.

출처 및 참고 문헌 :

R-FCN 논문 https://arxiv.org/pdf/1605.06409.pdf

개인 블로그 http://herbwood.tistory.com/16
