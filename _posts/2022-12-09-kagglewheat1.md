---
title: '[Kaggle/CV] Global Wheat - Faster RCNN(1) ğŸŒ¾'
toc: true
toc_sticky: true
categories:
  - kaggle-objectdetection
---
## Faster RCNNìœ¼ë¡œ Global Wheat Detection ë¬¸ì œí’€ê¸° - Train

### 0. ë°ì´í„° ì¤€ë¹„í•˜ê¸°
ì´ë²ˆ ê¸€ì—ì„œëŠ” [**ì €ë²ˆ ê¸€**](https://hamin-chang.github.io/cv-objectdetection/ffrcnn/)ì—ì„œ ì•Œì•„ë³¸ Faster RCNNë¥¼ Pytorchë¡œ êµ¬í˜„í•´ì„œ ì£¼ì–´ì§„ ì´ë¯¸ì§€ë“¤ì—ì„œ ë³´ë¦¬ì˜ ë¨¸ë¦¬ë“¤ì´ ì–´ë””ì— ìˆëŠ”ì§€ ì°¾ëŠ” Object Detectionì„ ì§„í–‰í•œë‹¤. ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¡œë“œí•˜ê³ , ë°ì´í„°ë“¤ì˜ ê²½ë¡œë¥¼ ì •ì˜í•œë‹¤. 


```python
import pandas as pd
import numpy as np
import cv2
import os
import re

from PIL import Image

import albumentations as A # pytorch ë²„ì „ image augmentation
from albumentations.pytorch.transforms import ToTensorV2

import torch
import torchvision

from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator

from torch.utils.data import DataLoader, Dataset
from torch.utils.data.sampler import SequentialSampler

from matplotlib import pyplot as plt

DIR_INPUT = '/kaggle/input/global-wheat-detection'
DIR_TRAIN = f'{DIR_INPUT}/train'
DIR_TEST = f'{DIR_INPUT}/test'
```


```python
train_df = pd.read_csv(f'{DIR_INPUT}/train.csv')
print(train_df.head())
print(train_df.shape)
```

        image_id  width  height                         bbox   source
    0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1
    1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1
    2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1
    3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1
    4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1
    (147793, 5)


train.csvì˜ ìƒìœ„ 5ê°œë¥¼ ì¶œë ¥í•´ë³´ë‹ˆ ê° ì´ë¯¸ì§€ì— ë³´ë¦¬ ë¨¸ë¦¬ê°€ ì–´ë””ì— ìˆëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” bounding box ì¢Œí‘œì™€ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ train.csvê°€ ë‹´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆê³ , ì´ 147793ê°œì˜ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤.

ê·¸ë¦¬ê³  bboxì— ìˆëŠ” ê°’ë“¤ì„ x,y,w,h ê°’ìœ¼ë¡œ ë‚˜ëˆ ì„œ train_dfì— ì…ë ¥í•´ì¤€ë‹¤.


```python
train_df['x'] = -1
train_df['y'] = -1
train_df['w'] = -1
train_df['h'] = -1

def expand_bbox(x):
    r = np.array(re.findall("([0-9]+[.]?[0-9]*)",x)) # ì •ê·œ í‘œí˜„ì‹
    if len(r) == 0:
        r = [-1,-1,-1,-1]
    return r

train_df[['x','y','w','h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))
train_df.drop(columns=['bbox'], inplace=True)
train_df['x'] = train_df['x'].astype(np.float32)
train_df['y'] = train_df['y'].astype(np.float32)
train_df['w'] = train_df['w'].astype(np.float32)
train_df['h'] = train_df['h'].astype(np.float32)
```


```python
train_df.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>image_id</th>
      <th>width</th>
      <th>height</th>
      <th>source</th>
      <th>x</th>
      <th>y</th>
      <th>w</th>
      <th>h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>b6ab77fd7</td>
      <td>1024</td>
      <td>1024</td>
      <td>usask_1</td>
      <td>834.0</td>
      <td>222.0</td>
      <td>56.0</td>
      <td>36.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>b6ab77fd7</td>
      <td>1024</td>
      <td>1024</td>
      <td>usask_1</td>
      <td>226.0</td>
      <td>548.0</td>
      <td>130.0</td>
      <td>58.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>b6ab77fd7</td>
      <td>1024</td>
      <td>1024</td>
      <td>usask_1</td>
      <td>377.0</td>
      <td>504.0</td>
      <td>74.0</td>
      <td>160.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>b6ab77fd7</td>
      <td>1024</td>
      <td>1024</td>
      <td>usask_1</td>
      <td>834.0</td>
      <td>95.0</td>
      <td>109.0</td>
      <td>107.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>b6ab77fd7</td>
      <td>1024</td>
      <td>1024</td>
      <td>usask_1</td>
      <td>26.0</td>
      <td>144.0</td>
      <td>124.0</td>
      <td>117.0</td>
    </tr>
  </tbody>
</table>
</div>



ì´ì œ ì „ì²´ ë°ì´í„°ë¥¼ í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¡œ ë‚˜ëˆ ì¤€ë‹¤.


```python
image_ids = train_df['image_id'].unique()
valid_ids = image_ids[-665:] # 665ê°œì˜ ì´ë¯¸ì§€ë¥¼ valid_dataë¡œ ì‚¬ìš©
train_ids = image_ids[:-655] # 2718ê°œì˜ ì´ë¯¸ì§€ë¥¼ trianì— ì‚¬ìš©
```


```python
valid_df = train_df[train_df['image_id'].isin(valid_ids)]
train_df = train_df[train_df['image_id'].isin(train_ids)]
valid_df.shape, train_df.shape
```




    ((25006, 8), (123025, 8))



### 1. ë°ì´í„° ë¡œë“œ, Albumentation í´ë˜ìŠ¤ ì •ì˜í•˜ê¸°
ê·¸ ë‹¤ìŒ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” WheatDatasetì´ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•œë‹¤.


```python
class WheatDataset(Dataset):
    def __init__(self, dataframe, image_dir, transforms=None):
        super().__init__()
        self.image_ids = dataframe['image_id'].unique()
        self.df = dataframe
        self.image_dir = image_dir
        self.transforms = transforms
        
    def __getitem__(self, index:int):
        image_id = self.image_ids[index]
        records = self.df[self.df['image_id'] == image_id]
        
        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg',cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
        image /= 255.0
        
        boxes = records[['x','y','w','h']].values
        boxes[:,2] = boxes[:,0] + boxes[:,2]
        boxes[:,3] = boxes[:,1] + boxes[:,3]
        
        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])
        area = torch.as_tensor(area, dtype = torch.float32)
        
        # there is only one class
        labels = torch.ones((records.shape[0],), dtype=torch.int64)
        
        # suppose all instances are not crowd
        iscrowd = torch.zeros((records.shape[0],),dtype=torch.int64)
        
        target = {}
        target['boxes'] = boxes
        target['labels'] = labels
        # target['masks'] = None
        target['image_id'] = torch.tensor([index])
        target['area'] = area
        target['istarget'] = iscrowd
        
        if self.transforms:
            sample = {
                'image':image,
                'bboxes' : target['boxes'],
                'labels' : labels
            }
            sample = self.transforms(**sample)
            image = sample['image']
            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)
        return image, target, image_id
    
    def __len__(self) -> int:
        return self.image_ids.shape[0]
```

ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ìƒì„±í•œ ë‹¤ìŒ, Albumentationsë¥¼ í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì •ì˜í•´ì¤€ë‹¤.


```python
# Albumentations
def get_train_transform():
    return A.Compose([
        A.Flip(0.5),
        ToTensorV2(p=1.0)
    ], bbox_params={'format':'pascal_voc','label_fields':['labels']})

def get_valid_transform():
    return A.Compose([
        ToTensorV2(p=1.0)
    ], bbox_params={'format':'pascal_voc','label_fields':['labels']})
```

### 2. ëª¨ë¸ êµ¬ì¶•í•˜ê¸°
ì´ì œ ëª¨ë¸ì„ êµ¬ì¶•í•´ë³´ì. ë¨¼ì € Faster RCNNì—ì„œ ì‚¬ìš©í•  ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ loadí•œë‹¤. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì€ ResNet50ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.


```python
# model pretrained on COCO
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
```

    Downloading: "https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth





      0%|          | 0.00/160M [00:00<?, ?B/s]



ê·¸ ë‹¤ìŒ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ fine-tuning í•´ì¤€ë‹¤.


```python
num_classes = 2 # 1 class(wheat) + ë°°ê²½(ì•„ë¬´ objectë„ ì—†ìŒ)

# ë¶„ë¥˜ê¸°ì— ì‚¬ìš©í•  ì…ë ¥ íŠ¹ì§•ì˜ ì°¨ì› ì •ë³´
in_features = model.roi_heads.box_predictor.cls_score.in_features

# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ë¨¸ë¦¬ ë¶€ë¶„ì„ ìƒˆë¡œìš´ ê²ƒìœ¼ë¡œ fine tuning
model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)
```


```python
class Averager:
    def __init__(self):
        self.current_total = 0.0
        self.iterations = 0.0
        
    def send(self, value):
        self.current_total += value
        self.iterations += 1
        
    @property
    def value(self):
        if self.iterations == 0 :
            return 0
        else:
            return 1.0*self.current_total / self.iterations
    
    def reset(self):
        self.current_total = 0.0
        self.iterations =0.0
```


```python
def collate_fn(batch):
    return tuple(zip(*batch))

train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())
valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())

indices = torch.randperm(len(train_dataset)).tolist()

train_data_loader = DataLoader(train_dataset,
                              batch_size=16,
                              shuffle=False,
                              num_workers=4,
                              collate_fn=collate_fn)

valid_data_loader = DataLoader(valid_dataset,
                              batch_size=8,
                              shuffle = False,
                              num_workers = 4,
                              collate_fn= collate_fn)
```

    /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      cpuset_checked))



```python
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
```

 ### 3. ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥í•˜ê¸°


```python
images, targets, image_ids = next(iter(train_data_loader))
images = list(image.to(device) for image in images) 
targets = [{k: v.to(device) for k,v in t.items()} for t in targets]
```


```python
boxes = targets[4]['boxes'].cpu().numpy().astype(np.int32)
sample = images[4].permute(1,2,0).cpu().numpy()
```


```python
fig, ax = plt.subplots(1,1,figsize=(16,8))
for box in boxes:
    cv2.rectangle(sample,
                 (box[0],box[1]),
                 (box[2],box[3]),
                 (220,0,0),3)

ax.set_axis_off()
ax.imshow(sample)
```




    <matplotlib.image.AxesImage at 0x7fadeea71c10>




    
![111](https://user-images.githubusercontent.com/77332628/206717442-0e959229-6070-4675-952b-df9f4b6c140e.png)
    


### 4. ëª¨ë¸ í›ˆë ¨í•˜ê¸°


```python
model.to(device)
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr = 0.005, momentum=0.9, weight_decay=0.0005)
lr_scheduler = None

num_epochs=2
```


```python
loss_hist = Averager()
itr = 1

for epoch in range(num_epochs):
    loss_hist.reset()
    
    for images, targets, image_ids in train_data_loader:
        
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)

        losses = sum(loss for loss in loss_dict.values())
        loss_value = losses.item()

        loss_hist.send(loss_value)

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        
        if itr % 10 == 0:
            print(f"Iteration #{itr} loss: {loss_value}")

        itr += 1
    
    # update the learning rate
    if lr_scheduler is not None:
        lr_scheduler.step()

    print(f"Epoch #{epoch} loss: {loss_hist.value}")   
```

    Iteration #10 loss: 0.8767896085686229
    Iteration #20 loss: 1.0034717957629287
    Iteration #30 loss: 0.9212688329972594
    Iteration #40 loss: 0.8998036128129249
    Iteration #50 loss: 1.0090722438562278
    Iteration #60 loss: 0.9332446984702532
    Iteration #70 loss: 0.9520588092352517
    Iteration #80 loss: 0.8880465836542588
    Iteration #90 loss: 1.2082256340983488
    Iteration #100 loss: 0.7833284031450136
    Iteration #110 loss: 0.695484001848069
    Iteration #120 loss: 0.7146148842041496
    Iteration #130 loss: 0.7105493748761558
    Iteration #140 loss: 0.9121268667220603
    Iteration #150 loss: 0.7621172243502237
    Iteration #160 loss: 0.8594372129115371
    Iteration #170 loss: 0.7435640633778376
    Epoch #0 loss: 0.8754310350903002
    Iteration #180 loss: 0.8129185204232254
    Iteration #190 loss: 0.9815745474449904
    Iteration #200 loss: 0.8815886739787134
    Iteration #210 loss: 0.8623899764875205
    Iteration #220 loss: 0.9818613345877085
    Iteration #230 loss: 0.9182466013264382
    Iteration #240 loss: 0.9697364914691113
    Iteration #250 loss: 0.8432915679451991
    Iteration #260 loss: 1.1767832955492075
    Iteration #270 loss: 0.8194232906491191
    Iteration #280 loss: 0.6887506687183993
    Iteration #290 loss: 0.7078626006163129
    Iteration #300 loss: 0.7143421875285303
    Iteration #310 loss: 0.9085941208984142
    Iteration #320 loss: 0.7327133478059575
    Iteration #330 loss: 0.8352103870566991
    Iteration #340 loss: 0.7305171478843716
    Epoch #1 loss: 0.8581980241023012



```python
images, targets, image_ids = next(iter(valid_data_loader))

images = list(img.to(device) for img in images)
targets = [{k : v.to(device) for k, v in t.items()} for t in targets]
```

    /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      cpuset_checked))



```python
boxes = targets[4]['boxes'].cpu().numpy().astype(np.int32)
sample = images[4].permute(1,2,0).cpu().numpy()
```


```python
model.eval()
cpu_device = torch.device('cpu')

outputs = model(images)
outputs = [{k : v.to(cpu_device) for k,v in t.items()} for t in outputs]
```


```python
fig,ax = plt.subplots(1,1,figsize=(16,8))

for box in boxes:
    cv2.rectangle(sample,
                 (box[0],box[1]),
                 (box[2],box[3]),
                 (220,0,0),3)
    

ax.set_axis_off()
ax.imshow(sample)
```




    <matplotlib.image.AxesImage at 0x7fadef73fe10>




    
![222](https://user-images.githubusercontent.com/77332628/206717450-a95f4a81-7d12-48a0-b86c-384db1731d9a.png)    



```python
torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')
```
