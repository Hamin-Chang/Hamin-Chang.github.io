---
title: '[DL/KERAS] 딥러닝 최적화 - 원리 파해치기 📉'
layout : single
toc: true
toc_sticky: true
categories:
  - kerasBasic
---

# 2. 그레디언트 기반 최적화 (확률적 경사 하강법, 역전파 알고리즘, Gradient Tape)

## 2.1 확률적 경사 하강법

다음의 순서로 최적화를 진행할 수 있다.


1.   훈련 샘플 배치 x와 이에 상응하는 타깃 y_true를 추출한다.
2.   x로 모델을 훈련하고 예측 y_pred를 구한다(정방향 패스).
3.   이 배치에서 y_pred와 y_true 사이의 오차(모델의 손실)을 계산한다.
4.   모델의 파라미터에 대한 손실 함수의 그레디언트를 계산한다(역방향 패스).
5.   그레디언트의 반대 방향으로 파라미터를 조금씩 이동시킨다. 예를 들어 W -= learning_rate * gradient처럼 손실을 조금 감소시킨다.

위의 방법이 **미니 배치 SGD**이다.

만약 대상 파라미터가 작은 학습률을 가진 SGD로 최적화되었다면 전역 최솟값이 아닌 지역 최솟값에 갇히게 될 것이다.

이러한 문제를 **모멘텀**이라는 개념으로 해결할 수 있다. 모멘텀은 현재 그레디언트 값 뿐만 아니라 이전에 업데이트한 파라미터에 기초하여 파라미터 **W**를 업데이트 한다.

다음은 모멘텀의 단순 구현 예다.


```python
past_velocity = 0
momentum = 0.1
while loss > 0.01:
  w, loss , gradient = get_current_parameters()
  velocity = momentum * velocity - learning_rate * gradient
  w = w + momentum * velocity - learning_rate * gradient
  past_velocity = velocity
  update_parameter(w)
```

## 2.2 역전파 알고리즘

복잡한 식의 그레디언트를 계산하는 방법이 **역전파 알고리즘**이다.

연쇄 법칙을 역방향 그래프에 적용하면 , 노드가 연결된 경로를 따라서 우리가 원하는 그레디언트의 계산이 가능해진다. 

역전파는 최종 손실값에서 시작해서 맨 위층까지 거꾸로 올라가서 각 파라미터가 손실값에 기여한 정도는 계산한다. 요즘에는 텐서플로와 같이 **자동 미분**이 가능한 프레임 워크 덕분에 역전파를 쉽게 구현할 수 있다.

### 그레디언트 테이프 (GradientTape)
그레디언트 테이프는 해당 코드 블록 안의 모든 텐서 연산을 계산 그래프 형태(**Tape**)로 기록한다. 그 다음 계산 그래프를 사용해서 tf.Variable 클래스 변수 또는 변수 집합에 대한 어떤 출력의 그레디언트도 계산할 수 있다.
(tf.Variable은 변경 가능한 상태를 담기 위한 특별한 텐서이다.)


```python
import tensorflow as tf

x = tf.Variable(0.) # 초기값 0으로 스칼라 변수를 생성
with tf.GradientTape() as tape: 
  y = 2 * x + 3     # 변수에 텐서 연산을 적용
grad_of_y_wrt_x = tape_gradient(y,x) # tape를 사용해서 x에 대한 y의 그레디언트 계산

```
[<케라스 창시자에게 배우는 딥러닝 개정 2판>(길벗, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.] 출처: 프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496
