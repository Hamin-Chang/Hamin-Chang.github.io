---
title : '[CV/KERAS] 데이터 증식 - 소규모 데이터 훈련하기 🤹'
layout : single
toc: true
toc_sticky: true
categories:
  - CVBasic
---

# 2. 소규모 데이터로 컨브넷 훈련하기 (데이터 증식)

### 2.0 작은 데이터셋으로 딥러닝 훈련하기
모델을 훈련하기에 '충분한 샘플'이라는 정의는 상대적이다. 우선 훈련하려는 모델의 크기와 깊이에 대해 상대적이다. 예를 들어, 복잡한 문제를 푸는 컨브넷을 수십개의 데이터셋으로만 훈련한다는 것은 불가능하지만 , 모델이 작고 규제가 잘되어 있는 모델로 간단한 문제를 푼다고 하면 수백개의 샘플로도 충분할 수 있다. 

컨브넷은 지역적이고 평행이동으로 변하지 않는 특성을 학습하기 때문에 지각에 관한 문제에서 데이터를 효율적으로 사용한다. 따라서 작은 이미지 데이터셋에서 특성공학을 사용하지 않고 처음부터 컨브넷을 훈련해도 어느정도의 결과를 만들 수 있다. 거기에 더해 딥러닝 모델은 태생적으로 다목적이기 때문에 대규모 이미지 데이터셋인 ImageNet 데이터셋에서 사전 훈련된 모델들을 내려받아서 매우 적은 데이터로 강력한 컴퓨터 비전 모델을 만드는데 사용할 수 있다. (*이것이 딥러닝의 가장 큰 장점 중 하나인 특성 재사용이다.*)

### 2.0.1 데이터 내려받기
이번 글에서 사용할 강아지 vs 고양이 데이터셋은 캐글에서 컴퓨터 비전 대회를 개최할 당시 제공했던 데이터셋이다. 원본 데이터셋을 캐글에서 다운 받을수 있지만 다음 코드로 간단히 데이터를 내려받겠다.


```python
import gdown
gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip')
```

    Downloading...
    From: https://drive.google.com/uc?id=18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd
    To: /content/dogs-vs-cats.zip
    100%|██████████| 852M/852M [00:12<00:00, 66.9MB/s]





    'dogs-vs-cats.zip'




```python
!unzip -qq dogs-vs-cats.zip
!unzip -qq train.zip
```

내려받은 데이터셋은 25,000개의 강아지와 고양이 이미지 (각각 12,500개씩)를 담고 있고, 3개의 서브셋이 들어있는 새로운 데이터셋을 만들것이다. 클래스마다 1,000개의 샘플로 이루어진 훈련세트, 클래스마다 500개의 샘플로 이루어진 검증세트, 클래스마다 1,000개의 샘플로 이루어진 테스트세트이다. 

(*소규모 데이터셋으로 컨브넷을 훈련하는 것을 연습하는 것이기 때문에 실제 데이터셋의 일부만 사용한다.*)

shutil 패키지를 사용해서 3개의 서브셋이 들어있는 새로운 데이터셋을 만든다.


```python
import os, shutil, pathlib

original_dir = pathlib.Path("train")  # 원본 데이터셋이 압축 해제되어 있는 디렉터리 경로
new_base_dir = pathlib.Path("cats_vs_dogs_small")  # 서브셋 데이터를 저장할 디렉터리

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)  # 카테고리마다 처음 1,000개의 이미지를 훈련 서브셋으로 저장
make_subset("validation", start_index=1000, end_index=1500) # 카테고리마다 그 다음 500개의 이미지를 검정 서브셋으로 저장
make_subset("test", start_index=1500, end_index=2500)  # 카테고리마다 그 다음 1,000개의 이미지를 테스트 서브셋으로 저장
```

이제 훈련 이미지 2,000개, 검증 이미지 1,000개, 테스트 이미지 2,000개가 준비되었다. 

### 2.1 모델 구축하기

[**합성곱 신경망 : 컴퓨터 비전의 기본**](https://hamin-chang.github.io/conv/) 이 글에서 사용했던 Conv2D층과 MaxPooling2D층을 번갈아 쌓은 컨브넷을 사용해서 모델을 구축하겠다. 임의로 선택한 입력 크기 180x180의 입력으로 시작해서 Flatten 층 이전에 7x7 크기의 특성맵으로 줄어들것이다. Dogs vs Cats 문제는 이진분류 문제이므로 합성곱 층 다음 Dense 층은 크기가 1이고 sigmoid 활성화 함수를 사용해서 모델이 보고 있는 샘플이 한 클래스에 속할 확률을 인코딩할 것이다. 또한 모델의 첫 층을 Rescaling 층으로 시작하는데, 이 층은 [0,255] 범위인 이미지 입력을 [0,1] 범위로 스케일링하는 층이다.


```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(180, 180, 3))  # 임의로 입력 이미지 크기를 180x180으로 설정
x = layers.Rescaling(1./255)(inputs)  # 입력을 255로 나눠서 [0,1]범위로 스케일링
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 180, 180, 3)]     0         
                                                                     
     rescaling (Rescaling)       (None, 180, 180, 3)       0         
                                                                     
     conv2d (Conv2D)             (None, 178, 178, 32)      896       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 89, 89, 32)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 87, 87, 64)        18496     
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 43, 43, 64)       0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 41, 41, 128)       73856     
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 20, 20, 128)      0         
     2D)                                                             
                                                                     
     conv2d_3 (Conv2D)           (None, 18, 18, 256)       295168    
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 9, 9, 256)        0         
     2D)                                                             
                                                                     
     conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    
                                                                     
     flatten (Flatten)           (None, 12544)             0         
                                                                     
     dense (Dense)               (None, 1)                 12545     
                                                                     
    =================================================================
    Total params: 991,041
    Trainable params: 991,041
    Non-trainable params: 0
    _________________________________________________________________


컴파일 단계에서는 RMS 옵티마이저, 손실함수로 이진 크로스엔트로피(binary crossentropy) (*모델의 마지막이 하나의 시그모이드 유닛이기 때문에*)를 사용한다.


```python
model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])
```

### 2.2 데이터 전처리
현재 데이터가 JPEG 파일로 되어 있기 때문에 모델에 데이터를 주입하려면 다음의 과정을 거쳐야한다.


1.   사진 파일을 읽는다.
2.   JPEG 콘텐츠를 RGB 픽셀값으로 디코딩한다.
3.   RGB 픽셀값을 부동 소수점 타입의 텐서로 변환한다.
4.   동일한 크기 (여기서 임의로 정한 180x180)로 변환한다.
5.   배치로 묶는다.(하나의 배치당 32개의 이미지로 구성)

위의 과정을 자동으로 처리하는 케라스의 유틸리티를 사용한다. 케라스가 제공하는 image_dataset_from_directory( ) 함수를 사용하면 디스크에 있는 이미지 파일을 자동으로 전처리된 텐서의 배치로 변환할 수 있다. 

다음 코드와 함께 image_dataset_from_directory( ) 함수를 알아보자



```python
from tensorflow.keras.utils import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
    new_base_dir / 'train', # 먼저 directory의 서브 디렉터리를 찾고 , 이미지 파일을 인덱싱
    image_size = (180,180), # 각 서브 디렉터리에 있는 이미지 파일을 동일한 크기로 변환
    batch_size = 32) # 한 배치당 32개의 이미지가 들어있게 배치로 묶음

validation_dataset = image_dataset_from_directory( # 검증 데이터도 똑같이 전처리
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)

test_dataset = image_dataset_from_directory(   # 테스트 데이터도 똑같이 전처리
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)

''' 텐서플로 Dataset 객체의 유용한 메서드
.shuffle(buffer_size) : 버퍼 안의 원소를 섞는다.
.prefetch(buffer_size) : 장치 활용도를 높이기 위해 GPU 메모리에 로드할 데이터를 미리 준비한다.
.map(callable) : 임의의 변환을 데이터셋의 원소에 적용한다. (ex.원소 크기를 (16,)->(4,)로 변환)
(callable 함수는 데이터셋이 반환하는 1개의 원소를 입력으로 기대한다.)'''
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    Found 2000 files belonging to 2 classes.




```
'텐서플로 Dataset 객체의 유용한 메서드
.shuffle(buffer_size) : 버퍼 안의 원소를 섞는다.
.prefetch(buffer_size) : 장치 활용도를 높이기 위해 GPU 메모리에 로드할 데이터를 미리 준비한다.
.map(callable) : 임의의 변환을 데이터셋의 원소에 적용한다. (ex.원소 크기를 (16,)->(4,)로 변환)\n(callable 함수는 데이터셋이 반환하는 1개의 원소를 입력으로 기대한다.)'
```


위의 코드에서 준비한 Dataset 객체의 출력 하나를 살펴보자.


```python

for data_batch, labels_batch in train_dataset:
    print("데이터 배치 크기:", data_batch.shape)
    print("레이블 배치 크기:", labels_batch.shape)
    break
```

    데이터 배치 크기: (32, 180, 180, 3)
    레이블 배치 크기: (32,)


위의 출력은 180x180 RGB 이미지의 배치 ((32,180,180,3) 크기)와 정수 레이블 배치((32,) 크기)이다. 각 배치에는 32개의 샘플이 있다.

이제 준비한 데이터셋으로 모델을 훈련해보자.


```python
callbacks = [ 
    keras.callbacks.ModelCheckpoint(           # 콜백 사용
        filepath="convnet_from_scratch.keras", # 모델 저장 경로
        save_best_only=True,                   # val_loss 값이 이전보다 낮을 때만 저장
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,        # 검증 데이터
    callbacks=callbacks)
```

    Epoch 1/30
    63/63 [==============================] - 17s 97ms/step - loss: 0.7200 - accuracy: 0.5315 - val_loss: 0.6768 - val_accuracy: 0.5750
    Epoch 2/30
    63/63 [==============================] - 6s 90ms/step - loss: 0.7213 - accuracy: 0.5785 - val_loss: 0.9444 - val_accuracy: 0.5100
    .
    .
    .
    Epoch 29/30
    63/63 [==============================] - 5s 73ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 2.9502 - val_accuracy: 0.7080
    Epoch 30/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0475 - accuracy: 0.9875 - val_loss: 2.5123 - val_accuracy: 0.7250


훈련한 모델의 손실과 정확도를 그래프로 나타내보자.


```python
import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```


    
![small1](https://user-images.githubusercontent.com/77332628/196624197-e1024bcb-d139-4c28-897b-dcfb9cd10fde.png)
    



    
![small2](https://user-images.githubusercontent.com/77332628/196624206-1dbb5458-08ea-46a9-aaf5-a1e235eb56c3.png)
    


위의 그래프에서 볼 수 있듯이 모델이 과대적합된다. 훈련 정확도는 시간이 지남에 따라 100%에 가까이 도달합니다. 하지만 검증 정확도는 13번째 에포크만에 거의 최고점에 다다르고 더이상 진전되지 않는다. 그럼 테스트 정확도는 어떨까? 과대적합 되기 전의 상태를 평가하기 위해 콜백으로 저장한 모델을 로드하자.


```python

test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 36ms/step - loss: 0.5975 - accuracy: 0.6900
    테스트 정확도: 0.690


테스트 정확도는 69%를 얻었다.

이번장에서 다루는 데이터셋의 크기가 작기 때문에 과대적합을 막는 것이 가장 중요한 요소이다. 이전 글에서 언급한 드롭아웃이나 L2 규제 등의 방법들도 있지만 , 컴퓨터 비전에 특화된 과대적합을 막는 방법인 **데이터 증식**을 시도하겠다.

### 2.3 데이터 증식
데이터 증식은 기존 훈련 샘플을 이용해서 더 많은 훈련 데이터를 생성하는 방법이다. 여러가지 랜덤한 변환을 적용해서 샘플을 늘리는 방법이 있다. 데이터 증식의 궁극적인 목표는 모델이 훈련할 때 정확히 같은 데이터를 두번 만나지 않도록 하는 것이다. 그러면 모델이 데이터의 여러 측면을 학습하므로 더 잘 일반화할 수 있다.

케라스에서 모델의 시작부분에 **데이터 증식층**을 추가할 수 있다. 다음 코드는 데이터 증식 층의 예시 코드다.


```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),  # 랜덤하게 50% 이미지를 수평을 뒤집는다.
        layers.RandomRotation(0.1),       # [-10%,+10%] 범위 안에서 랜덤한 값만큼 이미지 회전
        layers.RandomZoom(0.2),  ])         # [-20%,+20%] 범위 안에서 랜덤한 비율만큼 이미지 확대 or 축소
```

증식된 이미지를 출력해보자.


```python
plt.figure(figsize=(10, 10))
for images, _ in train_dataset.take(1):
    for i in range(9):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
```


    
![small3](https://user-images.githubusercontent.com/77332628/196624212-775977a9-9d27-425b-9162-4d5e6deb392f.png)
    


출력된 이미지들을 보면 알겠지만 여전히 입력 데이터들 사이에 상호 연관성이 크다. 즉, 기존 정보의 재조합만 가능하기 때문에 완전히 과대적합을 제거할 수는 없다. 과대적합을 더욱 확실히 억제하기 위해 Dense 층 직전에 Dropout층을 추가한다.

또한 이미지 증식층은 Dropout 층처럼 predict나 evaluate같은 테스트 데이터를 사용하는 단계에서는 동작하는 않는다. 즉, 모델을 평가할 때는 데이터 증식과 드롭아웃이 없는 모델처럼 동작한다.


```python
inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)           # 이미지 증식층 추가
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)              # Dropout 층 추가
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])

```


```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

    Epoch 1/100
    63/63 [==============================] - 8s 101ms/step - loss: 0.7366 - accuracy: 0.5020 - val_loss: 0.6907 - val_accuracy: 0.5000
    Epoch 2/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.6942 - accuracy: 0.5330 - val_loss: 0.6770 - val_accuracy: 0.5590
    .
    .
    .
    Epoch 99/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1689 - accuracy: 0.9455 - val_loss: 0.9251 - val_accuracy: 0.8260
    Epoch 100/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2022 - accuracy: 0.9360 - val_loss: 0.7826 - val_accuracy: 0.8310



```python
import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```

![small4](https://user-images.githubusercontent.com/77332628/196624214-4cfb8c97-bda3-4284-943d-8c4e73a47cc6.png)

![small5](https://user-images.githubusercontent.com/77332628/196624216-391cd22e-fef3-4200-9740-acb05e7851ac.png)

결과를 그래프로 나타내보면 , 데이터 증식과 드롭아웃 덕분에과대적합이 이전의 모델 보다 훨씬 늦은 60,70번째 에포크 근처에서 시작된다. 즉, 모델의 성능이 월등히 좋아졌다.


마지막으로 테스트 세트의 정확도를 확인해보자.


```python
test_model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 37ms/step - loss: 0.4403 - accuracy: 0.8290
    테스트 정확도: 0.829


테스트 정확도를 82.9% 정도 얻었다. 이전에 얻었던 69% 보다 성능이 더 좋아진 것을 알 수 있다. 모델의 파라미터들을 튜닝하면 더 좋은 성능의 모델을 얻을 수 있지만 데이터의 개수가 적기 때문에 한계가 있다. 이런 상황에서 더 좋은 모델을 얻을 수 있는 방법은 **사전 훈련된 모델을 활용**하는 것이다. 다음 글에서 다뤄보도록 하겠다.

[<케라스 창시자에게 배우는 딥러닝 개정 2판>(길벗, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.] 출처: 프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496
