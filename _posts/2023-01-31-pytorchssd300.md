---
title : '[CV/Pytorch] 파이토치로 SSD300 구현하기 🔫'
layout: single
toc: true
toc_sticky: true
categories:
  - pytorchCV
---

## Pytorch로 SSD(Single Shot Detector) 구현하기

이번 글에서는 SSD 모델을 직접 pytorch로 구현해보도록 하겠다. SSD 모델에 대한 자세한 설명은 이전 글([**링크**](https://hamin-chang.github.io/cv-objectdetection/ssd/))를 참고하면 된다. 모델의 입력값을 크기가 300x300인 RGB 이미지를 사용하는 SSD300을 구현해볼 것이다.

### 1. Base Conv Network

SSD300은 VGG16 network를 base conv network로 사용한다. 여기서는 torchvision에서 제공하는 vgg16(pretrained=True) 모델을 사용한다. 이전 글([**링크**](https://hamin-chang.github.io/cv-objectdetection/ssd/))에서 설명했듯이, SSD300에서는 VGG16의 fc6,fc7 layer를 conv6, conv7로 수정한 모델 VGGBase 모듈을 사용한다.


![스크린샷, 2023-01-31 18-54-24](https://user-images.githubusercontent.com/77332628/215727269-518e78ec-4c11-480f-8644-fbe0d8c845f4.png)

[출처 링크](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)

또한 수정한 conv6에서 연산량이 많아지는 것을 막기 위해 artous convolution을 적용하는데, 이때 dilation 값을 6으로 지정한다. artous convolution은 다음 이미지처럼 작동하는 conv network이다.

![2](https://user-images.githubusercontent.com/77332628/215726739-06534b3d-3793-4919-af10-dc29d0105762.gif)

[출처 링크](https://better-tomorrow.tistory.com/entry/Atrous-Convolution)

수정된 VGGBase 모듈은 self.load_pretrained_layers()에 의해 torchvision에서 제공하는 ImageNet에서 사전 훈련된 모델의 weight 값으로 초기화된다. 참고로, 2개의 fc layer를 conv로 대체했기 때문에 일부 연산 shape이 바뀌지만 parameter 수는 동일하기 때문에 연산 shape 변환을 통해 weight 초기화가 가능하다.




```python
import torch.nn as nn
import torch.nn.functional as F
from utils import *
import torchvision

class VGGBase(nn.Module):
  def __init__(self):
    super(VGGBase, self).__init__()

    # VGG16 모델의 기존 conv layers
    self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    self.conv1_2 = nn.Conv2d(64,64, kernel_size=3, padding=1)
    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
    self.conv2_2 = nn.Conv2d(128,128, kernel_size=3, padding=1)
    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True) # ceiling(소수점 올림)

    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)

    # fc6, fc7 layer를 conv layer로 수정
    self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # artous conv 적용
    self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)

    # 사전 훈련된 layers 로드
    self.load_pretrained_layers()
    
  def forward(self, image):
    # image 변수 : 입력 images, (N, 3, 300, 300) shape의 tensor

    out = F.relu(self.conv1_1(image)) # (N, 64, 300, 300)
    out = F.relu(self.conv1_2(out)) # (N, 64, 300, 300)
    out = self.pool1(out) # (N, 64, 150, 150)

    out = F.relu(self.conv2_1(out)) # (N, 128, 150, 150)
    out = F.relu(self.conv2_2(out)) # (N, 128, 150, 150)
    out = self.pool2(out) # (N, 128, 75, 75)

    out = F.relu(self.conv3_1(out)) # (N, 256, 75, 75)
    out = F.relu(self.conv3_2(out)) # (N, 256, 75, 75)
    out = F.relu(self.conv3_3(out)) # (N, 256, 75, 75)
    out = self.pool3(out) # (N, 256, 38, 38) # ceiling mode이기 때문에 38

    out = F.relu(self.conv4_1(out)) # (N, 512, 38, 38)
    out = F.relu(self.conv4_2(out)) # (N, 512, 38, 38)
    out = F.relu(self.conv4_3(out)) # (N, 512, 38, 38)
    conv4_3_feats = out # (N, 512, 38, 38)
    out = self.pool4(out) # (N, 512, 19, 19)

    out = F.relu(self.conv5_1(out)) # (N, 512, 19, 19)
    out = F.relu(self.conv5_2(out)) # (N, 512, 19, 19)
    out = F.relu(self.conv5_3(out)) # (N, 512, 19, 19)
    out = self.pool5(out) # (N, 512, 19, 19) # pool5는 stride=1 때문에 dimension reduce X

    out = F.relu(self.conv6(out)) # (N, 1024, 19, 19)
    conv7_feats = F.relu(self.conv7(out)) # (N, 1024, 19, 19)

    # 6개의 multi-scale feature maps 중 2개의 feature map 추출
    return conv4_3_feats, conv7_feats

  def load_pretrained_layers(self):
    # ImageNet에서 사전 훈련된 VGG16 모델을 로드할건데,
    # 우리가 사용할 VGG16은 fc6, fc7 대신 conv가 들어갔으니 이 부분은 utils의 decimate로 처리

    # 현재 base의 상태
    state_dict = self.state_dict()
    param_names = list(state_dict.keys())

    # Pre-trained VGG base
    pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()
    pretrained_param_names = list(pretrained_state_dict.keys())

    # 사전 훈련된 params를 현재 모델에 전달
    for i, param in enumerate(param_names[:-4]): # conv6, conv7 일단 제외
      state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]

    # fc6, fcy를 conv layer로 변환 (decimation(=down sampling)을 이용해서)
    # fc6 -> conv6
    conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7) 
    conv_fc6_bias = pretrained_state_dict['classifier.0.bias'] # (4096,)
    state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3]) # (1024, 512, 3, 3)
    state_dict['convt.bias'] = decimate(conv_fc6_bias, m=[4]) # (1024,)

    self.load_state_dict(state_dict)

    print('\nLoaded base model.\n')

```

    /usr/lib/python3.8/utils.py:546: SyntaxWarning: "is" with a literal. Did you mean "=="?
      if d.__name__ is 'adjust_hue':


### 2. Auxiliary Convolution Network

SSD 모델은 Base VGG network 마지막 layer에서 추출한 2개의 feature map 이외에 Auxiliary Network에서 추가적으로 4개의 feature map을 추출해서 사용한다. Auxiliary network에서 각 layer에서는 stride를 통해서 pooling을 진행한다. 파라미터 초기화로는 여러 방법 중 Xavier를 사용한다. (Xavier에 대한 자세한 내용은 아직 모르기 때문에 생략한다.)

![3](https://user-images.githubusercontent.com/77332628/215726748-13d4bd9f-0569-40b8-ad2e-ecc5bec71495.jpg)

[출처 링크](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)




```python
class AuxiliaryConvolutions(nn.Module):

  def __init__(self):
    super(AuxiliaryConvolutions, self).__init__()

    self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)
    self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1) # stride 통해 downsampling

    self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)
    self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # stride 통해 downsampling

    self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)
    self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # padding=0 통해 downsampling

    self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)
    self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # padding=0 통해 downsampling

    # convolution parameter 초기화
    self.init_conv2d()

  def init_conv2d(self):
    # convolution parameter 초기화 함수 정의
    for c in self.children():
      if isinstance(c, nn.Conv2d):
        nn.init.xavier_uniform_(c.weight)
        nn.init.constant_(c.bias, 0.)
  
  def forward(self, conv7_feats):
    '''Base VGG 마지막 output인 conv7_feats를 입력으로 순전파해서 
    추가 4개의 feature maps 추출'''
    out = F.relu(self.conv8_1(conv7_feats)) # (N, 256, 19, 19)
    out = F.relu(self.conv8_2(out)) # (N, 512, 10, 10)
    conv8_2_feats = out # 3번째 feature map 추출

    out = F.relu(self.conv9_1(out)) # (N, 128, 10, 10)
    out = F.relu(self.conv9_2(out)) # (N, 256, 5, 5)
    conv9_2_feats = out # 4번째 feature map 추출

    out = F.relu(self.conv10_1(out)) # (N, 128, 5, 5)
    out = F.relu(self.conv10_2(out)) # (N, 256, 3, 3)
    conv10_2_feats = out # 5번째 feature map 추출

    out = F.relu(self.conv11_1(out)) # (N, 128, 3, 3)
    out = F.relu(self.conv11_2(out)) # (N, 256, 1, 1)
    conv11_2_feats = out # 6번째 feature map 추출

    return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats
```

### 3. Prediction Convolution Network

이제 보조 Conv network에서 추출된 6개의 multi-scale feature map을 활용해서 bounding box의 offset을 구하는 **Localization task**와 탐지된 객체의 클래스를 분류하는 **Classification task**를 수행하는 Prediction Convolution Network를 구현해보자. Localization은 4개의 offset을 출력해야하기 때문에 $(g^c_x,g^c_y,g_w,g_h)$ prior box에 4를 곱해준 output을 반환하고, Classification은 각 클래스별 score를 계산하기 때문에 데이터셋에 존재하는 전체 라벨의 갯수를 곱해주면 된다.




```python
class PredictionConvolutions(nn.Module):
  '''
  먼저 localization을 수행할 때, bounding box들은 총 8732개의 offset 값으로 예측된다.
  bounding box의 offset의 encoding은 utils의 cxcy_to_gcxgcy로 수행한다.

  그리고 class score은 각각의 8732개의 bounding box들의 score를 표현한다.
  '''
  def __init__(self, n_classes):
    super(PredictionConvolutions, self).__init__()
    self.n_classes = n_classes

    # feature map마다 다른 개수의 (비율이 다른)prior-box 적용
    n_boxes = {'conv4_3'  : 4,
               'conv7'    : 6,
               'conv8_2'  : 6,
               'conv9_2'  : 6,
               'conv10_2' : 4,
               'conv11_2' : 4}

    # Localization prediction convolution (각 prior box의 offset 예측)
    self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)
    self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv_7'] * 4, kernel_size=3, padding=1)
    self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)
    self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)
    self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)
    self.loc_conv11_2 = nn.Conv2d(512, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)
    
    # Class prediction convolution (localization box의 클래스 예측)
    self.c1_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv7 = nn.Conv2d(512, n_boxes['conv_7'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv9_2 = nn.Conv2d(512, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv10_2 = nn.Conv2d(512, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv11_2 = nn.Conv2d(512, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)

    # convolution param 초기화
    self.init_conv2d()

  def init_conv2d(self):
    # convolution param 초기화 함수 정의
    for c in self.children():
      if isinstance(c, nn.Conv2d):
        nn.init.xavier_uniform_(c.weight)
        nn.init.constant_(c.bias, 0.)
  
  def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):
    # 6개의 multiscale feature map를 순전파해서 8732개의 location과 class score 출력
    batch_size = conv4_3_feats.size(0)

    # Localization bounding boxes (as offsets of prior boxes)
    l_conv4_3 = self.loc_conv4_3(conv4_3_feats) # (N, 16, 38, 38)
    l_conv4_3 = l_conv4_3.permute(0,2,3,1).contiguous() # (N, 38, 38, 16) 
    l_conv4_3 = l_conv4_3.view(batch_size, -1, 4) # (N, 5776, 4) 총 5776개의 boxes 존재

    l_conv7 = self.loc_conv7(conv7_feats) # (N, 24, 19, 19)
    l_conv7 = l_conv7.permute(0,2,3,1).contiguous() # (N, 19, 19, 24) 
    l_conv7 = l_conv7.view(batch_size, -1, 4) # (N, 2166, 4) 총 2166개의 boxes 존재
    
    l_conv8_2 = self.loc_conv8_2(conv8_2_feats) # (N, 24, 10, 10)
    l_conv8_2 = l_conv8_2.permute(0,2,3,1).contiguous() # (N, 10, 10, 24) 
    l_conv8_2 = l_conv8_2.view(batch_size, -1, 4) # (N, 600, 4) 총 600개의 boxes 존재

    l_conv9_2 = self.loc_conv9_2(conv9_2_feats) # (N, 24, 5, 5)
    l_conv9_2 = l_conv9_2.permute(0,2,3,1).contiguous() # (N, 5, 5, 24) 
    l_conv9_2 = l_conv9_2.view(batch_size, -1, 4) # (N, 150, 4) 총 150개의 boxes 존재

    l_conv10_2 = self.loc_conv10_2(conv10_2_feats) # (N, 16, 3, 3)
    l_conv10_2 = l_conv10_2.permute(0,2,3,1).contiguous() # (N, 3, 3, 16) 
    l_conv10_2 = l_conv10_2.view(batch_size, -1, 4) # (N, 36, 4) 총 36개의 boxes 존재

    l_conv11_2 = self.loc_conv11_2(conv11_2_feats) # (N, 16, 1, 1)
    l_conv11_2 = l_conv11_2.permute(0,2,3,1).contiguous() # (N, 1, 1, 16) 
    l_conv11_2 = l_conv11_2.view(batch_size, -1, 4) # (N, 4, 4) 총 4개의 boxes 존재

    # localization boxes의 클래스 예측
    c_conv4_3 = self.cl_conv4_3(conv4_3_feats) # (N, 4 * n_classes, 38, 38)
    c_conv4_3 = c_conv4_3.permute(0,2,3,1).contiguous() # (N, 38, 38, 4 * n_classes)
    c_conv4_3 = c_conv4_3.view(batch_size, -1, self.n_classes) # (N, 5776, n_classes)

    c_conv7 = self.cl_conv7(conv7_feats) # (N, 6 * n_classes, 19, 19)
    c_conv7 = c_conv7.permute(0,2,3,1).contiguous() # (N, 19, 19, 6 * n_classes)
    c_conv7 = c_conv7.view(batch_size, -1, self.n_classes) # (N, 2166, n_classes)

    c_conv8_2 = self.cl_conv8_2(conv8_2_feats) # (N, 6 * n_classes, 10, 10)
    c_conv8_2 = c_conv8_2.permute(0,2,3,1).contiguous() # (N, 10, 10, 6 * n_classes)
    c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes) # (N, 600, n_classes)

    c_conv9_2 = self.cl_conv9_2(conv9_2_feats) # (N, 6 * n_classes, 5, 5)
    c_conv9_2 = c_conv9_2.permute(0,2,3,1).contiguous() # (N, 5, 5, 6 * n_classes)
    c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes) # (N, 150, n_classes)

    c_conv10_2 = self.cl_conv10_2(conv10_2_feats) # (N, 4 * n_classes, 3, 3)
    c_conv10_2 = c_conv10_2.permute(0,2,3,1).contiguous() # (N, 3, 3, 4 * n_classes)
    c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes) # (N, 36, n_classes)

    c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)
    c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)
    c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)

    # 위의 출력값들을 prior boxes의 순서에 맞게 합쳐준다.
    locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1) # (N, 8732, 4)
    classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1) # (N, 8732, n_classes)

    return locs, classes_scores
    
```

위의 PredictionConvolution에서 forward 부분에서 이뤄지는 contiguous() 함수에 대해서 설명을 추가하자면, view 함수를 이용해서 텐서의 shape를 변경해줘야 하는데, Pytorch에서는 항상 contiguous한 메모리 연산을 보장하지 않기 대문에 아래 이미지처럼 연손적인 값들이 하나의 grid를 나타내는 속성을 표현한다는 보장이 없다. 이때 contiguous() 함수를 호출하게 되면 변형된 텐서가 원본 Element의 순서와 같은 모양의 텐서가 만들어진다.

![4](https://user-images.githubusercontent.com/77332628/215726770-f1508770-1cbd-4e11-84a2-dc0f0f6170b2.jpg)

[출처 링크](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)

### 4. Training SSD300
이제 Base, Auxiliary, Prediction Network를 이용해서 SSD300 모델을 훈련시켜보자. Base -> Auxiliary -> Prediction 순서로 훈련을 시키는 것을 구현해 볼 것이다. 코드 중에서 주목할 부분은, self.base에서 나온 conv4_3_feats의 output의 feature value는 Auxiliary Network에서 추출한 4개의 feature map에 비해 scale 적인 측면에서 차이값이 크기 때문에, Prediction Network로 입력 전 이를 L2-normalization을 적용한다. 이를 통해서 (0,1) 사이로 scaling 할 수 있기 때문에, 비정상적인 feature value 차이로 인한 loss exploding을 방지한다.



```python
from math import sqrt
class TrainSSD300(nn.Module):

  def __init__(self, n_classes):
    super(TrainSSD300, self).__init__()
    self.n_classes = n_classes

    self.base = VGGBase()
    self.aux_convs = AuxiliaryConvolutions()
    self.pred_convs = PredictionConvolutions(n_classes)

    # conv4_3_feats가 상대적으로 큰 scale을 가지고 있기 떄문에 L2 norm과 rescale 적용
    self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))
    nn.init.constant_(self.rescale_factors, 20)

    # Prior boxse
    self.priors_cxcy = self.create_prior_boxes()

  def forward(self, image):
    # (N, 3, 300, 300) 크기의 image tensor를 순전파해서 8732개의 location과 class score 출력

    conv4_3_feats, conv7_feats = self.base(image) # (N,512,38,38), (N,1024,19,19)

    # L2 정규화, rescaling conv4_3_feats에 적용
    norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt() # (N, 1, 38, 38)
    conv4_3_feats = conv4_3_feats / norm # (N, 512, 38, 38)
    conv4_3_feats = conv4_3_feats * self.rescale_factors 

    conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \
    self.aux_convs(conv7_feats) # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)

    locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)
    # (N, 8732, 4) , (N, 8732, n_classes)

    return locs, classes_scores
  
  def create_prior_boxes(self):
    # 총 8732개의 prior box를 생성한다. (center-size coordinates 형태로)
    fmap_dims = {'conv4_3': 38,
                 'conv7': 19,
                 'conv8_2': 10,
                 'conv9_2': 5,
                 'conv10_2': 3,
                 'conv11_2': 1}
    
    obj_scales = {'conv4_3': 0.1,
                  'conv7': 0.2,
                  'conv8_2': 0.375,
                  'conv9_2': 0.55,
                  'conv10_2': 0.725,
                  'conv11_2': 0.9}

    aspect_ratios = {'conv4_3': [1., 2., 0.5],
                     'conv7': [1., 2., 3., 0.5, .333],
                     'conv8_2': [1., 2., 3., 0.5, .333],
                     'conv9_2': [1., 2., 3., 0.5, .333],
                     'conv10_2': [1., 2., 0.5],
                     'conv11_2': [1., 2., 0.5]}

    fmaps = list(fmap_dims.keys())

    prior_boxes = []

    for k, fmap in enumerate(fmaps):
      for i in range(fmap_dims[fmap]):
        for j in range(fmap_dims[fmap]):
          cx = (j + 0.5) / fmap_dims[fmap]
          cy = (i + 0.5) / fmap_dims[fmap]

          for ratio in aspect_ratios[fmap]:
            prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])

            '''
            aspect ratio = 1일 때, 현재 feature map의 scale과 다음 feature map의 scale의
            기하 평균의 scale을 가지는 prior 추가적으로 사용
            '''
            if ratio == 1.:
              try :
                  additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k+1]])
                # 만약 마지막 feature map일 경우, 넘어간다.
              except IndexError:
                additional_scale = 1.
              prior_boxes.append([cx, cy, additional_scale, additional_scale])

    prior_boxes = torch.FloatTensor(prior_boxes).to(device) # (8732, 4)
    prior_boxes.clamp_(0,1)

    return prior_boxes

  def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):
    '''
    각 class에 대해서 NMS를 수행해서 상위 top_k개의 box, label, score를 반환한다.
    predicted_locs : 8732개의 prior boxes에 대한 예측한 위치 (N, 8732, 4)
    predicted_scores : 8732개의 prior boxes에 대한 클래스 예측 (N, 8732, n_classes)
    min_score : NMS의 threshold 값
    max_overlap : 2개의 box가 겹칠 수 있는 최댓값
    '''
    batch_size : predicted_locs.size(0)
    n_priors = self.priors_cxcy.size(0)
    predicted_scores = F.softmax(predicted_scores, dim=2) # (N, 8732, n_classes)

    # 최종 전체 이미지에 대한 예측 boxes, labels, scores를 담을 list
    all_images_boxes = list()
    all_images_labels = list()
    all_images_scores = list()

    assert n_priors == predicted_locs.size(0) == predicted_scores.size(1)

    for i in range(batch_size):
      # 예측한 box의 좌표 형식을 x,y 값으로 변환
      decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy)) # (8732, 4)

      # 하나의 해당 이미지에 대한 결과값
      image_boxes = list()
      image_labels = list()
      image_scores = list()

      max_scores, best_labels = predicted_scores[i].max(dim=1) #(8732,)
    
      # Class 하나하나 체크
      for c in range(1, self.n_classes):
        # minimum score 보다 높은 box와 score만 가지고 간다.
        class_scores = predicted_scores[i][:, c] # (8732,)
        score_above_min_score = class_scores > min_score
        n_above_min_score = score_above_min_score.sum().item()
        if n_above_min_score == 0:
          continue
        class_scores = class_scores[score_above_min_score] # (n_qualified)
        class_decoded_locs = decoded_locs[score_above_min_score] # (n_qualified, 4)

        # 예측한 boxes와 scores를 scores에 따라 분류
        class_scoers, sort_ind = class_scoers.sort(dim=0, descending=True) # (n_qualified), (n_min_score)
        class_decoded_locs = class_decoded_locs # (n_min_score, 4)

        # predicted boxes끼리 겹치는 값 찾기
        overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)

        # Non-Maximum Suppression 구현
        # suppress => 1, suppress X => 0 
        suppress = torch.zeros((n_above_min_score)).bool().to(device) # (n_qualified)
        for box in range(class_decoded_locs.size(0)):
          # 만약 해당 box가 이미 suppress로 되었다면 
          if suppress[box] == 1:
            continue
          
          # maximum overlap보다 많이 overlap 된 box들은 suppress
          suppresss = suppress | (overlap[box] > max_overlap)

        # 해당 클래스에서 suppress 되지 않은 box들만 가지고 간다.
        image_boxes.append(class_decoded_locs[~suppress])
        image_labels.append(torch.LongTensor((~suppress).sum().item() * [c]).to(device))

      # 만약 해당 클래스에 대해 아무 object도 없으면 '배경' 클래스로 처리한다.
      if len(image_boxes) == 0:
        image_boxes.append(torch.FloatTensor([[0.,0.,1.,1.]]).to(device))
        image_labels.append(torch.LongTensor([0]).to(device))
        image_scores.append(torch.FloatTensor([0.]).to(device))

      # 위의 값들을 병합한다.
      image_boxes = torch.cat(image_boxes, dim=0) # (n_objects, 4)
      image_labels = torch.cat(image_labels, dim=0) # (n_objects)
      image_scores = torch.cat(image_scores, dim=0) # (n_objects)
      n_objects = image_scores.size(0)

      # 상위 top_k 개의 값만 가지고 간다.
      if n_objects > top_k :
        image_scores, sort_ind = image_scores.sort(dim=0, descending= True)
        image_scores = image_scores[:top_k] # (top_k)
        image_boxes = image_boxes[sort_ind][:top_k] # (top_k, 4)
        image_labels = image_labels[sort_ind][:top_k] # (top_k)

      # 이제 전체 이미지에 대한 결과값 반환
      all_images_boxes.append(image_boxes)
      all_images_labels.append(image_labels)
      all_images_scores.append(image_scores)
    
    return all_images_boxes, all_images_labels, all_images_scores 


```

### 5. MultiBox Loss

이제 마지막으로 손실함수를 정의할건데, 손실함수는 이전 글에서도 정의했듯이 localization loss와 confidence loss를 합친것으로 구현한다. MultiBoxLoss 모듈에서는 localization loss와 confidence loss를 구현하기 전에 먼저 Matching Strategy를 구현한다.




```python
class MultiBoxLoss(nn.Module):
  def __init__(self, priors_cxcy, threshold = 0.5, neg_pos_ratio = 3, alpha = 1.):
    super(MultiBoxLoss, self).__init__()
    self.priors_cxcy = priors_cxcy
    self.priors_xy = cxcy_to_xy(priors_cxcy)
    self.threshold = threshold
    self.neg_pos_ratio = neg_pos_ratio
    self.alpha = alpha

    self.smooth_l1 = nn.L1Loss()
    self.cross_entropy = nn.CrossEntropyLoss(reduce=False)

  def forward(self, predicted_locs, predicted_scores, boxes, labels):
    '''
    predicted_locs : 8732개의 prior box에 대한 predicted locations (N, 8732, 4)
    predicted_scores : predicted locations에 대한 class score (N, 8732, n_classes)
    boxes : true bounding boxes in boundary coordinates
    labels : true object labels
    ==> return multibox loss
    '''
    batch_size = predicted_locs.size(0)
    n_priors = self.priors_cxcy.size(0)
    n_classes = predicted_scores.size(2)

    assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)

    true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device) # (N, 8732, 4)
    true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device) # (N, 8732)

    # for each image
    for i in range(batch_size):
      n_objects = boxes[i].size(0)
      overlap = find_jaccard_overlap(boxes[i], self.priors_xy) # (n_objects, 8732)

      # prior box마다 가장 overlap 값이 큰 object를 찾는다.
      overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)

      '''
      그 전에, 2가지 우리가 원하지 않는 상황이 있는데, 

      1. 해당 object가 모든 prior boxes에 대해 최적의 물체가 아닐수도 있다.
      2. threshold의 값이 0.5이기 때문에 객체가 없는 배경(background)와 할당되는 prior box들이 있을 수도 있다.
      ==> Matching Strategy로 해결!
      '''

      # 먼저, 각 object에 대해 maximum overlap을 가지는 prior를 찾는다.
      _, prior_for_each_object = overlap.max(dim=0)

      # 그리고 각 물체에 해당하는 maximum-overlap-prior를 할당한다. (원치 않는 상황 1 해결!)
      object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)

      # 인위적으로 해당 prior box의 overlap을 0.5보다 큰 값을 준다. (원치 않는 상황 2 해결!)
      overlap_for_each_prior[prior_for_each_object] = 1.

      # 각 prior box의 label 
      labels_for_each_prior = labels[i][object_for_each_prior] # (8732)

      # object과의 overlap이 threshold보다 작은 prior box는 배경(no object)에 할당한다.
      labels_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)

      # 구한 값들 저장
      true_classes[i] = labels_for_each_prior

      # 찾은 prior box들을 center-size object 좌표로 encode 한다.
      true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy) # (8732, 4)

    # positive(non-background) priors를 정의한다.
    positive_priors = true_classes != 0 # (N, 8732)

    # 이제 Localization Loss를 구현할건데, 이는 positive priors에 대해서만 계산한다.
    loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])

    '''
    Confidence Loss를 구현할건데, 일반적으로 이미지 내에서 배경에 해당하는 box의 비율이 높기 때문에 
    negative sample이 positive sample보다 훨씬 많아서 클래스 불균형이 일어나는데,
    이를 해결하기 위해 Hard Negative Mining 기법을 사용한다.

    각 image에 대해 가장 큰 loss를 가지는 (neg_pos_ratio * n_positives)인 
    hardest negative prior boxes를 sample에 추가해서 모델이 hardest negative priors에 집중해서 클래스 불균형을 최소화하도록 한다.
    '''
    n_positives = positive_priors.sum(dim=1)
    n_hard_negatives = self.neg_pos_ratio * n_positives # negative, postive 비율은 3:1로 한다.

    # 전체 prior box에 대해 loss 계산
    conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))
    conf_loss_all = conf_loss_all.view(batch_size, n_priors) # (N, 8732)

    conf_loss_pos = conf_loss_all[positive_priors]

    # hard negative priors를 찾을건데, 이를 위해서 negative priors 중에서 상위 n_hard_negatives개의 priors를 추출한다.
    conf_loss_neg = conf_loss_all.clone()
    conf_loss_neg[positive_priors] = 0. # positive prior의 loss는 제외
    conf_loss_neg, _ = conf_loss_neg.sort(dim=1, desceding = True)
    hardness_rank = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(deivce)
    hard_negatives = hardness_rank < n_hard_negatives.unsqueeze(1)
    conf_loss_hard_neg = conf_loss_neg[hard_negatives]

    # 논문에서처럼 hard negative와 positive priors의 loss의 합을 positive priors의 수로만 나눈다.
    conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()

    # 최종 loss (alpha 값은 두 loss 사이의 가중치를 조절하는 balancing parameter)
    return conf_loss + self.alpha * loc_loss
      
```

굉장히 코드가 길었지만, 이것으로 SSD 모델이 어떻게 작동하는지 구체적으로 알아보았다.

출처, 참고문헌:

[원본 github 코드](https://github.com/Jeffkang-94/pytorch-SSD300/blob/6cb6b3ce0cb98e3f5d2a9fa805cd9b9273a7714b/model.py#L372)

[개인 블로그](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)
