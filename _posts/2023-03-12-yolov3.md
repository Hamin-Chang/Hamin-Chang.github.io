---
title : '[DL/CV] 객체 탐지 - YOLO v3 🤟'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## YOLO v3 논문 읽어보기

이번 글에서는 [<U>YOLO v3 논문</U>](https://arxiv.org/pdf/1804.02767.pdf)을 리뷰해보도록 하겠다. YOLO v3는 당시에 유행하던 새로운 기법들을 도입해서 성능을 향상시킨 모델이다. [<U>YOLO v2 논문 리뷰 포스팅</U>](https://hamin-chang.github.io/cv-objectdetection/yolov2/)과 비교하며 읽으면 도움이 될 것이다. 새롭게 도입된 아이디어들을 위주로 다루겠다.

### 1. Bounding box Prediction

![1](https://user-images.githubusercontent.com/77332628/224516888-a5875b2b-d2eb-413f-a9bb-ae4d109e9b38.png)

YOLO v2는 dimension cluster를 anchor box로 사용해서 bounding box를 예측하는 방법을 사용했다. 모델은 각 bounding box의 좌표 값인 $t_x, t_y, t_w, t_h$를 예측한다. 그리고 위의 이미지처럼 셀의 시작 위치를 $(c_x, c_y)$로 설정하고 , anchor box의 너비와 높이를 $p_w, p_h$로 정해서 다음 과 같은 값으로 변환한 후 L2 loss를 통해 학습시킨다.

<center>$b_x = σ(t_x) + c_x$</center>

<center>$b_y = σ(t_y) + c_y$</center>

<center>$b_w = p_we^{t_w}$</center>

<center>$b_h = p_he^{t_h}$</center>

하지만 YOLO v3는 ground truth 좌표를 위의 공식을 거꾸로 적용시켜서 $t_*$로 변환한 후 $t_x$와 직접 L1 loss를 통해 학습시키는 방법을 사용한다. 예를 들어 ground truth box의 $x$좌표는 다음과 같이 변환시킨다.

<center>$b_* = σ(t_*) + c_*$</center>

<center>$σ(t_*)= b_* - c_*$</center>

<center>$t_* = log(b_* - c_*)$</center>


또한 YOLO v3은 bounding box의 objectness score도 예측한다. bbox와 ground truth의 IoU가 가장 높으면 1, 가장 높진 않고 threshold(=0.5)값을 넘기면 그냥 무시해버려서 IoU 값이 가장 높은 bbox만 매칭시킨다. ground truth box에 할당되지 못한 bbox는 bounding box regression loss를 유발하지 않고, objectness score에 대한 loss만 유발한다.

### 2. Class Prediction

![2](https://user-images.githubusercontent.com/77332628/224516890-92a112db-ec8a-4f43-846d-b251fb5c041f.png)

[<U>이미지 출처</U>](https://kr.mathworks.com/help/deeplearning/ug/multilabel-image-classification-using-deep-learning.html)

YOLO v3에서는 각각의 bounding box에 대해 multi-label classification을 수행한다. 여기서 softmax 함수를 사용하면 성능 면에서 좋지 않기 때문에 **binary cross-entropy**를 사용한다고 하는데, 이러한 방식은 더 복잡한 데이터셋이 있을 때 유의미한 결과를 보여주기 때문이다. 가령 하나의 box안에 복수의 객체가 존재하는 경우 softmax 함수를 사용해서 class를 예측하면 적절하게 포착하지 못한다는 문제가 있다. 따라서 box에서 각 class가 존재하는 여부를 확인하는 binary cross-entropy가 보다 적절하다고 한다.



### 3. Predictions Across Scales

YOLO v3는 3개의 multi-scale feature map을 사용해서 최종 결과를 예측하는데, **multi-scale feature map을 얻는 방법은 FPN과 유사**하다. 다음 이미지처럼 416x416 크기의 이미지를 네트워크에 입력해서 feature map의 크기가 5**2x52. 26x26, 13x13이 되는 layer에서 feature map을 추출**한다. 

![3](https://user-images.githubusercontent.com/77332628/224516891-60b7e640-7409-46ee-8275-d43aeac86e92.jpeg)

그 다음 가장 높은 level(= 가장 낮은 해상도)의 feature map을 1x1, 3x3 conv layer로 구성된 작은 **FCN(Fully Convolutional Network)에 입력**한다. 이후 FCN의 output channel이 512가 되는 지점에서 feature map을 추출하고 **2배로 upsampling한 다음 바로 아래 level의 feature map과 concatenate**한다. 이후 합쳐진 feature map을 FCN에 입력한다. 이 과정을 다음 level의 feature map에 똑같이 수행해서 3개의 scale을 가진 multi-scale feature map을 얻는다. 

이때 각 scale의 feature map의 output channel 수가 [3x(4+1+80)](=255)가 되도록 마지막 conv layer의 channel 수를 조정한다. 여기서 3은 grid cell당 예측하는 anchor box의 수, 4는 bounding box offset, 1은 objectness score, 80은 COCO 데이터셋을 사용했을 때의 class 수이다. 다음 이미지와 같은 과정을 거쳐서 최종적으로 52x52(x255), 26x26(x255), 13x13(x255) 크기의 feature map을 얻을 수 있다.

![4](https://user-images.githubusercontent.com/77332628/224516893-e7865cfb-fb69-4289-b29e-e32864e75250.jpeg)

이러한 방법을 사용하면 더 높은 level의 feature map으로부터 **fine-grained한 정보**를 얻을 수 있고, 더 낮은 level의 feature map으로부터 더 유용한 **semantic 정보**를 얻을 수 있다.

### 4. Feature Extractor

![5](https://user-images.githubusercontent.com/77332628/224516894-9e833725-2a51-4295-84e6-aac9ae4e78da.png)

YOLO v3에서는 shortcut connection이 추가되어 53개의 layer를 가지는 Darknet-53을 backbone network로 사용한다. 이는 ResNet-101보다 1.5배 빠르며, ResNet-152와 비슷한 성능을 보이지만 2배이상 빠르다. 또한 당시 초당 가장 높은 floating point operation 속도를 보였는데, 이는 GPU를 효율적으로 사용함을 의미한다.

### 5. Training YOLO v3

![YOLO v3 architecture](https://user-images.githubusercontent.com/77332628/224516895-e37d4555-2d21-440b-889d-6cc6b515b833.png)


1) feature map by **Darknet-53**

* Input: 416x416 sized image
* Process : extract multi-scale feature maps
* Output : 52x52, 26x26, 13x13 sized feature maps

2) Building **feature pyramid by FCN**

(1)에서 얻은 3개의 feature map을 위에서 설명한 방법으로 FCN에 입력해서 feature pyramid를 설계한다. 

<img width="386" alt="7" src="https://user-images.githubusercontent.com/77332628/224516897-3c32369e-31d5-4f78-a400-1ea3a3d30b7d.png">

* Input: 52x52, 26x26, 13x13 sized feature maps
* Process : building feature pyramid by FCN
* Output : 52x52(x255), 26x26(x255), 13x13(x255) sized feature maps

3) Train YOLO v3 by loss function

논문에서는 구체적으로 언급하지는 않지만 loss function은 다음 4개의 항으로 구성되어 있다.

1. bbox offset의 MSE(Mean Squared Error)
2. 객체를 예측하도록 할당된(responsible for) bbox의 **objectness score의 BCE(Binary Cross Entropy)**
3. 할당되지 않은 bbox의 **no objectness score의 BCE**
4. bbox의 **multi-class BCE **

YOLO v2에서는 모든 예측 결과에 대해 MSE를 적용한 반면 YOLO v3는 **BCE를 주로 사용**한다.



### 6. Inference & 결론

Inferece 시에는 마지막 예측 결과에 대해 NMS(Non Maximum Suppression)을 적용한다.

![8](https://user-images.githubusercontent.com/77332628/224516900-9cb6cbdd-ad2e-4475-a99d-e5b305baa27b.png)

위 그래프에서 볼 수 있듯이 YOLO v3는 성능은 RetinaNet에 비해서는 약간 낮지만 SSD와 성능이 비슷하고 3배 이상의 빠른 속도를 보였다. 특히 x,y 축을 뚫고 나갈 정도로 inference 속도 면에서는 혁신적인 성과를 보였다고 할 수 있다.

출처 및 참고자료

개인 블로그 (https://herbwood.tistory.com/21)

YOLO v3 논문 (https://arxiv.org/pdf/1804.02767.pdf)
