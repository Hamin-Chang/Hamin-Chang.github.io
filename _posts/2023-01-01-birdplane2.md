---
title : '[DL/Pytorch] 파이토치로 분류기 구현2 - 새vs비행기 ✈️'
layout: single
toc: true
toc_sticky: true
categories:
  - pytorchBasic
---
## 파이토치로 구현하는 분류기(완전연결모델,미니배치,DataLoader)

저번 글 ([**링크**](https://hamin-chang.github.io/pytorchbasic/birdplane1/))에 이어서 이번 글에서는 CIFAR-10 데이터셋을 이용해서 본격적으로 이미지를 분류하는 모델을 구현한다. 둘다 하늘을 나는 것들인 새와 비행기를 분류하는 신경망을 구현해보자.

### 1. 데이터셋 구축
가장 먼저 할 일은 데이터의 차원 정보를 맞추는 것이다. 먼저 새와 비행기만 들어간 Dataset 서브 클래스를 만들자. cifar10에 있는 데이터를 필터링하고 레이블을 다시 매핑해서 연속적으로 만들어서 Dataset을 구축해보자.


```python
# cifar10 데이터 로드 (저번 글에서 다룸)
from torchvision import datasets, transforms
data_path = '../data-unversioned/p1ch7/'
cifar10 = datasets.CIFAR10(
    data_path, train=True, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4915, 0.4823, 0.4468),
                             (0.2470, 0.2435, 0.2616))
    ]))
cifar10_val = datasets.CIFAR10(
    data_path, train=False, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4915, 0.4823, 0.4468),
                             (0.2470, 0.2435, 0.2616))
    ]))
```

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz





      0%|          | 0/170498071 [00:00<?, ?it/s]



    Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/
    Files already downloaded and verified



```python
label_map = {0: 0, 2: 1}
class_names = ['airplane', 'bird']
cifar2 = [(img, label_map[label])
          for img, label in cifar10 
          if label in [0, 2]]
cifar2_val = [(img, label_map[label])
              for img, label in cifar10_val
              if label in [0, 2]]
```

### 2. 완전 연결 모델
이제 데이터셋을 준비했고, 데이터를 주입할 모델을 구축해보자.

이전의 글 ([**링크**](https://hamin-chang.github.io/pytorchbasic/neuron/))에서 신경망을 어떻게 만드는지 다뤘다. 간단히 말하자면, 신경망은 피처 텐서가 들어가고 피처 텐서가 출력되는 것이다. 이미지 데이터도 결국 공간 설정에 따라서 적절히 배치된 숫자들의 집합이다. 다음 이미지처럼 공간 설정을 적절하게 다뤄서 이미지 픽셀을 1차원의 긴 벡터로 늘어뜨린다면 결국 이미지의 일련의 숫자들도 입력 피쳐로 볼 수 있지 않을까? 

![bird1](https://user-images.githubusercontent.com/77332628/210169231-38e816c0-d819-4451-9eb2-9218f0fdf6ce.png)

(출처:https://medium.com/analytics-vidhya/from-convolutional-neural-network-to-variational-auto-encoder-97694e86bb51)

하나의 이미지가 32x32x3 크기이기 떄문에 샘플마다 3,072개의 입력 피처가 있는 것으로 생각할 수 있다. nn.Linear 모델이면서 입력 피쳐가 3,072개이고 몇개의 히든 피처를 거쳐서 활성 함수로 이어지는 모델을 구축해보자. 이후에 다른 nn.Linear로 신경망을 줄여나가면서 우리가 원하는 출력 피쳐 수인 2개(새&비행기)로 맞추자. 


```python
import torch.nn as nn

n_out = 2 # 출력 피처 수

model = nn.Sequential(nn.Linear(3072,512), # 512 = 은닉층 크기
                      nn.Tanh(),
                      nn.Linear(512,n_out)) # 512 = 은닉층 크기
```

은닉층에서는 임의로 512개의 은닉 피처를 골랐다. 또한 하나 이상의 활성화 함수를 포함하지 않으면 그저 선형 모델에 불과하기 때문에 Tanh 활성함수를 골랐다. 이제 모델을 만들었으니 모델의 출력이 어떻게 되어야 하는지 생각해보자.

### 3. 분류기의 출력
이전 글([**링크**](https://hamin-chang.github.io/pytorchbasic/neuron/))에서 다룬 온도 문제와는 다르게 비행기와 새의 이미지를 분류하는 분류기의 출력값은 카테고리라는 것을 먼저 인식해야한다. 카테고리 출력값을 표현할 때는 비행기는 [1,0]으로 하고, 새를 [0,1]으로 하는 것처럼 **원핫 인코딩**으로 바꿔줘야 한다(순서는 상관 X). 만약 CIFAR-10 전체 데이터셋에 대한 클래스 10를 사용하는 경우에는 길이가 10인 벡터를 사용하면 된다. 이상적인 신경망은 비행기에 대해서는 torch.tensor([1.0, 0.0])을 출력하고, 새에 대해서는 torch.tensor([0.0, 1.0])을 출력한다. 하지만 우리가 구축하는 분류기는 완벽하지 않기 때문에 두 값 사이가 출력값이 될 것이다. 여기서 중요한 점은 텐서에서 첫번째 값은 '비행기'일 확률이고 두번째 값은 '새'일 확률로 출력을 확률로 해석할 수 있다는 것이다. 

출력값을 확률로 보게 되면 신경망의 출력에 다음의 제약을 수반한다.
* 출력값의 요소는 [0.0~1.0]의 범위로 제한된다.
* 모든 출력 요소의 값의 합은 항상 1.0이다.


#### 3.1 출력을 확률로 표현한다.
위 두 제약을 극복하면서 미분 가능하게 해야하는 것은 가혹한 제한처럼 보일 수 있다. 이런 제약을 극복하면서 미분가능하게 만드는 방법이 존재하는데, 이를 **소프트맥스 (softmax)**라고 부른다. 

소프트맥스는 벡터값을 받아서 동일한 차원의 다른 벡터를 만드는데, 값이 확률로 표현되어야 하는 제약을 만족한다. 다음은 소프트맥스의 표현식이다. 

![bird2](https://user-images.githubusercontent.com/77332628/210169234-d9c3ca48-4040-4c20-a215-9117286f3c6e.jpeg)

(출처:https://limitsinx.tistory.com/36)

nn 모듈은 소프트맥스를 모듈처럼 사용할 수 있게 해준다. 통상적으로 입력 텐서는 0번 차원이 배치에 해당하거나 확률 값을 특정 차원에 대해서만 인코딩하기 때문에 nn.Softmax는 소프트맥스를 적용할 차원을 지정하도록 요구한다. 입력값을 임의로 만들어서 소프트맥스를 한번 적용해보자.


```python
import torch
softmax = nn.Softmax(dim=1) # 적용차원 명시적으로 지정
x = torch.tensor([[1.0,2.0,3.0],
                  [1.0,2.0,3.0]])

softmax(x)
```




    tensor([[0.0900, 0.2447, 0.6652],
            [0.0900, 0.2447, 0.6652]])



이제 우리의 분류기 모델 끝에 소프트맥스를 추가해서 신경망이 확률값을 출력하도록 만들자.


```python
model = nn.Sequential(nn.Linear(3072,512),
                      nn.Tanh(),
                      nn.Linear(512,2),
                      nn.Softmax(dim=1)) # Softmax 추가!!
```

모델을 훈련시키기 전에도 모델을 돌려볼 수는 있으니 새 이미지 하나로 단일 이미지 배치를 만들어보자.


```python
from matplotlib import pyplot as plt
img,label = cifar2[0]

plt.imshow(img.permute(1,2,0))
plt.show()
```

    WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).



    
![bird0](https://user-images.githubusercontent.com/77332628/210169280-73ecdd52-760f-4299-a364-77bbd0dbb956.png)
    


이미지 하나를 선택했고, 이제 모델을 호출하려면 먼저 입력 차원이 맞아야한다. 우리 모델은 3,072개의 입력 피처를 가지고 있고 0번 차원을 따라서 배치로 이뤄지는 데이터를 대상으로 nn에서 작업하게 되어 있다. 따라서 3x32x32 이미지를 1차워 텐서로 만들고 추가 차원을 0번 포지션에 넣자. 


```python
img_batch = img.view(-1).unsqueeze(0) # view로 1차원 텐서로 만들고 unsqueeze로 추가 차원 넣는다.
```

이제 모델을 호출해서 단일 배치를 모델에 주입해보자.


```python
out = model(img_batch)
out
```




    tensor([[0.4096, 0.5904]], grad_fn=<SoftmaxBackward0>)



어라? 확률을 얻었는데 비행기일 확률이 더 높게 출력되었다. 이는 어쩌면 당연한 것인게, 우리가 만든 선형 계층의 가중치나 편향값은 전혀 훈련되지 않고 그저 파이토치에 의해 -1.0과 1.0 사이의 랜덤 갑으로 초기화된 상태다. 또한 지금은 어떤 확률값이 어떤 클래스를 의미하는지 알고 있지만(class_names에서 알 수 있다) 신경망에서는 이를 표시하지 않는다. 구 숫자값에 역전파 후 이런 의미를 부여하는 것은 활성함수다. 레이블에서 0번 인덱스가 '비행기', 1번 인덱스가 '새'로 주어졌다면 이게 출력값이 의미하는 순서가 된다. 리를 가지고 훈련 후에 출력된 확률에 대해 argmax 연산으로 레이블을 얻어낼 수 있다. argmax는 가장 높은 확률에 대한 인덱스다. 차원이 주어지면 torch.max는 해당 차원에서 가장 높은 요소와 인덱스를 리턴한다. 이제 배치를 가로지르는 대신 확률 벡터 내에서 최댓값을 찾아야 한다.


```python
_,index = torch.max(out,dim=1)
index
```




    tensor([1])



아쉽게도 신경망이 이미지가 비행기라고 분류했다. 하지만 이는 그저 우연이기 때문에 이제 모델에 이미지를 넣고 돌려서 훈련을 하면 된다.

### 4. 분류를 위한 손실값
손실값은 확률에 의미를 부여한다. 이번 모델에서는 MSE를 사용해서 출력 확률이 [0.0,1.0] 과 [1.0, 0.0]에 수렴하도록 만들 것이다. 하지만 우리는 argmax 연산으로 예측된 클래스의 인덱스를 뽑아내기 때문에 정확하게 0.0이나 1.0의 확률을 만드는 것보다는 비행기에 대해서는 첫번째 확률이 높게 나오고, 새에 대해서는 두번째 확률이 더 높게 출력되는 것이 더 중요하다. 다시 말해서, 정확하게 0.0이나 1.0이 아닌 모든 경우에 벌점을 주는 대신 분류가 어긋날 경우에 벌점을 주는 것이 낫다.

out이 소프트맥스의 출력이고 class_index가 비행기일 때는 0, 새일뗴는 1을 포함하는 벡터라고 하면, 이제 정답 클래스 out[class_index]에 대한 확률 수치인 **가능도 (likelihood)**를 극대화할 필요가 있다. 바꿔 말하면 가능도가 낮을 때 손실값이 커지는 손실함수가 필요하다. 이런 식으로 동작하는 손실 함수가 있는데, 바로 NLL함수다. 다음은 예측 확률 함수로서의 NLL 손실값이다.

![bird3](https://user-images.githubusercontent.com/77332628/210169311-c4285423-8d43-4913-a6ab-e539ac8dcba7.png)

(출처 : https://gaussian37.github.io/dl-concept-nll_loss/)

위 그래프를 살펴보면 데이터에 낮은 확률이 주어지면 NLL은 무한으로 늘어나는 반면, 확률이 0.5 보다 커지면 NLL은 완만하게 감소하는 경향을 볼 수 있다. NLL은 확률을 입력으로 받기 때문에 가능도가 증가하면 다른 확률은 필연적으로 줄어든다는 점을 기억하자.

파이토치에서 제공하는 nn.NLLLoss 클래스를 사용해서 NLL을 구현할 수 있는데, 입력값으로 확률이 아니라 로그 확률의 텐서를 받는다. 근데 입력을 확률의 로그값으로 받으면 확률이 0에 가까울 때 문제가 되는데, 이를 해결하기 위해서 nn.Softmax 대신 nn.LogSoftmax를 사용한다. 이제 nn.LogSoftmax를 출력 모듈로 사용해서 모델을 수정하자.



```python
model = nn.Sequential(nn.Linear(3072,512),
                      nn.Tanh(),
                      nn.Linear(512,2),
                      nn.LogSoftmax(dim=1)) # 출력 모듈 수정

loss = nn.NLLLoss() # NLL 손실값 초기화
```

손실값은 배치에 대한 nn.LogSoftmax 출력을 첫번째 인자로 받고 클래스 인덱스(여기서는 여러개의 0이나 1) 텐서를 두번째 인자로 받는다. 이제 새 이미지를 가지고 테스트해보자.


```python
img, label = cifar2[0]
out = model(img.view(-1).unsqueeze(0))
loss(out,torch.tensor([label]))
```




    tensor(0.7651, grad_fn=<NllLossBackward0>)



### 5. 분류기 훈련
이제 이전 글 ([**링크**](https://hamin-chang.github.io/pytorchbasic/pytrain2/))에서 다룬 훈련 루프로 모델을 훈련시키자.


```python
import torch
import torch.nn as nn
import torch.optim as optim

# 모델 정의
model = nn.Sequential(nn.Linear(3072,512),
                      nn.Tanh(),
                      nn.Linear(512,2),
                      nn.LogSoftmax(dim=1)) 

learning_rate = 1e-2 # 학습률 정의

optimizer = optim.SGD(model.parameters(), lr = learning_rate) # 옵티마이저 정의

loss_fn = nn.NLLLoss() # 손실함수 정의

n_epochs = 100 

# 훈련 루프 정의
for epoch in range(n_epochs):
  for img, label in cifar2:
    out = model(img.view(-1).unsqueeze(0))
    loss = loss_fn(out,torch.tensor([label]))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  print(f'Epoch : {epoch}, Loss : {float(loss)}')
```

훈련 루프를 보면 이전 글 ([**링크**](https://hamin-chang.github.io/pytorchbasic/pytrain2/))에서 정의한 훈련 루프와 다른 점이 있다. 이전 글에서는 에포크 전체에 하나의 루프가 있어서 하나의 단일 배치에 1만개의 이미지를 모두 평가했었는데, 이는 너무 많기 때문에 내부 루프 안에서 한번에 하나의 샘플을 평가하고 단일 샘플에 대해 역전파를 시킨다. 그런데 하나의 샘플에 기반해서 손실 값을 줄이는 것이 다른 샘플에는 악영향을 미칠수도 있다. 그래서 각 에포크마다 샘플을 섞은 후 한번에 하나 혹은 여러개의 샘플(**미니배치**)에 대해 기울기를 평가하면 경사 하강에 랜덤한 효과를 넣어줄 수 있다. 미니 배치에서 얻은 기울기를 사용하면 전체 데이터셋에 대한 값만큼 근사하지는 않지만, 훈련 중에 만날수도 있는 지역 최솟값에 최적화 과정이 빠져버리는 것을 예방하고 수렴을 돕는 것으로 알려져있다. 그래서 우리는 미니배치를 사용할 때는 작은 학습률을 사용한다. 각 에포크에서 데이터셋을 섞으면 미니 배치에서 얻은 기울기의 시퀀스가 전체 데이터셋에서 계산한 기울기를 대표하도록 만드는 데 도움이 된다. 다음 이미지는 미니 배치를 사용했을 때와 전체 데이터셋을 사용했을 때의 손실값 그래프다.

<img width="1326" alt="bird4" src="https://user-images.githubusercontent.com/77332628/210169313-0a309faf-77c8-4c59-a5da-ef324909f4f3.png">

(출처:https://kenndanielso.github.io/mlrefined/blog_posts/13_Multilayer_perceptrons/13_6_Stochastic_and_minibatch_gradient_descent.html)

통상 미니 배치 크기는 학습률을 지정했던 것처럼 훈련 전에 상수로 고정한다. 이와 같이 고정하는 값들을 모델이 파라미터와 구분해서 **하이퍼파라미터 (Hyperparameter)**라고 부른다.

위의 훈련 루프는 데이터셋에서 한번에 하나의 샘플을 골라서 크기가 1인 미니 배치를 만든다. torch.utils.data 모듈에는 미니 배치의 데이터를 섞거나 구조화하는 작업을 돕는 **DataLoader** 클래스가 있다. 데이터 로더를 활용해서 데이터셋에서 미니 배치에 포함될 샘플을 가져올 때 각 에포크마다 데이터를 섞은 후 고르게 샘플링 할 수 있다. 다음 이미지는 DataLoader가 Dataset에서 개별 데이터 아이템을 샘플링해서 미니배치를 구성하는 모습니다.


![bird5](https://user-images.githubusercontent.com/77332628/210169238-7d4f3d38-90f0-4216-b7ae-a3564cdafc63.png)

(출처:https://livebook.manning.com/concept/deep-learning/dataloader)

이를 이제 구현해보자.


```python
import torch
import torch.nn as nn

train_loader = torch.utils.data.DataLoader(cifar2,batch_size=64,shuffle=True)

# DataLoader는 순회 가능하기 때문에 새로운 훈련 루프 안에 바로 넣어 사용 가능하다.

# 모델 정의
model = nn.Sequential(nn.Linear(3072,512),
                      nn.Tanh(),
                      nn.Linear(512,2),
                      nn.LogSoftmax(dim=1)) 

learning_rate = 1e-2 # 학습률 정의

optimizer = optim.SGD(model.parameters(), lr = learning_rate) # 옵티마이저 정의

loss_fn = nn.NLLLoss() # 손실함수 정의

n_epochs = 100 

for epoch in range(n_epochs):
  for imgs, labels in train_loader:
    batch_size = imgs.shape[0]
    outputs = model(imgs.view(batch_size, -1))
    loss = loss_fn(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  if epoch % 10 ==0:
    print(f'Epoch : {epoch}, Loss : {float(loss)}')
```

    Epoch : 0, Loss : 0.3013339042663574
    Epoch : 10, Loss : 0.4183729588985443
    Epoch : 20, Loss : 0.20577408373355865
    Epoch : 30, Loss : 0.16332823038101196
    Epoch : 40, Loss : 0.13230416178703308
    Epoch : 50, Loss : 0.07194090634584427
    Epoch : 60, Loss : 0.0998682901263237
    Epoch : 70, Loss : 0.038238171488046646
    Epoch : 80, Loss : 0.039314646273851395
    Epoch : 90, Loss : 0.03406476974487305


루프 내부에서 imgs 는 64 RGB 이미지에 해당하는 64x3x32x32인 텐서이며 labels는 레이블 인덱스를 가진 64 크기의 텐서다.

훈련 루프를 실행한 결과 손실값이 줄어들긴 했지만 아직 손실값이 충분히 줄어든 건지는 모른다. 일단 검증셋으로 테스트해서 전체 중에 정확하게 분류된 샘플 수를 세어 모델의 정확도를 계산해보자.


```python
val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size = 64, shuffle = False)

correct = 0
total = 0

with torch.no_grad():
  for imgs, labels in val_loader:
    batch_size = imgs.shape[0]
    outputs = model(imgs.view(batch_size, -1))
    _, predicted = torch.max(outputs, dim=1)
    total += labels.shape[0]
    correct += int((predicted == labels).sum())

print('Accuracy : ', correct/total)
```

    Accuracy :  0.8225


약 82.25%의 정확도가 나왔다. 완벽한 분류기라고는 할 수 없지만 맘대로 예측하는 것보다는 좋은 성능을 내고 있다. 사용한 데이터셋이 너무 단순해서 더 좋은 정확도가 나오지 않은 걸수도 있다.모델 계층을 더 추가해서 모델 용량을 늘려서 한번 훈련 루프를 돌려보자.


```python
model = nn.Sequential(
            nn.Linear(3072, 1024),
            nn.Tanh(),
            nn.Linear(1024, 512),
            nn.Tanh(),
            nn.Linear(512, 128),
            nn.Tanh(),
            nn.Linear(128, 2),
            nn.LogSoftmax(dim=1))
```

nn.LogSoftmax와 nn.NLLLoss 조합은 nn.CrossEntropyLoss와 동일하다. nn.NLLLoss는 크로스엔트로피를 계산하지만 입력으로 로그 확률 예측을 받는 반면, nn.CrossEntropyLoss는 점수를 입력으로 받는다. 그래서 일반적으로 신경망의 마지막 계층에 nn.LogSoftmax를 사용하는 대신에 손실함수로 nn.CrossEntropyLoss를 사용한다.


```python
model = nn.Sequential(
            nn.Linear(3072, 1024),
            nn.Tanh(),
            nn.Linear(1024, 512),
            nn.Tanh(),
            nn.Linear(512, 128),
            nn.Tanh(),
            nn.Linear(128, 2))

learning_rate = 1e-2

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

loss_fn = nn.CrossEntropyLoss()

n_epochs = 100

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if epoch % 10 ==0 or epoch == 99:
      print("Epoch: %d, Loss: %f" % (epoch, float(loss)))
```

    Epoch: 0, Loss: 0.435317
    Epoch: 10, Loss: 0.307439
    Epoch: 20, Loss: 0.124069
    Epoch: 30, Loss: 0.220176
    Epoch: 40, Loss: 0.281665
    Epoch: 50, Loss: 0.026775
    Epoch: 60, Loss: 0.006008
    Epoch: 70, Loss: 0.002413
    Epoch: 80, Loss: 0.008653
    Epoch: 90, Loss: 0.000639
    Epoch: 99, Loss: 0.000300



```python
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
                                           shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in train_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Train Accuracy: %f" % (correct / total))
```

    Train Accuracy: 1.000000



```python
val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,
                                         shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Validate Accuracy: %f" % (correct / total))
```

    Validate Accuracy: 0.812500


모델이 클수록 정확도가 올라가야 하지만, 생각보다는 그렇게 향상되지 않음을 발견할 수 있다. 훈련셋에 대한 정확도는 굉장히 높은데, 어떻게 된 것일까? 모델의 용량이 더 커졌기 때문에 완전 연결 모델이 훈련셋을 암기하는 방식으로 비행기와 새를 구별하기 돼서 검증셋의 성능이 그렇게 좋지 못한 것이다. 

파이토치는 nn.Model의 parameters() 메소드를 통해 모델이 가지고 있는 파라미터 수를 빠르게 확인할 수 있다. 각 텐서 객체에 얼마나 많은 요소가 있는지 확인하려면 numel 메소드를 사용하고 얻은 값을 합산해서 전체 수를 파악하면 된다. 참고로 파라미터 수를 세려면 파라미터에 대해 requires_grad가 True로 설정되어 있는지도 확인해야 한다. 전체 델 크기 대비 학습 가능한 파라미터의 수는 구별되어야 하기 때문이다. 


```python
sum([p.numel() for p in model.parameters() if p.requires_grad == True])
```




    3737474



370만개가 넘는 파라미터를 모델이 가지고 있다. 조그만 입력 이미지를 다룰만한 작은 신경망이라고 보기는 어렵다. 그렇기 때문에 과대적합이 발생한것으로 보인다. 심지어 모델의 용량을 늘리기전의 모델의 파라미터 수도 적지 않다.


```python
first_model = nn.Sequential(
                nn.Linear(3072, 512),
                nn.Tanh(),
                nn.Linear(512, 2),
                nn.LogSoftmax(dim=1))

numel_list = [p.numel() for p in first_model.parameters()]
sum(numel_list), numel_list
```




    (1574402, [1572864, 512, 1024, 2])



이렇게 간단한 신경망에도 어마무시한 수의 파라미터가 있는데, 입력 이미지의 크기가 더 커지면 모델의 파라미터 수는 오늘날 최신 GPU 리소스를 이미 다 채워버릴 만큼 많아진다. 이것이 완전 연결 모델의 한계 중 하나다.

### 6. 완전 연결의 한계
다음 이미지를 보면서 이미지를 1차원으로 놓고 선형 모듈을 사용하는 방식에 대해서 생각해보자. 입력으로 들어오는 RGB 이미지의 모든 정보 하나하나를 사용하며 출력 피처 하나하나에 대해 각 정보의 선형 조합을 계산한다. 그리고 현재의 식별 작업에 잠재적으로 필요하다는 가정하에 픽셀 하나마다 다른 모든 픽셀과의 조합을 고려하고 있지만 상대적으로 가까운 위치나 먼 위치에 있는 점을 사용하지 않고 이미지를 하나의 큰 숫자 벡터로 취급한다.

![bird6](https://user-images.githubusercontent.com/77332628/210169239-aeed5a57-5957-4db0-9e23-9ccc37dcec24.png)

(출처 : https://livebook.manning.com/book/deep-learning-with-pytorch/chapter-7/182)

32x32 이미지에 잡힌 하늘을 나는 비행기는 파란색 배경에 어두운 십자가 모양과 유사하다. 위의 이미지에서의 완전 연결 신경망은 학습을 통해서 픽셀 0,1은 어둡고 픽셀 1,1도 어둡고 하는 식으로 인식하면 비행기일 가능성이 높다고 판단할 것이다. 하지만 다음 이미지처럼 비행기를 한 픽셀 이상 위치를 이동하면 각 픽셀 간의 관계를 처음부터 다시 학습해야한다. 이를 완전 연결 신경망은 **평행이동 불변성**이 없다고 말한다.

![bird7](https://user-images.githubusercontent.com/77332628/210169241-aff1f785-cbc7-4b99-8f0d-b8aa4608adfc.png)

(출처 : https://livebook.manning.com/book/deep-learning-with-pytorch/chapter-7/182)

이를 보완하기 위해서는 데이터를 증강해서, 즉 비행기를 여러 곳으로 랜덤하게 평행이동 시켜서 이미지의 모든 영역에서 비행기를 모델이 보게 해야한다. 하지만 이러한 데이터 증강 전략은 상당한 비용이 든다. 이것이 바로 완전 연결 모델의 한계점이다. 이러한 한계점을 해결하기 위해 컨볼루션 층을 사용할 것인데 이는 다음 글에서 다뤄보겠다.

[<파이토치 딥러닝 마스터:모의암 진단 프로젝트로 배우는 신경망 모델 구축부터 훈련,튜닝,모델 서빙까지>(책만, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.] 도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496



