---
title : '[OD/개념] 객체 탐지 - Faster RCNN ⚓'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---
## Faster RCNN 논문 읽어보기

### 0. 핵심 아이디어
Faster-RCNN은 Real Time Object Detection의 포문을 연 기법이라고 할 수 있다. Faster-RCNN의 가장 큰 특징은 그동안 Selective Search를 사용해서 계산해왔던 Region Proposal 단계를 신경망에 포함시켜서 Fast-RCNN보다 완전한 end-to-end 모델을 제시했다.

Faster-RCNN의 핵심 아이디어는 Region Proposal Network (RPN)이다. 기존 Fast-RCNN에서 selective search를 없애고 대신 RPN을 사용해서 RoI를 계산한다. 다음은 Faster-RCNN의 전반적인 구조이다.

![ffrcnn1](https://user-images.githubusercontent.com/77332628/206435587-c4b64db8-7d84-4657-94fb-80415c6fa813.png)

개략적인 구조를 보면 Fast-RCNN과 같이 CNN을 통과시켜서 Feature Map을 추출한 후 RPN에 전달해서 RoI를 계산하는 구조를 가지고 있다. 

### 1. Anchor Box
먼저 RPN에 사용되는 아이디어인 Anchor box에 대해서 알아보자. Selective Search를 통해 region proposal을 하지 않을 경우 원본 이미지를 일정 간격의 grid로 나눠서 grid cell을 bounding box로 간주해서 feature map에 encode하는 **Dense Sampling**을 사용한다. 

![ffrcnn2](https://user-images.githubusercontent.com/77332628/206435589-8d6a2e74-f420-4c50-bc77-e0ef2a0b8ffd.jpeg)

하지만 고정된 크기의 bounding box를 사용할 경우 다양한 크기의 객체를 포착하지 못할 수도 있다는 문제가 있다. 그렇기 때문에 논문에서는 다양한 크기와 가로세로비(ascept ratio)를 가지는 bounding box인 Anchor Box를 사용한다. 다음과 같이 3가지 크기와 3가지 비율을 사용해서 9개의 서로 다른 anchor box를 사전에 정의한다.

![ffrcnn3](https://user-images.githubusercontent.com/77332628/206435591-42f2624e-80fc-45f2-bb59-e16c6f59513b.jpeg)

anchor box는 원본 이미지의 각 grid cell의 중심(anchor)을 기준으로 생성한다. 원본 이미지에서 sub-sampling ratio를 이용해서 가전에 정의한 anchor box 9개를 생성한다. 다음 이미지에서 원본 이미지의 크기는 600x800, sub-sampling ratio는 1/16이다. anchor은 총 1900(=600/16 x 800/16)개이고, anchor box는 총 17100(=1900x9)개가 생성된다. 이 방식을 사용하면 고정된 크기의 bounding box를 사용할 때보다 9배 많은 bounding box를 생성하고 그 결과 더 다양한 크기의 객체를 포착할 수 있다.

### 2. Region Propsal Network (RPN)

원본 이미지에서 anchor box를 생성하면 수많은 region proposals가 생성된다. RPN은 region proposals에 대해서 class score을 매기고, bounding box coefficient를 출력한다. RPN의 자세한 알고리즘을 알아보자.

![ffrnn4](https://user-images.githubusercontent.com/77332628/206435595-5bc7087b-7c21-405c-85d9-d1e97acce90e.png)

(괄호에 있는 내용은 각 과정의 예시이다.)
1. 원본 이미지를 사전 훈련된 VGG 모델에 통과시켜서 feature map을 얻는다. (원본 이미지 크기가 800x800이고 sub-sampling ratio가 1/100일때, 8x8 크기의 feature 맵이 생성된다.)
2. (1)에서 얻은 feature map에 대해서 3x3 conv 연산을 적용하고, 이때 feature map의 크기를 유지하기 위해 padding=1을 사용한다. (8x8x512 feature map에 3x3 연산을 적용해서 8x8x512 feature map이 출력된다.)
3. class score를 매기기 위해 feature map에 1x1 conv 연산을 적용한다. 여기서 입력 이미지의 크기에 상관없이 작동할 수 있도록 fully-connected가 아닌 Fully Convolution Network를 사용한다.
4. RPN에서는 후보 영역이 어떤 class에 해당하는지까지는 구분하지 않고 객체가 포함되어 있는지 여부만 분류한다. 따라서 channel 수는 2(object 존재 여부) x 9(anchor box 9개)가 된다. (8x8x512의 feature map을 8x8x2x9의 feature map으로 출력한다.)
5. Bounding Box Regression을 얻기 위해 feature map에 1x1 conv를 적용한다. 이 때 feature map의 채널 수는 4(bounding box regressor) x 9(anchor box 9개)가 된다.(8x8x512의 feature map을 8x8x4x9의 feature map으로 출력한다.)

![ffrcnn5](https://user-images.githubusercontent.com/77332628/206435606-3c80b77b-d2e5-4a92-8051-4815d6d5f5eb.jpeg)

RPN의 출력 결과는 위의 표와 같다. 좌측 표는 객체 포함 여부(classification)를 나타낸 feature map이고, 우측 표는 anchor box의 종류에 따라 bounding box regressor를 나타낸 feature map이다. 8x8 grid cell마다 9개의 anchor box가 생성되기 때문에 576개의 region proposals가 추출되며, feature map을 통해 각각에 대한 객체 포함 여부와 bounding box regressor를 파악할 수 있다. 이 중 상위 N개의 region proposal만 추출하고 이를 Non maximum suppression을 적용해서 최적의 region proposals만을 Fast-RCNN에 전달한다.

### 3. Multi-task loss
RPN에서는 classification과 Bounding Box Regression을 수행하기 때문에 손실 함수는 이 두가지에서 얻은 손실을 엮은 Multi-task Loss를 사용한다. 

![ffrcnn6](https://user-images.githubusercontent.com/77332628/206435612-b72e973a-31ac-4df5-af83-1fd831629d14.png)

* i : mini-batch에서 anchor의 index
* pi : anchor i에 객체가 포함되어 있을 예측 확률
* pi* : anchor가 양성일 경우 1, 음성일 경우 0
* ti : 예측 bounding box의 파라미터화된 좌표(coefficient)
* ti* : ground truth box의 coefficient
* Lcls : log loss를 활용한 classification loss 함수
* Lreg : Smooth L1 loss를 활용한 BBR loss 함수
* Ncls : mini-batch 사이즈 (논문에서 256)
* Nreg : anchor 위치의 수
* λ : Lcls 와 Lreg 사이에 가중치를 조절(논문에서 10=동일한 가중치)

### 4. Faster RCNN 훈련하기

Faster RCNN의 훈련 과정은 다음 이미지와 같은데, 작동 방식이 복잡하기 때문에 각 layer들을 설명하면서 작동 방식을 알아보겠다.

![ffrcnn7](https://user-images.githubusercontent.com/77332628/206435615-18674f33-6c4a-4dc8-b2b9-46b536a3d816.png)

1) 사전 훈련된 VGG16

sub-sampling ratio = 1/16 을 사용해서 feature extraction을 진행한다.
* Input : 800x800x3 원본 이미지
* Process : feature extraction by pre trained VGG16
* Output : 50x50x512 feature map

2) Anchor generation layer

원본 이미지에 대해서 anchor box를 생성한다.

* Input : 800x800x3 원본 이미지
* Process : anchor 생성
* Output : 22500(50x50x9)개의 anchor boxes

3) RPN

class score, boudning box regressor를 반환한다.
* Input : 50x50x512 feature map
* Process : Region proposal
* Output : 50x50x2x9 feature map(class score), 50x50x4x9 feature map(bounding box regressor)

4) Proposal layer

(2)에서 생성된 anchor boxes와 (3)에서 반환한 class scores와 bounding box regressor를 이용해서 region proposals를 추출한다. Non maximum suppression을 적용한 후, class score 상위 N개의 anchor box를 추출한다. 
* Input : 22500 anchor boxes, class scores, bounding box regressors
* Process : region proposal by proposal layer 
* Output : top N-ranked region proposals

5) Anchor target layer

RPN이 학습하는데 사용할 수 있는 anchor를 선택한다. (2)에서 생성한 anchor box중에 원본 이미지의 경계를 벗어나지 않는 anchor box를 선택하고 객체가 존재하는 foreground는 positive, 객체가 존재하지 않는 background는 negative로 데이터를 sampling한다.

전체 anchor box중에서 ground truth box와 가장 큰 IoU를 가지는 경우와 IoU가 0.7 이상인 경우 positive, 0.3 이하인 경우는 negative로 선정한다.(0.3~0.7 IoU는 무시)

* Input : anchor boxes, ground truth boxes
* Process : RPN에 사용할 수 있는 anchor 선택
* Output : positive/negative samples with target regression coefficients

6) Proposal Target layer

Fast RCNN을 학습시키기 유용한 sample을 선택한다. region proposals들과 ground truth box와의 IoU를 계산해서 0.5이상이면 positive, 0.1~0.5일 경우 negative sample로 label한다.

* Input : top N-ranked region proposals, ground truth boxes
* Process : select region proposals for training Fast RCNN
* Output : positive/negative samples with target regression coefficients

7) RoI Pooling

Fast RCNN에서 진행했던 Max Pooling을 이용한 RoI Pooling을 진행한다.

* Input : 50x50x512 feature map, positive/negative samples with target regression coefficients
* Process : RoI Pooling
* Output : 7x7x512 feature map

8) Training Fast RCNN by Multi-task loss

나머지는 Fast RCNN의 동작 순서와 동일하게 진행된다. 입력 받은 feature map을 fc layer에 주입해서 4096크기의 feature vector를 얻고, 이를 Classifier와 BBR에 주입해서 (K개의 클래스라고 할 때) 각각 (K+1), (K+1)x4 크기의 feature vector를 출력한다. 출력된 결과를 사용해서 Fast RCNN을 학습시킨다.

* Input : 7x7x512 feature map
* Process : feature extraction, classification, BBR, train Fast RCNN by mulit task loss
* Output : loss (Log loss + Smooth L1 loss)

### 5. Alternating Training
전체 모델을 한번에 학습 시키기에는 굉장히 어렵다. RPN이 제대로 RoI를 계산하지 못하는데 뒤의 Classification layer가 제대로 학습되기는 힘든것처럼 말이다. 그래서 저자는 4단계에 걸쳐서 RPN과 Fast RCNN을 번갈아서 학습시키는 Alternating Training 기법을 사용한다.

![ffrcnn8](https://user-images.githubusercontent.com/77332628/206435618-eb433d70-e9fb-4aa5-a255-40ec2686b555.png)

1. 먼저 Anchor generation layer에서 생성된 anchor box와 ground truth box를 사용해서 Anchor target layer에서 positive/negative 데이터셋을 구성하고 이를 활용해서 **RPN을 학습**시킨다. 이 과정에서 VGG16도 학습된다.
2. Anchor generation layer에서 얻은 anchor box와 학습된 RPN에 원본 이미지를 중비해서 얻은 feature map을 사용해서 proposals layer에서 region proposals를 추출하고 이를 Proposal traget layer에 전달해서 positive/negative 데이터 셋을 구성하고 이를 활용해서 **Fast RCNN을 학습**시킨다. 이때도 VGG16이 학습된다.
3. (1),(2)에서 학습시킨 RPN과 Fast RCNN에서 **RPN에 해당하는 부분한 fine tuning**시킨다. 이 과정에서는 두 네트워크끼리 공유하는 VGG16은 동결(freeze)시킨다.
4. (3)에서 학습시킨 RPN을 활용해서 추출한 region proposals를 이용해서 **Fast RCNN을 fine tuning시킨다.** 이때 RPN과 VGG16은 동결시킨다.

위의 과정을 쉽게 생각하면 RPN과 Fast RCN을 번갈아서 학습시키면서 공유된 convoluiton layer를 사용한다고 생각하면 된다.

### 6. Inference(Detection)

![ffrcnn9](https://user-images.githubusercontent.com/77332628/206435620-67e11c27-e756-419f-8dd5-f17e9b658cbe.png)

inferene 시에는 네트워크를 학습시키기 위한 데이터셋을 구성하는 layer인 Anchor target layer와 Proposal target layer는 사용하지 않는다. Inference는 Proposal layer에서 추출한 region proposal을 활용해서 Fast RCNN이 수행한다. 그리고 최종적으로 얻은 predicted box에 Non maximum suppression을 적용해서 최적의 bounding box만을 결과로 출력한다.

