---
title : '[OD/개념] 객체 탐지 - Mask R-CNN 🎭'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## Mask R-CNN 논문 읽어보기

이번 글에서는 Mask R-CNN 논문을 읽고 리뷰해보도록 하겠다. Mask R-CNN은 object detection 보다는 instance segmantation task에 주로 사용되는 모델이다. 지금까지 대부분의 글들에서 segmantation에 대해서 다뤄본 적이 거의 없으니 Instance Segmentation에 대해서 간단히 알아보자.

### 0. Instance Segmantation

![0](https://user-images.githubusercontent.com/77332628/218316087-608cbbd2-586a-42a1-9568-efc9c5bfc9b6.jpeg)

위 이미지에서 (c)는 Semantic segmantation으로 픽셀 단위로 객체를 구분하는 것인데, 동일하게 분류된 카테고리의 instance는 구분하지 않는다. 반면에 (d) 이미지는 Instance segmantation으로 같은 카테고리로 분류된 객체더라도 분리해서 탐지한다. 이는 object detection과 각각의 픽셀의 카테고리를 분류하는 semantic segmantation이 결합된 것이라고 볼 수 있다.

### 1. Mask branch

![2](https://user-images.githubusercontent.com/77332628/218316091-ef3ea8aa-c3ea-4db0-9466-b3b15603167e.jpeg)

Mask R-CNN 모델에 대해서 자세히 알아보기 전에 개략적인 모델의 구조를 알아보자. Mask R-CNN은 Faster R-CNN의 RPN에서 얻은 RoI에 대해서 class classification branch, bbox regression branch에 segmantation mask를 예측하는 **mask branch가 추가**된 구조라고 생각하면 된다. 위 이미지처럼 mask branch는 각각의 RoI에 작은 크기의 FCN(Fully Convolutional Network)이 추가된 형태이다. 

![3](https://user-images.githubusercontent.com/77332628/218316094-619e0740-8b93-4469-a58e-a5c4f1748d70.png)

Fster R-CNN과 Mask R-CNN의 차이점을 알아보기 위해 Faster R-CNN의 구조를 다시한번 살펴보자. Faster R-CNN은 위 이미지와 같이 backbone network에서 얻은 feature map을 RPN에 입력해서 RoI를 얻고, RoI pooling을 통해 고정된 크기의 feature map을 얻고 이를 fc layer에 입력한 후 classification branch와 bbox regression branch에 입력해서 class label과 bbox offset의 두 가지 결과를 도출한다.

![4](https://user-images.githubusercontent.com/77332628/218316096-d80ff683-1bbf-4bcf-9af0-9532ec52ef7c.png)

Mask R-CNN은 Faster R-CNN의 두 branch와 평행으로 segmantation mask를 예측하는 mask branch가 추가된 구조이다. (*참고로 위 이미지는 정식 Mask R-CNN 구조가 아니다. Mask branch가 추가된 것을 시각적으로 이해하기 위한 이미지다.*) Mask R-CNN은 mask branch가 추가된 것 외에도 RoI poolin 대신 **RoI align**을 통해 추출한 feature map을 각 branch에 입력해서 class label, bbox offset과 segmantation mask를 얻는다. 여기서 segmantation mask는 class에 따라 분할된 이미지 조각(segment)이다. (RoI align은 뒤에서 다룬다.)

segmantation task는 픽셀 단위로 class를 분류하는 detection task보다 더 정교한 작업이기 때문에 spatial layout(공간에 대한 배치 정보)를 필요로 한다. 이를 위해 mask branch는 여러 개의 conv layer로 구성된 작은 FCN의 구조를 띈다.이를 통해 class label이나 bbox offset과 달리 mask는 이미지 내 객체에 대한 공간 정보를 효과적으로 encode하는 것이 가능하다.

![5](https://user-images.githubusercontent.com/77332628/218316099-73cbb2d8-b22f-4092-8aaf-1267a646cfa2.jpeg)

mask branch는 각각의 RoI에 대해 class별로 binary mak를 출력한다. 기존의 instance segmentation 모델은 하나의 이미지에서 여러 class를 예측한 반면, Mask R-CNN은 class 별로 mask를 생성한 후 픽셀이 해당 class에 해당하는지 안하는지의 여부만 표시한다. 이는 classification branch가 의존하는 기본과 방식과 반대이다. 즉 mask branch와 classification branch를 분리시켰다는 것이다. 논문에서는 두 branch를 결합할 경우 성능이 크게 하락했다고 한다.


![00](https://user-images.githubusercontent.com/77332628/218316089-5feffa11-c82a-44da-a3e6-0565f8799b7c.png)
 

mask branch는 최종적으로 $Km^2$ 크기의 feature map을 출력한다. 여기서 m은 feature map의 크기이고, K는 class수를 의미한다. 위 이미지에서 14x14x80 크기의 feature map을 출력했는데, 논문에서 COCO 데이터셋을 이용했기 때문에 80개의 class를 가져서 K=80이고 m=14이다. 이후 post-processing 과정이 있는데, 이는 뒷 부분에서 다루겠다.




### 2. RoI Align

RoI pooling을 사용하면 입력 이미지 크기와 상관없이 고정된 크기의 feature map을 얻을 수 있다는 이점이 있다. 하지만 논문에서는 RoI pooling으로 얻은 feature map과 RoI 사이가 어긋나는 **misalignment**가 발생한다고 주장한다. 이 misalignment는 pixel mask를 예측하는데 매우 안좋은 영향을 끼친다고 한다. 그래서 논문에서는 RoI pooling 대신 **RoIAlign**이라는 방법을 제시한다. 

![6](https://user-images.githubusercontent.com/77332628/218316101-03a62f13-454a-4b91-9894-61e1a245a76b.png)

먼저 RoI pooling의 방식을 살펴보자. 위 이미지의 경우 RoI의 크기는 145 x 200이고, feature map은 16x16이다. 이는 sub sampling ratio(=32)에 맞게 나눠주면 projection을 수행한 feature map은 약 4.53 x 6.25의 크기를 가지게 된다. 픽셀값은 정수값을 사용하기 때문에 4x6 크기의 feature map을 얻게 된다. 이후 3x3의 고정된 크기의 feature map을 얻기 위해 4x6 크기의 RoI에 대해서 RoI pooling을 수행한다. 이 과정에서 고정된 크기에 맞추기 위해 stride는 1x2로 설정된다. 이를 통해 최종적으로 3x3 크기의 feature map이 출력된다.

![7](https://user-images.githubusercontent.com/77332628/218316104-31f67455-1dd9-4a1b-ad8b-09a37d330eb5.png)

논문에서는 RoI pooling이 quantization을 사용하기 때문에 misalignment를 유도한다고 한다. quantization은 실수 입력값을 정수와 같은 이산 수치(discrete value)로 제한하는 방법이다. 위 그림처럼 quantization으로 인해서 RoI pooling시에 정보가 소실된다. 왼쪽 이미지처럼 RoI projection을 수행하는 과정에서 초록색과 파란색 영역에 대한 정보가 손실된다. 추가적으로 오른쪽 그림에서처럼 RoI pooling시 stride를 반올림하게 되면서 feature map의 마지막 row(하늘색)에 대한 정보도 소실된다.

quantization으로 인한 misalignment는 translation invariant한 classification task에는 큰 영향이 없기 때문에 Faster R-CNN에서는 별 문제가 없었지만, pixel 단위로 mask를 예측하는 segmentation task에는 큰 악영향을 미친다. 논문에서는 RoIAlign 방법으로 이를 해결한다. RoIAlign은 다음과 같이 동작한다.

![8](https://user-images.githubusercontent.com/77332628/218316107-45b6b4b7-3394-43ec-a039-f6ab59f624b6.png)

1. RoI projection을 통해 얻은 feature map을 quantization 없이 그대로 사용한다. 그 다음 출력할 feature map의 크기에 맞게 projection된 feature map을 분할해준다. 위 예시에서는 3x3 feature map을 출력할 것이기 때문에 width와 height를 각각 3등분 해준다.

2. 분할된 하나의 cell에서 4개의 sampling point를 잡는다. 이는 cell의 height와 width를 각각 3등분하는 점에 해당한다.

3. Bilinear interpolation을 통해서 sampling point의 값을 찾는다. 

![9](https://user-images.githubusercontent.com/77332628/218316108-1a37bbf7-b55d-425f-956a-c9eacab8bde4.png)

이를 구하는 공식은 다음과 같다.

![10](https://user-images.githubusercontent.com/77332628/218316109-7d529b90-d730-4b59-a781-8dd597cf0c6e.png)

위 공식에서 $x,x_1,x_2,y,y_1,y_2$는 2)과정에서 얻은 4개의 sampling point 좌표이며, $Q_{11},Q_{21},Q_{12},Q_{22}$는 sampling point에 인접한 cell의 값이다. 위 공식에 따라 입력된 feature의 각각 RoI bin의 4개의 sampled location을 연산한다.

4. 위 과정을 모든 cell에 반복해서 적용한다.

![11](https://user-images.githubusercontent.com/77332628/218316111-59c5ae39-1530-48b1-901c-de45abee3095.png)

5. 마지막으로 하나의 cell에 있는 4개의 sampling point에 대해 max pooling을 수행한다.

![12](https://user-images.githubusercontent.com/77332628/218316113-73d6db8f-f2d5-492f-833f-499ff366f80c.png)

위 과정의 RoIAlign을 통해서 feature와 RoI 사이가 어긋나는 misalignment를 해결하고 결과적으로 RoI의 정확한 spatial location을 보존하는 것이 가능해진다. 이 변화를 통해서 mask accuracy가 크게 향상된다고 한다.

### 3. Loss function

![13](https://user-images.githubusercontent.com/77332628/218316114-3469ab02-e01d-4702-a9c3-53e5995c5ae2.png)

Mask R-CNN은 위와 같이 구성된 multi-task loss function을 통해서 네트워크를 학습시킨다. $L_{cls}$와 $L_{box}$는 각각 classification과 bounding box loss로 Faster R-CNN과 동일하다. $L_{mask}$는 mask loss로 **binary cross entropy** loss이다. mask branch에서 출력한 $Km^2$ 크기의 feature map의 각 cell에 sigmoid function을 적용해서 loss를 구한다.

기존의 segmantation 모델은 픽셀별로 서로 다른 class를 softmax loss function을 사용했지만, Mask R-CNN은 class branch와 mask branch를 분리해서 class별로 mask를 생성하기 때문에 binary loss를 구한다.



### 4. Training Mask R-CNN

1) **Input image Pre-processing**

원본 이미지가 width, height 중 더 짧은 쪽(shorter edge)의 길이인 800 pixel로 resize된다. 

* Input : image
* Process : image pre-processing
* Output : resized-image

2) **Feature extraction by backbone network**

![14](https://user-images.githubusercontent.com/77332628/218316115-477d8b6a-1b85-4955-8ef4-e2d2d5b2578d.jpeg)

전처리된 이미지를 ResNet-FPN backbone network에 입력해서 feature pyramid {P2, P3, P4, P5}를 얻는다. (FPN에 대한 자세한 설명은 [<U>FPN 논문 리뷰</U>](https://hamin-chang.github.io/cv-objectdetection/fpn/)를 참고하길 바란다.

* Input : resized image
* Process : constructing feature pyramid
* Output : feature pyramid {P2, P3, P4, P5}

3) **Region proposal by RPN**

(2)에서 얻은 feature pyramid별로 RPN에 입력해서 objectness score과 bbox regressor를 가진 Region proposal을 출력한다. 

(*이 과정에서 Anchor generation layer가 생략되어 있다.* 자세한 과정은  [<U>Faster R-CNN논문 리뷰</U>](https://hamin-chang.github.io/cv-objectdetection/ffrcnn/)를 참고하길 바란다.)

* Input : feature pyramid {P2, P3, P4, P5} 
* Process : Region proposal
* Output :  Region proposals with objectness score and bbox regressor per feature pyramid {P2, P3, P4, P5}

4) **Select best RoI by Proposal layer**
 
그 다음 RPN으로 얻은 RoI 중 최적의 RoI를 선정한다. 이 과정은 Faster R-CNN의 Proposal layer, Anchor target layer, Proposal target layer에서 수행하는 과정이다.

1. objectness score가 높은 상위 K개의 anchor를 설정한다.
2. bbox regressor에 따라 anchor box의 크기를 조정하고, 이미지 경계를 벗어나는 anchor box는 제거한다.
3. threshold = 0.7로 지정해서 NMS를 수행한다.
4. 1~3의 과정은 각각의 feature pyramid level별로 수행되었다. 모든 feature pyramid level의 anchor box에 대한 정보를 concatenate해준다.
5. 결합된 모든 anchor box에 대해서 objectness score에 따라 상위 N개의 anchor box를 선정해서 이를 학습에 사용한다.

* Input : Region proposals 
* Process : selecting top-N RoIs
* Output : top-N RoIs

5) **RoI Align**

backbone network를 통해 multi-scale feature map {P2,P3,P4,P5}가 생성되기 때문에 RoI를 어떤 scale의 feature map과 매칭시킬지를 결정해야한다. 아래의 공식을 통해서 이를 결정한다. 그리고 나서 RoIAlign을 수행해서 feature map을 출력한다.

![14](https://user-images.githubusercontent.com/77332628/218316116-5b0e3b49-a26d-4dfc-a5f5-5b00d8b161aa.png)

* Input : feature pyramid, RoIs 
* Process : RoIAlign
* Output : 7x7 sized feature map

6) **Classification and Bounding box regression** 

![15](https://user-images.githubusercontent.com/77332628/218316117-953d3f58-c990-4f42-b5c7-55982223a122.png)

* Input : 7x7 sized feature map
* Process : classification by classification branch, bbox regressor by bbox regression branch
* Output : class scores and bbox regressors





7) **Mask segment by Mask branch**

![17](https://user-images.githubusercontent.com/77332628/218316123-6d73ea68-b335-4a3d-b479-1276b78f6c88.jpeg)
![18](https://user-images.githubusercontent.com/77332628/218316125-e349bc6c-020c-4b50-90e0-3b99b0045185.jpeg)

mask branch는 3x3 conv - ReLU - deconv(by 2) - 1x1(xK) conv layer로 구성되어 있다. 여기서 K는 class 수이다. 이를 통해 14x14(xK) 크기의 feature map을 얻을 수 있다. 해당 feature map은 class 별로 생성된 binary mask이다. 다음 이미지에서처럼 backbone model에 따라 다른 head를 사용한다.

![16](https://user-images.githubusercontent.com/77332628/218316120-b1c82b1a-8e98-49e0-9274-f88b0f16d719.png)

14x14(xK) feature map 중 앞서 classification branch에서 얻은 가장 높은 score의 class에 해당하는 feature map을 선정해서 최종 prediction에 사용한다. 즉, 단 하나의 14x14 feature map이 선정된다는 것이다. 이후 feature map의 각 cell별로 sigmoid 함수를 적용해서 0~1 사이의 값을 출력하도록한다.

* Input : 7x7 sized feature map  
* Process : mask segment by mask branch
* Output : 14x14 sized feature map

8) **Post-processing of masks**

![19](https://user-images.githubusercontent.com/77332628/218316127-174ecd27-6db6-4bab-a0f2-33302b629c15.jpeg)


최종적으로 선정된 14x14 feature map을 원본 이미지의 mask와 비교하기 위해 원본 이미지에 맞게 rescale해준다. 이후 threshold(=0.5)에 따라 mask segment의 각 픽셀값이 0.5 이상인 경우 class에 해당하는 객체가 있어 1을 할당하고, threshold 미만의 경우 0을 할당한다.

* Input : 14x14 sized feature map
* Process : rescale and apply mask threshold
* Output : mask segment

9) Train by multi-task loss

위에서 언급한 multi-task loss function으로 Mask R-CNN을 학습시킨다.



### 5. Inferene & 결론

Inference 시에 Proposal layer 구간에서 모든 feature pyramid level에 걸쳐 상위 1000개의 RoI만을 선정한 후 RoI를 classification branch와 bbox regression branch에 입력하여 나온 예측 결과에 Non maximum suppression을 적용하고, 상위 100개의 box만을 선정하여 mask branch에 입력한다. 이러한 추론 과정은 학습과정처럼 3개의 branch가 평행하지는 않지만, inference 시간을 줄여주고 정확도가 더 높게 나온다는 장점이 있다고 한다.


Mask R-CNN은 ResNeXt-101-FPN을 backbone network로 사용하여 COCO 데이터셋을 학습에 사용한 결과, AP값이 37.1%를 보였다. Instance segmentation에 대해 다루는 모델이어서 조금은 어려워 보였지만 결국 object detection을 하는 원래의 Faster R-CNN 모델에 semantic segmentation 역할을 하는 mask branch만 추가한 것이라는 것을 알고 나서는 쉽게 이해한것 같다.

출처 및 참고문헌 

개인 블로그1 (http://herbwood.tistory.com/20)

개인 블로그2 (https://blahblahlab.tistory.com/139)

Mask R-CNN 논문 (https://arxiv.org/pdf/1703.06870.pdf)
