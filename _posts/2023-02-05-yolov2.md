---
title : '[DL/CV] 객체 탐지 - YOLO v2 🤟'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## YOLO v2 논문(YOLO9000 : Better, Faster, Stronger) 읽어보기

이번 글에서는  YOLO v2 논문(YOLO9000:Better, Faster, Stronger)[[**링크**](https://arxiv.org/pdf/1612.08242.pdf)]를 리뷰해보도록 하겠다. 300x300의 이미지를 사용하는 SSD300 모델은 detection 속도는 빠르지만 정확도가 낮고, SSD512 모델은 정확도는 높지만 느리다는 단점이 있다. YOLO v2 모델은 다음의 3가지 아이디어를 도입해서 정확도와 속도 사이의 trade-off의 균형을 잘 맞춘 모델이다. 

* Better : 정확도를 올리기 위한 기법
* Faster : detection 속도를 향상시키기 위한 기법
* Stronger : 더 많은 범위의 class를 예측하기 위한 기법

### 1. Better

1) **Batch Normalization**

논문에서는 모든 conv layer 뒤에 batch normalization 층을 추가해서 mAP 값을 2% 정도 향상시켰을 뿐만 아니라, overfitting 없이 기타 정규화나 dropout을 제거했다.

2) **High Resolution Classifier**

YOLO v1 모델은 Darknet을 224x224 크기의 이미지로 pre-train 시키고 detection task 시에는 448x448 크기의 이미지를 입력으로 사용했다. 이는 network가 object detection task를 학습하는 동시에 새로운 입력 이미지의 resolution(해상도)에 적응해야 함을 의미한다.

YOLO v2 모델에서는 Darknet을 처음부터 448x448 이미지로 pre-train 시켜서 네트워크가 상대적으로 높은 해상도의 이미지에 적응할 시간을 제공해서 mAP 값을 4% 정도 향상시켰다.

3) **Convolutional with Anchor boxes**

YOLO v1 모델은 각 grid cell의 bounding box의 좌표가 0~1 사이의 값을 가지도록 랜덤으로 설정한 뒤 학습을 통해 최적의 값을 찾아가는 과정을 거친다. 반면에 Faster R-CNN 모델은 사전에 9개의 anchor box를 정의하고 bounding box regression을 통해서 offset을 조정하는 과정을 거친다. 두 방법을 비교하자면 **좌표 대신 offset을 예측하는 문제가 더 단순하고 모델이 학습하기 쉽다.**

이러한 아이디어를 이용해서** YOLO v2에서는 anchor box를 도입**하고, 이 과정에서 network를 수정한다. 먼저 conv layer의 output이 보다 높은 resolution을 가지도록 pooling layer를 제거한다. 그리고 앞서 448x488의 입력 이미지를 사용한다고 했지만, 네트워크를 줄여서 **416x416의 입력 이미지를 사용**한다. 이는 보통 객체의 크기가 크면 이미지 내에서 하나의 중심 cell (single center cell)이 존재하면 이를 더 잘 포착할 수 있기 때문에 입력 이미지 크기를 416x416으로 수정해서 최종 output feature map의 크기가 홀수가 되도록 하기 위해서이다. 

416x416의 입력 이미지를 사용하면 downsample ratio = 1/32를 사용하기 때문에 최종적으로 13x13 크기의 feature map을 얻을 수 있다. anchor box를 도입하게 되면 YOLO v1는 cell별로 2개의 bounding box를 예측해서 총 98(=7x7x2)개의 bounding box를 예측하지만 YOLO v2는 v1보다 많은 수의 bounding box를 예측하게 된다. anchor box를 사용하면 사용하지 않은 경우보다 mAP값은 낮은 값을 보이지만 recall 값이 상승한다. 이는 모델이 더 향상될 여지가 있음을 나타낸다고 한다.

[ **Object detection task에서 recall 값이 높다는 것은 모델이 실제 객체의 위치를 예측할 비율이 높음을 의미한다. YOLO v2에서는 v1보다 더 많은 bounding box를 예측하기 때문에 더 높은 recall 값을 보인다.** ] 


4) **Dimension Clusters**

Faster R-CNN에서는 anchor box의 크기와 aspect ratio를 사전에 미리 정의(hand-picked) 했었다. 하지만 논문의 저자는 다른 방법으로 사전 조건 (**prior**)를 선택했다. 이를 해서 **k-means clustering**기법을 사용했는데, 이 때 데이터셋에 있는 모든 ground truth box의 width, height 값을 사용해서 k-means clustering 수행한다. 일반적인 k-means clustering은 유클리안 거리를 이용해서 centroid와 sample 간의 거리를 계산한다. 하지만 이렇게하면 큰 bounding box는 작은 box에 비해 큰 error를 발생시키는 문제가 생긴다. 그래서 논문의 저자는 다음의 새로운 distance metric을 사용한다. 

![1](https://user-images.githubusercontent.com/77332628/216801342-43afe18d-096e-4b99-a1cf-4fb86ba6b9ad.png)

이는 bounding box와 centroid의 IoU 값이 클수록 거리가 가깝다는 점을 나타낸다.

![2](https://user-images.githubusercontent.com/77332628/216801344-5aefa3f3-449c-4c97-936f-55f132a2ce6e.png)

논문의 저자는 다양한 수의 $k$(centroid의 수이자 anchor box의 수)에 대하여 k-means clustering을 수행한 결과 $k=5$ 일 때 복잡도와 recall 값이 적절한 trade-off를 보여주었다는 결론을 내린다.  hand-picked anchor box 9개를 사용할 경우 평균 IoU값이 61.0이며, lustering 전략을 사용하여 뽑은 prior 5개를 사용할 경우 평균 IoU 값은 60.09이다. 이는 5개의 anchor box만으로도 최적의 prior를 선택하면 네트워크가 detection task를 보다 쉽게 학습함을 의미한다.

5) **Direct location prediction**

![3](https://user-images.githubusercontent.com/77332628/216801345-510e9054-f5be-4b07-841d-01ea45075d69.png)

YOLO에 anchor box를 도입했을 때 문제점은 초기 iteration에서 모델이 불안정하다는 것이다. anchor box는 bounding box regression 계수를 통해 위의 공식으로 bounding box의 위치를 조정한다. 하지만 $t_x,t_y$와 같은 계수는 제한된 범위가 없기 때문에 anchor box는 이미지 내의 임의의 지점에 위치할 수 있다는 문제점이 있기 때문에 최적화된 값을 찾기까지 오랜 시간이 걸려서 모델 초기에 불안정하게 된다.

![4](https://user-images.githubusercontent.com/77332628/216801347-ae3cf28c-9e87-470a-a7fd-1e3de13f83a0.png)

논문의 저자는 이를 해결하기 위해 YOLO v1의 grid cell에 상대적인 위치 좌표를 예측하는 방법을 사용했다. 이는 예측하는 bounding box의 좌표는 0 ~ 1 사이의 값을 가짐을 의미한다. 위 이미지에서 $c_x,c_y$는 grid cell의 좌상단 offset이다. bounding box regression을 통해 얻은  $t_x,t_y$ 값에 logistic regression 함수$(σ)$를 적용해서 0 ~ 1 사이의 값을 가지도록 조정했다.


예측하는 위치의 범위가 정해졌기 때문에 안정적으로 network가 학습을 할 수 있게 되었고, Dimension clustering을 통해 최적의 prior를 선택하고 anchor box 중심부 좌표를 직접 예측함으로서 recall 값을 5% 정도 향상시켰다고 한다.

6) **Fine-Grained Features**

YOLO v2는 최종적으로 13x13의 feature map을 출력한다. 하지만 이처럼 feature map의 크기가 작으면 큰 객체를 예측하기는 용이하지만 작은 객체는 예측하기 어렵다는 문제가 있다.

![5](https://user-images.githubusercontent.com/77332628/216801348-1b36d715-45aa-48b8-bd75-a381de7f564b.png)

이 문제를 해결하기 위해서 마지막 pooling을 수행하기 전 26x26(x512) 크기의 feature map을 추출한다. 그 다음 위 이미지처럼 feature map을 channel은 유지하면서 4개로 분할한 후 concat해서 13x13(x2048) 크기의 feature map을 얻는데, 이러한 feature map은 보다 작은 정보를 함축하고 있다. 다음 이미지처럼 이를 13x13(x1024) feature map에 추가해서 13x13(x3072) 크기의 feature map을 얻는다.

![6](https://user-images.githubusercontent.com/77332628/216801349-69c95914-ca80-46f6-91f7-a3a6f609b573.png)

최종적으로 3x3 conv와 1x1 conv를 적용해서 13x13(x125) 크기의 feature map을 얻는다. 여기서 channel 수 125는 (5개의 bounding box) x (20개의 class score + (confidence,x,y,w,h))로 얻어진다. 이처럼 fine-grained feature를 사용해서 1%정도의 성능 향상을 이뤄냈다고 한다.

7) **Multi-Scale Training**

![7](https://user-images.githubusercontent.com/77332628/216801350-0c2cf6f4-5baa-4bc6-bcbb-779233cdeee7.png)

논문의 저자는 YOLO v2 모델을 보다 강건하게 만들기 위해서 다양한 입력 이미지를 사용해서 모델을 학습시킨다. 논문에서는 10 batch마다 입력 이미지의 크기를 랜덤하게 선택해서 학습시켰다. 모델은 downsample ratio = 1/32로 사용하기 때문에 입력 이미지의 크기는 32의 배수 {320, 352, ..., 608} 중에서 선택하도록 설계했다. 가장 작은 입력 이미지는 320x320이며, 가장 큰 이미지는 608x608이다.

이를 통해서 모델은 다양한 크기의 이미지를 입력받을 수 있고, 속도와 정확도 사이의 trade-off를 제공한다. 위 표에서 확인할 수 있듯이 입력 이미지의 크기가 작은 경우 더 높은 FPS를 가지며 입력 이미지의 크기가 크면 더 높은 mAP를 가지게 된다.

[FPS : 모델이 초당 detection하는 비율]



### 2. Faster

1) Darknet-19

![8](https://user-images.githubusercontent.com/77332628/216801351-e1a40030-f6a8-4d6a-9f4b-8ae0621f0e55.png)

YOLO v2는 Darknet-19라는 독자적인 classification 모델을 backbone network로 사용한다. Darknet-19의 전체 구조는 위 표와 같다.

YOLO v1 모델은 네트워크 마지막에 fc layer를 통해서 예측을 수행한다. 하지만 fc layer 때문에 파라미터 수가 증가하고 detection 속도가 느려지는 단점이 있다. YOLO v2의 Darknet-19 는 마지막 layer에 **Global average pooling**을 사용해서 fc layer를 제거해서 파라미터 수를 줄이고 detection 속도를 향상시켰다.

![9](https://user-images.githubusercontent.com/77332628/216801359-e1de04e6-3bd5-4770-9074-6e2a1f4bb8ba.png)

2) Training for classification

Darknet-19는 class 수가 1000개인 ImageNet을 통해서 학습시킨다. 위의 Darknet-19 네트워크에서 global average pooling 이후의 output의 수가 1000개인 이유이다. 학습 결과 top-1 정확도는 76.5%, top-5 정확도는 93.3%의 성능을 보였다.

3) Training for detection

Darknet-19를 detection task로 사용하기 위해 마지막 conv layer를 제거하고 3x3(x1024) conv layer로 대체하고, 이후 1x1 conv layer를 추가한다. 이 때 1x1 conv layer의 channel 수는 예측할 때 필요한 수로, 앞서 살펴보았듯이, 각 grid cell마다 5개의 bounding box가 5개의 값(confidence score, x, y, w, h)과, PASCAL VOC 데이터셋을 사용하여 학습하기 때문에 20개의 class score를 예측한다. 따라서 1x1 conv layer에서 channel 수를 125(=5x(5+20))개로 지정한다.






### 3. Stronger

논문에서는 YOLO v2를 classification 데이터와 detection 데이터를 함께 사용해서 학습시킴으로서 보다 많은 class를 예측하는 YOLO 9000을 소개한다. 하지만 세부적인 객체에 대한  classification 데이터와 일반적이고 범용적인 객체에 대한 정보를 가진 detection 데이터를 섞어서 학습시키면, 문제가 발생한다. 예를 들어서 detection 데이터셋은 모든 개 이미지를 "개"라는 하나의 일반적인 class로 분류하는 반면, classification 데이터셋은 "요크셔 테리어", "불독" 등 개를 종류별로 세부적인 class로 분류한다. 이렇게 되면 두 데이터를 섞어서 학습시키면 "개"와 "요크셔 테리어"를 별개의 배타적인 class로 분류할 수도 있는 문제가 발생한다.

1) Hierarchical classification

![10](https://user-images.githubusercontent.com/77332628/216801360-9fea01ac-1d8e-4556-970b-7b0140ae1a9b.png)

논문에서는 이러한 문제를 해결하기 위해서 ImageNet label로부터 **계층적인 트리(Hierarchical tree)인 WordTree**를 구성하는 방법을 제안한다. WordTree에서 각 노드는 범주를 의미하고 하위 범주는 자식 노드가 되는 구조를 가지고 있다. 먼저 물리적 객체(physical object)를 루트 노드로 두고, 하위 범주는 자식 노드가 되고, 자식 노드의 하위 범주는 자식 범주가 되어 점차 뻗어나가는 구조다. 예를 들어 "요크셔 테리어"는 "물리적 객체 - 동물 - 포유류 - 사냥개 - 테리어 - 요크셔 테리어" 노드를 통해서 도달할 수 있다. ImageNet 데이터셋을 통해서 WordTree를 구성하면 최상위 노드부터 최하위 노드까지 총 범주를 합치면 1369의 범주가 존재한다.

![11](https://user-images.githubusercontent.com/77332628/216801361-0bf9913e-95a6-445c-badc-aa72860cf0bd.png)

트리에서 특정 범주에 속할 확률은 루트 노드에서부터 해당 범주의 노드까지의 조건부 확률의 곱으로 표현된다. 예로 입력으로 들어온 이미지가 "요크셔 테리어"일 확률은 위 이미지와 같이 계산한다.

2) Dataset combination with WordTree

WordTree 방법을 통해서 ImageNet 데이터와 COCO 데이터를 합쳐서 WordTree를 구성한다.

![12](https://user-images.githubusercontent.com/77332628/216801363-e6edf67b-38ca-45b0-8660-d60748b1411f.jpeg)

논문에서는 COCO 데이터셋과 ImageNet 데이터셋을 4:1 비율로 합쳐서 9418개의 범주를 가지는 WordTree를 구성한다. YOLO v2 모델을 grid cell별로 3개의 anchor box를 사용해서 학습시킨다.

3) Joint classification and detection

네트워크가 detection 데이터셋의 이미지를 보면 detection loss는 평소와 같이 loss를 backward pass하지만 classification loss의 경우에는 특정 범주와 상위 범주에 대해서만 loss를 계산한다. 예를 들어 "개"라는 범주에 대해 deteciton 데이터셋은 하위 범주를 포함하지 않기 때문에 "물리적 객체 - 동물 - 포유류 - 개"에 대한 조건부 확률만 사용해서 loss를 구한다.

만약 네트워크가 classification 데이터셋의 이미지를 보면 오직 classification loss에 대해서 backward pass를 수행한다. 이 때 ground truth box와의 IoU 값이 0.3 이상인 경우에만 역전파를 수행한다.

이와 같은 **Joint training** 방식을 통해서 YOLO 9000 모델은 COCO 데이터셋을 활용해서 이미지 내에서 객체를 찾는 detection task와 ImageNet 데이터셋을 통해서 보다 넓은 범주의 객체를 분류할 수 있도록 학습된다.





### 4. Training

![13](https://user-images.githubusercontent.com/77332628/216801364-daa0ba70-4bc3-4be7-baf7-0d0af9eddfa4.png)

1) Feature extraction

* Input : 416x416 sized image
* Process : feature extraction by **Darknet-19**
* Output : 13x13(x1024) sized feature map

2) Reorganize feature map



위에서 언급했듯이 13x13 feature map을 예측에 사용하면 작은 객체는 detect하기 어렵기 때문에 이러한 문제를 해결하기 위해서 26x26(x512) 크기의 feature map(위 이미지에서 최우측 빨간색 block 전)을 추출해서 채널은 유지하면서 feature map을 4개로 분할하고 concat해서 13x13(x2048) 크기의 feature map을 얻는다. 이러한 feature map은 **작은 객체에 대한 정보(fine-grained)를 함축**한다.

* Input : 26x26(x512) sized feature map
* Process : Reorganize feature map
* Output : 13x13(x2048) sized feature map

3) Concat feature maps

![14](https://user-images.githubusercontent.com/77332628/216801366-fbc5b948-f1bf-4261-a6a3-6b7848ed59f3.png)

1)에서 얻은 feature map과 2)에서 얻은 feature map을 결합한다.

* Input : 13x13(x1024) sized feature map, 13x13(x2048) sized feature map
* Process : concat feature maps
* Output : 13x13(x3072) sized feature maps

4) Prediction by applying conv layer

3)에서 얻은 feature map에 3x3 conv와 1x1 conv를 적용해서 예측값을 얻는다.

* Input : 13x13(x3072) sized feature maps
* Process : applying 3x3, 1x1 conv
* Output : 13x13(x125) sized feature map




### 5. Loss function

YOLO v2의 전체 loss function은 다음과 같다. YOLO v1과 같이 Localization loss, Confidence loss, Classification loss로 구성되어 있다. 또한 v1과 같이 전체 loss가 SSE(Sum of Squared Error)이다. 

![15](https://user-images.githubusercontent.com/77332628/216801367-e461eea7-8901-4e52-b298-11fa7a045d8f.png)

1) Localization loss

![16](https://user-images.githubusercontent.com/77332628/216801369-ba360e11-8a7b-4ee7-981f-0a7f955c0972.png)

이미지 내 특정 객체의 중심이 특정 grid cell에 있으면, 해당 grid cell은 객체를 예측하도록 할당(responsible for)된다. Localization loss는 객체의 중심이 존재하는 grid cell의 bounding box loss와 존재하지 않는 grid cell의 bounding box loss를 구한 후 더했다.

첫 번째 항은 객체를 예측하도록 할당된 grid cell의 bounding box loss이다.

* $λ_{obj}^{coord}$ : 객체를 포함하는 grid cell에 가중치를 주는 하이퍼 파라미터, 1로 설정한다.
* $S^2$  : grid cell의 수 (=13x13 = 169)
* B : grid cell 별 anchor box의 수 (=5)
* $1^{responsible-obj}_{ij}$ : i번째 grid cell의 j번째 anchor box가 객체를 예측하도록 할당되었으면 1, 그렇지 않을 경우 0인 index parameter
* $x^{pred}_{ij},y^{pred}_{ij},w^{pred}_{ij},h^{pred}_{ij}$: 예측한 anchor box의 x,y,w,h 값
* $x^{obj}_{ij},y^{obj}_{ij},w^{obj}_{ij},h^{obj}_{ij}$: 실제 ground truth box의 x,y,w,h 값

두 번째 항은 객체를 예측하도록 할당되지 않은 grid cell의 bounding box loss인데, 객체를 예측하지 않아도 되는 grid cell에 대해서 loss를 구하는 이유는 무엇일까? 바로 해당 grid cell이 객체를 예측하지 않도록 학습시키기 위함이다. 이는 오직 객체를 예측하도록 할당된 grid cell만이 객체를 예측하도록 학습시킨다는 것을 의미한다.

![17](https://user-images.githubusercontent.com/77332628/216801371-cbdd6e13-b745-45db-bcd6-f8850e4adf32.png)

예를 들어서 위의 이미지에서 객체인 스마일 마크를 detect하는 것이 목적이고, 이를 위해 이미지를 13x13 grid로 분할 했다. 그 중 빨간색 grid cell이 객체의 중심이 위치해있기 때문에 스마일 마크를 찾도록 할당된다. 반면 하늘색 네모에 해당하는 grid cell은 객체를 찾도록 할당되지 않았다. YOLO v2에서는 모든 grid cell은 5개의 anchor box를 예측한다.

하지만 객체를 예측하도록 할당받지 않은 grid cell이 넓은 크기의 anchor box를 예측하는 것은 불필요한 일이다. 따라서 responsible for한 grid cell이 정확하게 객체를 detect하는 것만큼 not responsible for한 grid cell이 객체를 detect하지 않도록 완전히 배제하는 과정도 필요하다. 이를 위해서 **객체를 detect하지 못하도록 anchor box의 위치와 범위를 grid cell과 같게 줄여버리는 방법을 사용한다.** 이는 anchor box의 위치를 grid cell의 정가운데가 되도록, anchor box의 width, height가 grid cell과 같아지도록 학습시키면 된다. 이제 두번째 항에 대한 loss 부분을 살펴보자.

![18](https://user-images.githubusercontent.com/77332628/216801372-56fc00d2-0ff3-4b0a-b0d6-88a6db70c953.png)

* $λ_{noobj}^{coord}$ : 이미지 내에서 할당되지 않은 grid cell의 수는 매우 많지만 객체의 중심을 포함한  grid cell에 비해 중요도가 떨어진다. 이를 조정하기 위해 not resposible for 한 grid cell에 대한 가중치 값으로 , 0.1로 설정한다.
* $x^{anchor-center}_{ij},y^{anchor-center}_{ij}$ : 예측한 anchor box의 위치의 학습 목표로, grid cell 내의 중ㅅ미 위치가 되도록 0.5로 지정한다. x,y 좌표가 grid cell 내 (0.5,0.5)에 위치한 경우 grid cell의 중심이다.
* $w^{anchor-default}_{ij},h^{anchor-default}_{ij}$ : 예측한 anchor box의 크기의 학습 목표로, grid cell 전체 크기가 되도록 1로 설정한다.





2) Confidence loss


![19](https://user-images.githubusercontent.com/77332628/216801374-418b11c7-3800-4bf8-a2c9-1d5925cf962e.png)

* $λ_{obj}^{conf}$ : 객체를 예측하도록 grid cell에 대한 가중치 파라미터, 5로 지정한다.
* $conf^{pred}_{ij}$: $i$번째 grid cell의 $j$번째 anchor box의 confidence score. 실체 객체를 포함한다고 판단할 경우 1, 그렇지 않을 경우 0.
* $iou(box^{pred}_{ij})$ : 예측한 anchor box와 ground truth box와의 IoU 값.
* $λ_{noobj}^{conf}$ : 객체를 예측하도록 할당되지 않은 grid cell에 대한 가중치 파라미터, 0.5로 지정한다.

위 loss 식의 첫 항은 $conf^{pred}_{ij} = 1$이 되고 IoU 값이 1이 될 경우 loss가 가장 적어지고, 반대로 두번째 항은  $conf^{pred}_{ij} = 1$이 되면 loss가 가장 커진다. 이는 not responsible for한 grid cell은 $conf^{pred}_{ij} = 0$이 될 경우, 즉 confidence score가 낮을수록 loss가 낮아짐을 의미한다.

3) Classification loss

![20](https://user-images.githubusercontent.com/77332628/216801375-fdd1f93c-c6e9-436d-ae82-06db072e71cf.png)

* $p^{pred}_{ij}(c)$ : 예측한 class probabilites
* $p^{truth}_{ij}(c)$ : 실제 class probabilites

지금까지 살펴본 loss function에 따라서 YOLO v2 모델을 훈련시킨다.

### 6. 결론

![21](https://user-images.githubusercontent.com/77332628/216801376-a4724aa1-d5ce-4f37-94fb-71e09317cd82.png)


YOLO v2는 mAP 값이 73.4%로, 당시 SOTA 모델인 ResNet, Faster R-CNN과 비슷한 성능 보였지만 detection 속도 측면에서 크게 앞섰다. 위의 그래프를 통해 YOLO v2가 사용하는 입력 이미지의 크기에 따라 정확도와 detection 속도의 trade-off가 발생한 다양한 선택지를 보여준다. 입력 이미지의 크기가 클 경우 정확도가 높아지지만 detection 속도가 느려지고, 입력 이미지가 작을 경우 정확도는 낮지만 detection 속도가 빨라진다. 이는 상황에 맞게 YOLO v2 모델은 문제를 보다 적합하게 해결할 수 있다는 점에서 의의가 있는 모델이라고 볼 수 있다.

출처 및 참고문헌:

YOLO v2 논문 (https://arxiv.org/pdf/1612.08242.pdf)

개인 블로그 (http://herbwood.tistory.com/17)

Loss function 참고 github (https://github.com/leetenki/YOLOv2/blob/master/YOLOv2.md)



