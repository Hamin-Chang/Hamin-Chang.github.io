---
title : '[DL/Pytorch] 파이토치로 학습하기1 - 파라미터 최적화 🌡️'
layout: single
toc: true
toc_sticky: true
categories:
  - pytorchBasic
---

## 파이토치 모델이 학습하는 과정\(파라미터 최적화\)

이번 글에서는 파이토치 모델이 학습을 하는 과정에 대해서 다룬다. 간단한 예제를 풀어가면서 파이토치 모델이 어떤 과정을 통해 학습을 하는지 알아보자.

### 1. 온도문제

예를 들어보자. 우리가 어디로 여행을 가서 그곳의 기온을 알아보기 위해 온도계를 확인해봤는데 그 온도계는 단위가 적혀있지 않은 온도계여서 정확한 온도를 알지 못한다. 이 온도 문제를 간단한 파이토치 모델을 만들어서 풀어보자. 우리가 측정한 단위를 모르는 온도에 대응하는 섭씨 온도를 얻어왔다.




```python
import torch
t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0] # 섭씨 온도
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # 미지의 온도
t_c = torch.tensor(t_c) # 리스트를 텐서로 변환
t_u = torch.tensor(t_u)
```

t_c 와 t_u가 선형적인 관계를 가진다고 가정하고 선형 모델을 선택해서 문제를 풀어보자. 즉, t_u에 어떤 값을 곱하고 상수를 더하면 t_c를 얻을 수 있다고 가정하고 선형 모델을 선택한다. 

t_c = t_u * w + b (w=가중치, b=편향값)

이제 w,b는 우리 선형 모델의 파라미터이다. 따라서 모델이 일련의 계산 과정을 거쳐가며 최적의 파라미터를 찾아가는 과정을 '학습'이라고 볼 수 있다. 여기서 최적의 파라미터란 측정된 값(t_c)와 예측값 사이의 오차를 최대한 작게 만드는 파라미터를 칭한다. 

#### 1.1 손실을 줄이는 방법

**손실 함수**는 학습 과정을 통해 최소화하고자 하는 값을 계산하는 함수다. 손실 함수는 일반적으로 훈련 샘플로부터의 출력값과 실제 정답값 사이의 차이를 계산한다(온도 문제에 경우 t_p - t_c). 손실함수는 항상 양수값이 나오게 해서 t_p가 t_c로 맞춰가는 데 사용할 수 있게 해야한다. 이를 위해서 두 값의 차이값의 절대값 혹은 제곱값을 사용한다. 그럼 절댓값과 제곱값 그래프 중 어떤 손실함수를 사용해야 할까? 아래 그림을 보면 절댓값(파랑) 그래프는 제곱값(빨강) 그래프와 달리 우리가 수렴하고자 하는 경우에 미분 값을 정의할 수 없기 때문에 제곱값 손실 함수를 사용한다. 또한 제곱값을 이용한 손실 함수는 절댓값 손실함수보다 잘못된 결과(오차가 큰 결과)에 더 많은 불이익(오차)를 주기 때문에 오차 보정에 우선순위를 주도록 동작한다.

[2022-12-24-pytrain1.md](https://github.com/Hamin-Chang/Hamin-Chang.github.io/files/10298157/2022-12-24-pytrain1.md)

### 2. 파이토치로 문제 풀어보기
이제 모델을 선형 모델을 선택했고, 손실함수는 측정값과 정답값의 차이를 제곱한 텐서를 만든 후 모든 요소에 대한 평균을 구해서 스칼라 값을 만들어내는 **평균 제곱 손실(mean squared error)**을 사용하도록 하자.


```python
def model(t_u,w,b): # 차례대로 입력텐서, 가중치 파라미터, 편향값 파라미터
  return w * t_u + b 

def loss_fn(t_p, t_c): # 차례대로 출력값 , 정답값
  squared_diffs = (t_p - t_c)**2
  return squared_diffs.mean()
```

가중치 파라미터는 1로, 편향값 파라미터는 0으로 초기화하고 모델을 작동 시켜서 출력값을 출력해보고, 손실값을 출력해보자.


```python
w = torch.ones(())
b = torch.zeros(())

t_p = model(t_u,w,b)
t_p
```




    tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,
            48.4000, 60.4000, 68.4000])




```python
loss = loss_fn(t_p,t_c)
loss
```




    tensor(1763.8848)



### 3. 경사하강을 통한 파라미터 최적화
위의 코드에서 출력한 손실값은 굉장히 크다! 이제 본격적인 파라미터 최적화를   **경사 하강 알고리즘**을 사용해서 진행해보자. 경사하강은 각 파라미터와 관련해서 손실의 변화율을 계산해서 손실이 줄어드는 방향으로 파라미터 값을 바꿔나가는 기법이다. 


```python
delta = 0.1 # w와 b가 변하는 정도

loss_rate_of_change_w = (loss_fn(model(t_u,w+delta,b),t_c) - loss_fn(model(t_u,w-delta,b),t_c)) / (2.0 * delta) 

loss_rate_of_change_b = (loss_fn(model(t_u,w,b+delta),t_c) - loss_fn(model(t_u,w,b-delta),t_c)) / (2.0 * delta) 
```

위의 코드는 w와 b값에서 특정 단위만큼 w와 b가 증가했을 때의 손실이 변하게 만든다. 손실값이 줄어들면 w를 더 늘리고, 손실값이 늘어나면 반대로 w를 줄여서 손실을 최소화하는 식이다. 그렇다면 w를 얼마나 늘리거나 줄이면 좋을까? 보통 손실의 변화 비율에 비례해서 w를 바꾸는 방법을 많이 사용한다. 얼마만큼 바꿔갈 것인지에 대한 스케일링 비율을 나타내는 것을 머신러닝에서는 주로 learning_rate라는 변수명을 사용한다.


```python
learning_rate = 1e-2

w = w - learning_rate * loss_rate_of_change_w
b = b - learning_rate * loss_rate_of_change_b
```

위의 코드에서는 delta 값을 0.1로 두었는데, 이는 w와 b에 대한 손실함수의 모양에 따라 달라지기 대문에 인접 영역을 얼마의 거리까지로 볼 것인지 규정하기가 어렵다. 지정한 delta 값에 비해 손실값이 너무 빠르게 변한다면, 손실값을 최소화하기 위해 어던 방향으로 파라미터를 조정할지에 대한 판단을 내리기는 쉽지 않을 것이다. 그렇다면 아래 이미지처럼 인접한 거리(delta)를 극단적으로 줄여보자! 이렇게 하면 결국 파라미터에 대해 손실 함수를 미분하는 것과 같다. 두개 이상의 파라미터를 가진 모델에서는 각 파라미터에 대한 손실 함수의 편미분을 구하고 이 편미분 값들을 미분 벡터에 넣으면 , 이것이 바로 **기울기(gradient)**다.

![train2](https://user-images.githubusercontent.com/77332628/209425879-5a06f029-7ac3-4f7b-9be1-a9e90cd9692e.jpg)

손실함수를 파라미터에 대해 계산하려면 연쇄 법칙을 적용해서 손실함수를 입력값(모델의 출력값)에 대해 미분한 값과 입력(모델의 출력값)을 파라미터에 대해 미분한 값을 곱해서 계산하면 된다.

$dLoss/dw = (dLoss/dt_p) * (dt_p/dw)$

이제 코드로 구현해보자.

원래의 손실 함수는 다음과 같았는데,


```python
def loss_fn(t_p, t_c):
  squared_diffs = (t_p - t_c) ** 2
  return squared_diffs.mean()
```

$dx^2 /dx = 2x$를 이용해서 다음과 같이 손실 함수를 입력값에 대한 미분 값을 구현할 수 있다.


```python
def dloss_fn(t_p,t_c):
  dsq_diffs = 2*(t_p - t_c) / t_p.size(0) # 평균의 도함수로 나눔
  return dsq_diffs
```

우리의 모델은 다음과 같은데,


```python
def model(t_u,w,b): 
  return w * t_u + b 
```

다음과 같이 모델의 출력 값을 파라미터에 대한 미분값을 코드로 구현할 수 있다.


```python
def dmodel_dw(t_u,w,b): # w에 대해 미분
  return t_u

def dmodel_db(t_u,w,b): # b에 대해 미분
  return 1.0
```

위의 코드들을 연쇄법칙으로 엮어서 손실값을 w와b에 대해 미분한 값을 나타내는 함수를 다음과 같이 구현할 수 있다.


```python
def grad_fn(t_u,t_c,t_p,w,b):
  dloss_dtp = dloss_fn(t_p,t_c)
  dloss_dw = dloss_dtp * dmodel_dw(t_u,w,b)
  dloss_db = dloss_dtp * dmodel_db(t_u,w,b)
  return torch.stack([dloss_dw.sum(), dloss_db.sum()])
```

### 4. 모델 적합을 위한 반복
파라미터를 최적화하기 위한 준비가 완료되었으니, 파라미터를 임시값에서 출발해서 고정된 횟수(**에포크 epoch**)만큼 반복해서 파라미터가 변하지 않을때까지 조정한다. 각 에포크마다 훈련(순방향 전달, 손실함수 정의, 역방향 전달, 파라미터 조정)을 하는 훈련 루프를 만드는 함수를 정의해보자.


```python
def training_loop(n_epochs,learning_rate, params,t_u,t_c):
  
  for epoch in range(1, n_epochs+1):
    w,b = params

    t_p = model(t_u,w,b) # 순방향 전달
    loss = loss_fn(t_p,t_c) # 손실함수 정의
    grad = grad_fn(t_u,t_c,t_p,w,b) # 역방향 전달

    params = params - learning_rate * grad # 파라미터 조정
    if epoch % 500 == 0:
      print(f'Epoch {epoch}, Loss {float(loss)}')
      print('    Params:', params)
      print('    Grad:  ', grad)
    
  return params
```

훈련 루프를 정의했으니 훈련 루프를 정의하고 실행하자.


```python
training_loop(n_epochs=100,  # 100번 반복
              learning_rate = 1e-2,
              params = torch.tensor([1.0,0.0]), # 임의로 정한 파라미터 초기값
              t_u = t_u,
              t_c = t_c)
```

    Epoch 10, Loss 9.090110518901907e+34
        Params: tensor([3.2144e+17, 5.6621e+15])
        Grad:   tensor([-3.2700e+19, -5.7600e+17])
    Epoch 20, Loss inf
        Params: tensor([1.3457e+35, 2.3704e+33])
        Grad:   tensor([-1.3690e+37, -2.4114e+35])
    Epoch 30, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 40, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 50, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 60, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 70, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 80, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 90, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])
    Epoch 100, Loss nan
        Params: tensor([nan, nan])
        Grad:   tensor([nan, nan])





    tensor([nan, nan])



이런! 손실값이 폭발하면서 무한대(inf)가 되었다! 이건 params 조정이 너무 크다는 신호이며 이런 경우 조정 값이 점점 커지면서 더 심한 과잉 교정으로 이어진다. 결국 최적화에 수렴하지 않고 발산해 버린다.

![train3](https://user-images.githubusercontent.com/77332628/209425881-34375fcd-0f88-4c13-a310-43f243c9ff06.png)

위의 이미지처럼 learning_rate 값(이미지에선 α값)이 너무 크면 gradient가 폭발해버린다. 그럼 learning_rate을 1e-4 값으로 줄여서 한번 훈련을 진행해보자.


```python
training_loop(n_epochs=100,  
              learning_rate = 1e-4, # 학습률 조정
              params = torch.tensor([1.0,0.0]), 
              t_u = t_u,
              t_c = t_c)
```

    Epoch 1, Loss 1763.884765625
        Params: tensor([ 0.5483, -0.0083])
        Grad:   tensor([4517.2964,   82.6000])
    Epoch 2, Loss 323.09051513671875
        Params: tensor([ 0.3623, -0.0118])
        Grad:   tensor([1859.5493,   35.7843])
    Epoch 3, Loss 78.92963409423828
        Params: tensor([ 0.2858, -0.0135])
        Grad:   tensor([765.4666,  16.5122])
    Epoch 4, Loss 37.5528450012207
        Params: tensor([ 0.2543, -0.0143])
        Grad:   tensor([315.0790,   8.5787])
    Epoch 5, Loss 30.540283203125
        Params: tensor([ 0.2413, -0.0149])
        Grad:   tensor([129.6733,   5.3127])
    Epoch 6, Loss 29.351154327392578
        Params: tensor([ 0.2360, -0.0153])
        Grad:   tensor([53.3495,  3.9682])
    Epoch 7, Loss 29.148883819580078
        Params: tensor([ 0.2338, -0.0156])
        Grad:   tensor([21.9304,  3.4148])
    Epoch 8, Loss 29.113847732543945
        Params: tensor([ 0.2329, -0.0159])
        Grad:   tensor([8.9964, 3.1869])
    Epoch 9, Loss 29.107145309448242
        Params: tensor([ 0.2325, -0.0162])
        Grad:   tensor([3.6721, 3.0930])
    Epoch 10, Loss 29.105247497558594
        Params: tensor([ 0.2324, -0.0166])
        Grad:   tensor([1.4803, 3.0544])
    Epoch 20, Loss 29.09588050842285
        Params: tensor([ 0.2323, -0.0196])
        Grad:   tensor([-0.0531,  3.0268])
    Epoch 40, Loss 29.07756233215332
        Params: tensor([ 0.2324, -0.0256])
        Grad:   tensor([-0.0533,  3.0258])
    Epoch 60, Loss 29.059247970581055
        Params: tensor([ 0.2325, -0.0317])
        Grad:   tensor([-0.0533,  3.0247])
    Epoch 80, Loss 29.04095458984375
        Params: tensor([ 0.2326, -0.0377])
        Grad:   tensor([-0.0532,  3.0236])
    Epoch 100, Loss 29.022666931152344
        Params: tensor([ 0.2327, -0.0438])
        Grad:   tensor([-0.0532,  3.0226])





    tensor([ 0.2327, -0.0438])



다행히 학습률을 줄이니 훈련이 안정적으로 이루어지는 것을 볼 수 있다. 하지만 두가지 문제점이 있다. 하나는 파라미터의 조정이 너무 느리게 이루어지고 있다. 이는 learning_rate를 조정 규모에 따라 변하는 적응형으로 만들어서 해결할 수 있는데, 나중에 다룰것이다. 두번째 문제는 잠재적으로 문제가 될 만한 것인데, 기울기에 대한 문제다. 

#### 4.1 입력 정규화

최적화 작업 중에 에포크 1에서의 grad를 관찰해보면 w에대한 기울기가 b에 대한 기울기에 50배인 것을 볼 수 있다. 즉, 가중치와 편향값의 범위가 다르다는 것이다. 이런 경우에 하나의 파라미터를 업데이트하기 위한 적절한 학습률은 다른 파라미터의 업데이트를 불안정하게 만들 수도 있다. 파라미터 별로 다른 학습률을 주는 식의 세세한 관리를 우리는 원하지 않는다. 그 대신에 입력값을 변경해서 기울기가 서로 큰 차이가 나지 않게 하면 더 쉽게 제어가 가능하다! 입력 값의 번위가 -1과 1 사이를 벗어나지 않도록 바꿔놓으면 되는데, 우리 예제에서는 t_u에 0.1을 곱하면 유사하게 처리가 된다.


```python
t_un = 0.1 * t_u # 정규화한 입력의 변수 명은 t_un
```

정규화된 입력으로 훈련 루프를 돌려보자.


```python
training_loop(n_epochs=100,
              learning_rate=1e-2,
              params=torch.tensor([1.0,0.0]),
              t_u = t_un, # 정규화된 입력 사용
              t_c = t_c)
```

    Epoch 20, Loss 28.157804489135742
        Params: tensor([ 2.3746, -0.3615])
        Grad:   tensor([-0.5093,  2.8832])
    Epoch 40, Loss 26.498987197875977
        Params: tensor([ 2.4747, -0.9280])
        Grad:   tensor([-0.4923,  2.7868])
    Epoch 60, Loss 24.949235916137695
        Params: tensor([ 2.5714, -1.4755])
        Grad:   tensor([-0.4758,  2.6936])
    Epoch 80, Loss 23.501379013061523
        Params: tensor([ 2.6649, -2.0047])
        Grad:   tensor([-0.4599,  2.6035])
    Epoch 100, Loss 22.148710250854492
        Params: tensor([ 2.7553, -2.5162])
        Grad:   tensor([-0.4446,  2.5165])





    tensor([ 2.7553, -2.5162])



학습률을 1e-2로 다시 돌려놔도 파라미터가 발산하지 않는다. Grad 값을 살펴보면 둘 다 비슷한 자릿수로 이뤄졌기 때문에 발산하지 않는 것이 가능했던 것이다. 

이제 학습률을 다시 늘렸으니 params의 변화량이 충분히 작아질 때까지 에포크수를 5000까지 늘려서 다시 훈련 루프를 돌려보자.


```python
params = training_loop(n_epochs=5000,
              learning_rate=1e-2,
              params=torch.tensor([1.0,0.0]),
              t_u = t_un,
              t_c = t_c)

```

    Epoch 500, Loss 7.860115051269531
        Params: tensor([ 4.0443, -9.8133])
        Grad:   tensor([-0.2252,  1.2748])
    Epoch 1000, Loss 3.828537940979004
        Params: tensor([  4.8021, -14.1031])
        Grad:   tensor([-0.0962,  0.5448])
    Epoch 1500, Loss 3.092191219329834
        Params: tensor([  5.1260, -15.9365])
        Grad:   tensor([-0.0411,  0.2328])
    Epoch 2000, Loss 2.957697868347168
        Params: tensor([  5.2644, -16.7200])
        Grad:   tensor([-0.0176,  0.0995])
    Epoch 2500, Loss 2.933133840560913
        Params: tensor([  5.3236, -17.0549])
        Grad:   tensor([-0.0075,  0.0425])
    Epoch 3000, Loss 2.9286484718322754
        Params: tensor([  5.3489, -17.1980])
        Grad:   tensor([-0.0032,  0.0182])
    Epoch 3500, Loss 2.9278297424316406
        Params: tensor([  5.3597, -17.2591])
        Grad:   tensor([-0.0014,  0.0078])
    Epoch 4000, Loss 2.927680253982544
        Params: tensor([  5.3643, -17.2853])
        Grad:   tensor([-0.0006,  0.0033])
    Epoch 4500, Loss 2.9276506900787354
        Params: tensor([  5.3662, -17.2964])
        Grad:   tensor([-0.0002,  0.0014])
    Epoch 5000, Loss 2.927647590637207
        Params: tensor([  5.3671, -17.3012])
        Grad:   tensor([-0.0001,  0.0006])


손실값이 0까지 줄지는 않았지만 이는 에포크 수가 부족했거나 , 데이터가 완전한 선형 관계를 갖지 않거나 우리가 온도 데이터를 눈으로 측정하면서 생긴 노이즈 때문일 것이다. 마지막에 출력된 파라미터를 보면 섭씨 온도를 화씨로 변환하는데 필요한 값과 거의 일치한다! 우리가 측정한 온도계는 화씨 온도를 나타낸다는 것을 알게 되었다! 

### 5. 시각화하기
t_c와 t_u의 값을 그래프에 나타내고, 우리가 최적화를 한 파라미터를 입력한 선형 그래프를 동시에 그려서 우리가 만든 선형 모델이 데이터에 얼마나 적합한지 시각화를 해보자.


```python
%matplotlib inline
from matplotlib import pyplot as plt

t_p = model(t_un,*params) # *params는 params 요소를 개별 인자로 전달한다는 뜻
fig = plt.figure(dpi=200)
plt.xlabel("Temperature (°Fahrenheit)")
plt.ylabel("Temperature (°Celsius)")
plt.plot(t_u.numpy(), t_p.detach().numpy()) # 알 수 없는 원본 값을 그려봄
plt.plot(t_u.numpy(), t_c.numpy(), 'o')
```




    [<matplotlib.lines.Line2D at 0x7fc85f042610>]




    
![train4](https://user-images.githubusercontent.com/77332628/209425882-390a8fe8-b9b1-49f5-9703-f48c7c7fc174.png)
    


이번에는 간단한 온도 예제를 풀기 위한 간단한 선형 모델을 만들고 파이토치로 모델의 파라미터를 최적화하는 알고리즘까지 만들어서 문제를 풀어보았다. 다음 글에서 이어서 온도 문제를 파이토치로 풀어보도록 하겠다.
