---
title : '[DL/CV] 객체 탐지 - SSD 🔫'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## SSD (Single Shot MultiBox Detector) 논문 읽어보기

RCNN 계열의 2-stage detector는 region proposals와 같은 다양한 view를 모델에 제공해서 높은 정확도를 갖지만, region proposals를 추출하고 이를 처리하는 과정에서 모델의 속도가 느려진다는 단점이 있다. 반면 이전 글 ([**링크**](https://hamin-chang.github.io/cv-objectdetection/yolov1/))에서 다룬 YOLO v1은 원본 이미지 전체를 1-stage detector로 처리하기 때문에 속도가 매우 빨랐다. 하지만 YOLO v1은 grid cell별로 2개의 bounding box만 선택해서 상대적으로 적은 view를 모델에 제공하기 때문에 정확도가 떨어진다. 이처럼 정확도와 속도는 서로 trade-off 관계에 있다. 

이번 글에서는 다양한 view를 활용하면서도 통합된 1-stage network 구조를 가지는 SSD 모델을 다룬 논문을 리뷰해보도록 하겠다.

모델의 주요 아이디어들을 알아보자.

### 1. Multiscale feature maps
SSD는 하나의 통합된 network로 detection을 수행하는 1-stage detector이다. 전체 network는 pre-trained VGG 16를 **base network**로 사용하고 이후에 일반적인 conv layer로 구성한 보조(**auxiliary**) network를 추가한 구조를 가지고 있다. base network 후반부에 존재하는 fc layer를 conv layer로 바꿔서 보조 network와 연결해서 detection 속도가 향상되는 이점도 있다. 

<img width="560" alt="1" src="https://user-images.githubusercontent.com/77332628/214980381-7dba1760-037e-4ece-97d1-57a53112c486.png">

이전에 다룬 모델들은 convolution network를 거친 단일한 scale을 가진 feature map을 detection을 위해 사용했었는데, 이러면 다양한 크기의 객체를 포착하는 것이 어렵다는 단점이 있다. 이러한 단점을 해결하기 위해 논문의 저자는 SSD network 중간에 존재하는 conv layer의 feature map들을 추출해서 detection에 사용하는 방법을 제안한다. 이것이 SSD 모델의 핵심 아이디어인 **multiscale feature map** 사용이다.

![2](https://user-images.githubusercontent.com/77332628/214980229-68309703-6f95-4f7f-8f87-d89e1994a748.png)

위의 이미지리를 보면서 feature map들을 살펴보자. 입력 이미지는 300x300 크기다. 먼저 base network conv4_3layer에서 38x38(x512) 크기의 feature map을 추출하고 그 다음으로 base network conv7 layer에서 19x19(x1024) 크기의 feature map을 추출한다. 그리고 보조 network의 conv8_2, conv9_2, conv10_2, conv11_2 layer에서 각각 10x10(x512), 5x5(x256), 1x1(x256) 크기의 feature map을 추출해서 **총 6개의 scale을 가진 feature map**을 추출한다. Multiscale feature maps를 사용하여서 보다 다양한 크기의 객체를 탐지하는 것이 가능해진다.



### 2. Default boxes

SSD 모델에서는 원본 이미지에서 보다 다양한 크기의 객체를 탐지하기 위해 feature map의 각 cell마다 서로 다른 scale과 aspect ratio를 가진 **Default box**를 생성한다. 논문의 저자는 서로 다른 크기의 feature map에 적용한다는 차이점이 있다고 하지만, 이는 Faster R-CNN에서 사용하는 anchor box와 개념적으로 유사하다. SSD 모델은 38x38, 19x19, 10x10, 5x5, 3x3, 1x1 총 6개의 scale feature map의 각 cell마다 default box를 생성한다.

예를 들어서, 원본 이미지 크기가 300x300이고, $s=0.1$이며, aspect ratio가 1:1일 때, default box의 크기는 30x30이다. Default box의 scale, 즉 원본 이미지에 대한 비율을 나타내는 $s_k$를 구하는 식은 다음과 같다.

* $s_k = s_{min} + \frac{s_{max}-s_{min}}{(m-1)}*(k-1)$, $k∈
[1,m]$

* $s_{min}$ = 0.2
* $s_{max}$ = 0.9
* $m$ : 예측에 사용할 feature map의 수, SSD에선 $m=6$

첫 번째 feature map(38x38)의 $s_k=0.2$이며, 두 번째 feature map은 $k=2$이기 때문에 $s_k=0.34$이다. 즉 feature map의 scale이 작아질수록 default box의 scale은 커진다. 이는 다음 이미지에서 볼 수 있듯이 feature map의 크기가 작아질수록 더 큰 객체를 탐지할 수 있다.

![3](https://user-images.githubusercontent.com/77332628/214980233-7d2d3e8e-035c-4e80-98ea-d23859fd2fae.png)

[출처](https://www.researchgate.net/figure/Multi-scale-feature-maps-for-detection-in-SSD_fig4_350159612)

aspect ratio인 $a_r ∈ [1,2,3,1/2,1/3]$이며, default box의 너비는 $w^a_k=s_k\sqrt{a_r}$이며, 높이는 $h^a_k=s_k/\sqrt{a_r}$이다. 논문의 저자는 aspect ratio가 1:1인 경우에는 scale이 $s'_k = \sqrt{s_ks_{k+1}}$인 default box를 추가적으로 사용한다고 한다.

### 3. Predictions

각각의 feature map은 서로 다른 수의 default box를 적용한다. 첫번째(38x38)와 마지막(1x1) feature map은 aspect ratio가 1:1, 1:2, 1:0.5인 default box와 위에서 언급한 aspect ratio가 1:1일 때 추가적으로 사용하는 box까지 총 4개의 default box를 적용한다. 이는 feature map의 각 cell마다 4개의 default box가 생성됨을 의미한다. 첫 번째와 마지막을 제외한 나머지 4개의 feature map은 6개의 default box를 모두 적용한다. 모든 feature map에 맞는 default box를 적용하면 총 default box의 수는 8732(=38x38x4 + 19x19x6 + 10x10x6 + 5x5x6 + 3x3x6 + 1x1x4)개가 된다.

그 다음 최종 예측을 위해서 다음 이미지처럼 서로 다른 scale의 feature map을 추출한 후 3x3 conv 연산(strid=1, padding=1)을 적용한다. 

![4](https://user-images.githubusercontent.com/77332628/214980238-6c87719b-001e-455b-92cc-bdf6ab9cdde6.png)

이 때 default box의 수를 $k$, 예측하려는 class의 수를 $c$라고 할 때, output feature map의 channel 수는 $k(4+c)$가 되도록 설계한다. 이는 다음 이미지 같이 각 feature map의 cell이 $k$개의 default box를 생성하고 각 box마다 4개의 offset(x,y,w,h)과 class score를 예측한다는 것을 의미한다.

![5](https://user-images.githubusercontent.com/77332628/214980242-65181b56-a0aa-4956-8bee-10da5df9738d.png)

SSD 모델은 예측하려는 class수가 20개인 PASCAL VOC 데이터셋을 사용해서 학습을 진행하기 때문에 class의 수는 배경을 포함해서 $c=21$이다. 위의 이미지에서처럼 5x5(x256)의 feature map을 추출할 경우 $k=6, c=21$이기 때문에 conv 연산을 적용한 output feature map의 크기는 5x5x6x(4+21)이다. 




### 4. Matching Strategy

SSD 모델의 학습을 진행할 때 default box의 학습 대상을 지정하기 위해 어떤 default box가 어떤 ground truth box와 대응할지를 결정하기 위해 **default box와 ground truth box를 매칭**하는 작업이 필요하다. 먼저 ground truth box와 가장 큰 **jaccard overlap**(IoU와 동일한 개념)을 가지는 box와 jaccard overlap이 0.5 이상인 box는 모두 positive로 label하고, ground truth box와 0.4 미만의 jaccard overlap을 가지는 box를 negative로 label한다.

하지만 일반적으로 이미지 내에서 배경에 해당하는 box의 비율이 높기 때문에 negative sample이 positive sample보다 훨씬 많다. 이로 인해 클래스 불균형 (class imbalance)가 발생하는데, 논문의 저자는 이를 해결하기 위해 높은 confidence loss를 가진 sample을 추가하는 hard negative mining을 수행한다. Hard negative mining에 대한 설명은 이전 글 ([**링크**](https://hamin-chang.github.io/cv-objectdetection/ohem/))을 참고하길 바란다. 이 때 positive와 negative sample의 비율은 1:3이 되도록 한다.

### 5. Loss function

SSD 모델의 손실 함수는 confidence loss인 $L_{conf}$와 localization loss인 $L_{loc}$의 합으로 구성되어 있다. 

* $L(x,c,l,g) = \frac1N(L_{conf}(x,c) + αL_{loc}(x,l,g))$

여기서 $α$는 두 loss 사이의 가중치를 조절하는 balancing parameter로 디폴트값 $α=1$로 설정하고 사용하고 , $N$은 ground truth box와 매칭된 default box의 수다. 만약 $N=0$이라면 loss는 0이 된다.

Localization loss 는 Faster R-CNN 모델과 마찬가지로 default box의 중심 좌표 (cx,cy)와 너비,높이(w,h)를 사용해서 smooth L1 loss를 통해서 구한다.

![8](https://user-images.githubusercontent.com/77332628/214981944-ab9a697f-dc68-4a90-b263-fc346e4ee767.png)

참고로 smooth L1 loss는 다음과 같다.

![7](https://user-images.githubusercontent.com/77332628/214980247-7bd41f1c-5658-453b-b2a2-426cc47d24f5.png)


위의 식에서 $l$은 예측한 box의 파라미터(좌표)고, $g$는 ground truth box의 파라미터(좌표)를 의미한다. $x^k_{ij}$는 $i$번째 default box와 class가 $k$인 $j$번째 ground truth box와의 매칭 여부를 알려주는 indicator parameter로, 매칭되면 1, 그렇지 않으면 0의 값을 갖는다.

Confidence loss는 모든 class에 대한 loss를 softmax loss를 통해 계산한다.

![9](https://user-images.githubusercontent.com/77332628/214981948-4a0f6938-c60a-44ad-97ef-807615f4a239.png)




### 6. SSD 모델 훈련시키기

![6](https://user-images.githubusercontent.com/77332628/214980244-a47e477d-866e-4387-b2ed-3bdbe42ba45c.png)

1) 전체 네트워크 구성하기

학습을 위해 base network(VGG16)와 auxiliary network를 합쳐 전체 네트워크를 구성한다. pre-trained VGG16 모델을 불러와 마지막 2개의 fc layer를 conv layer로 대체하고 이후 최종 output feature map의 크기가 1x1이 되도록 auxiliary network를 설계한다.

2) 이미지 입력 + multiscale feature map 추출

* Input : 300x300 sized image
* Process : feature map extraction
* Output : 
  * 38x38(x512) sized feature map
  * 19x19(x1024) sized feature map
  * 10x10(x512) sized feature map
  * 5x5(x256) sized feature map
  * 3x3(x256) sized feature map
  * 1x1(x256) sized feature map

3) Multiscale feature map에 conv 연산 적용

* Input : 6 feature maps from (2)
* Process : 3x3 conv (stride=1, padding=1) [각 feature map마다 서로 다른 수의 default box 사용함에 주의]
* Output : 
  * 38x38(**x4**x(21+4)) sized feature map
  * 19x19(x6x(21+4)) sized feature map
  * 10x10(x6x(21+4)) sized feature map
  * 5x5(x6x(21+4)) sized feature map
  * 3x3(x6x(21+4)) sized feature map
  * 1x1(**x4**x(21+4)) sized feature map
  
4) (3)에서 추출한 모든 feature map 병합

* Input : 6 feature maps from (3)
* Process : concatenate feature maps
* Output : 8732 x (21+4) sized feature map, 이를 통해 default box별로 bounding box offset 값(4)과 class score(21)를 파악할 수 있다.

5) loss function을 통해 SSD network 학습시키기

(4)에서 얻은 feature map과 ground truth를 활용해서 localization loss를 계산한다. 이후 negative sample에 대해 Crossentropy loss를 구한 후 loss에 따라 내림차순으로 정렬한다. 그 다음 negative sample에서 loss가 높은 순으로 positive sample의 3배만큼의 수를 추출하는 hard negative mining 과정을 통해 얻은 hard negative sample과 positive sample을 사용해서 confidence loss를 계산한다. 그리고 앞서 얻은 localization loss와 confidence loss를 더해 최종 loss를 구한 후 backward pass를 수행해서 network를 학습시킨다.

### 6. SSD model Detection

SSD 모델은 Non maximum suppression을 마지막 예측에 대해 수행해서 Detection을 진행한다. 이를 통해 겹치는 default box를 적절하게 제거해서 정확도를 높인다. 

출처 및 참고문헌 :

[SSD 논문](https://arxiv.org/pdf/1512.02325.pdf)

[개인 블로그](http://herbwood.tistory.com/15)



