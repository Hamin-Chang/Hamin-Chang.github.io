---
title : '[DL/CV] 객체 탐지 - FPN 🥪'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## FPN (Feature Pyramid Networks for Object Detection) 읽어보기

이번 글에서는 FPN 논문([**링크**](https://arxiv.org/pdf/1612.03144.pdf))을 리뷰해보도록 하겠다. 이미지 내 존재하는 다양한 크기의 객체를 인식하는 것은 Object detection task의 핵심적인 문제다.

### 0. 기존의 방식

다양한 크기의 객체를 detect하도록 모델을 학습시키기 위해 다양한 시도가 있었다. 기존의 방식들과 각각의 문제점을 살펴보자.

![1](https://user-images.githubusercontent.com/77332628/217481799-80782b4b-a3c9-4673-9f85-5f5481ebd54d.jpeg)

(a) Featurized image pyramid

입력 이미지의 크기를 resize해서 다양한 scale의 이미지를 네트워크에 입력하는 방법이다. Overfeat 모델 학습 시 해당 방법을 사용했고, 다양하 크기의 객체를 포착하는데 좋은 결과를 보여준다. 하지만 이미지 한장 한장을 독립적으로 모델에 입력해서 feature map을 생성하기 때문에 추론 속도가 느리다는 문제점이 있다.

(b) Singe feature map

YOLO v1에서 사용한 방법으로, 단일 scale의 입력 이미지를 네트워크에 입력해서 단일 scale의 feature map을 통해 object detection을 수행하는 방법이다. 학습 및 추론 속도가 매우 빠르지만 성능이 떨어지는 문제점이 있다. 

(c) Pyramid feature hierarchy

SSD 모델에서 사용한 방법으로, 네트워크에서 미리 지정한 conv layer마다 feature map을 추출해서 detect하는 방법이다. multi-scale feature map을 사용하기 때문에 성능이 높지만 feature map 간의 해상도 차이로 인해서 학습하는 representation에서의 차이인 semantic gap이 발생한다는 문제점이 있다. 모델이 얕은 layer에서 추출한 feature map에서 저수준(low-level) feature까지 학습하면 representational capacity를 손상시켜서 객체 인식률이 낮아진다고 한다.

SSD는 이 문제점을 해결하기 위해 low-level feature이 아닌 전체 convolutional network 중간 지점부터 feature map을 추출하지만, FPN 논문의 저자는 높은 해상도의 feature map은 작은 객체를 detect할 때 유용하기 때문에 이를 사용하는 것은 부적절하다고 지적한다.

### 1. Pyramid란?

본 논문은 FPN을 통해서 컴퓨터 자원을 적게 차지하고 기존 방식들의 문제점을 해결한 다양한 크기의 객체를 인식하는 방법을 제시한다. FPN의 핵심 아이디어를 알아보기 전에 본 논문에는 **"Pyramid"**라는 단어가 자주 등장하는데, 이는 다음 이미지처럼 convolutional network에서 얻을 수 있는 서로 다른 해상도의 feature map을 쌓아올린 형태라고 생각하면 된다.


![2](https://user-images.githubusercontent.com/77332628/217481809-8294fced-af18-4f61-9b35-d65ab887a9d2.png)

그리고 **level**은 피라미드의 각 층에 해당하는 feature map이다. convolutional network에서 더 얕은, 즉 입력층에 가까울수록 feature map은 높은 해상도(high resolution)을 갖고, 가장자리, 곡선 등과 같은 저수준 특징(low-level feature)을 가진다. 반대로 더 깊은 layer에서 얻을 수 있는 feature map은 낮은 해상도(low-level resolution)을 가지며, 질감과 물체의 일부분 등 class를 추론할 수 있는 고수준 특징(high-level feature)을 가지고 있다.

![3](https://user-images.githubusercontent.com/77332628/217481811-4d728b3f-5f44-4e67-aaa6-547d1a81a2eb.png)

Object detection 모델은 피라미드의 각 level의 feature map을 일부 혹은 전부 사용해서 예측을 수행한다. 이 부분을 직관적으로 이해하면 FPN 모델을 조금 더 쉽게 이해할 수 있을 것이다.





### 2. Main Ideas

**Feature Pyramid Network**

![4](https://user-images.githubusercontent.com/77332628/217481818-30a3c685-0452-46f5-8ffb-efd4a1434535.png)

Feature Pyramid Network는 임의의 single-scale 이미지를 convolutional network에 입력해서 다양한 scale의 feature map을 출력하는 network이다. FPN은 완전 새롭게 설계된 모델은 아니고 기존 convolutional network에서 지정한 layer별로 feature map을 추출해서 수정하는 network라고 할 수 있다. FPN이 feature map을 추출해서 피라미드를 건축하는 과정은 **1) bottom - up pathway**, **2) top - down pathway, lateral connection**에 따라 진행된다.

1) **Bottom - up pathway**

![5](https://user-images.githubusercontent.com/77332628/217481822-497d4e47-e48e-4386-acc4-3d1c5540f3c5.png)

"상향식 과정"이라고도 하는 Bottom - up pathway는 이미지를 convolutional network에 입력해서 forward pass해서 2배씩 작아지는 feature map을 추출하는 과정이다. 위 이미지처럼 각 **stage**의 마지막 layer의 output feature map을 추출하는데, 논문에서는 같은 크기의 feature map을 출력하는 layer를 모두 같은 **stage**에 속해있다고 정의한다. 각 stage별로 마지막 layer를 pyramid layer로 지정하는 이유는 더 깊은 layer가 더 강력한 feature를 보유하기 때문이다.

본 논문에서는 convolutional network를 ResNet을 사용하는데, 이 경우 마지막 residual block의 output feature map을 활용해서 feature pyramid를 구성하는데 각 output을 {c2, c3, c4, c5}라고 지정한다. 이는 각각 {4, 8, 16, 32} stride를 가지고 있어서 각각은 원본 이미지의 1/4, 1/8, 1/16, 1/32 크기를 가진 feature map이다. c1이 없는 이유는 conv1의 output feature map은 너무 많은 메모리를 차지하기 때문에 피라미드에서 제외되었다고 한다.

2) **Top - down pathway + Lateral connections**


![6](https://user-images.githubusercontent.com/77332628/217481826-4b4cebbb-d337-4dda-b4d5-f74a27a22b1c.png)

Top - down Pathway는 각 pyramid level에 있는 feature map을 2배로 upsampling하고 channel 수를 동일하게 맞춰주는 과정이다. 각 pyramid level feature map을 2배로 upsampling해주면 바로 아래 level의 feature map과 크기가 같아진다. 예를 들어 c3를 upsampling하면 c2와 크기가 같아진다. upsampling은 **nearest neighbor upsampling** 방식을 사용한다. 

![7](https://user-images.githubusercontent.com/77332628/217481827-8ca2fabb-d4f9-4b44-8368-fe20eced6c8b.png)

이후 모든 pyramid level의 feature map에 1x1 conv 연산을 적용해서 channel을 256으로 맞춰준다. 그리고 upsample된 feature map과 바로 아래 level의 feature map과 element-wise addition을 하는 Later connection 과정을 수행한다.

![8](https://user-images.githubusercontent.com/77332628/217481829-68ca4b64-309a-4fdb-93db-f21b9bb4fbd7.png)

Lateral connection 과정을 거친 후 각각의 feature map에 3x3 conv 연산을 적용해서 얻은 feature map이 각각 {p2, p3, p4, p5}이다. 이는 각각 {c2, c3, c4, c5}와 크기가 같다. 가장 높은 level에 있는 feature map c5의 경우 1x1 conv 연산 후 그대로 출력해서 p5를 얻는다. 참고로 여기서 3x3 conv 연산을 적용하는 이유는 upsampling된 layer와 병합될 때 엘리어싱 효과를 줄이기 위해서이다.

![9](https://user-images.githubusercontent.com/77332628/217481833-282cdfe6-3a6f-4e42-a8c5-860edb5b10f3.png)

위 과정을 통해서 FPN은 single-scale 이미지를 입력해서 4개의 multi-scale feature map을 얻는다. 이 방식이 SSD에서 사용한 (c) 방식보다 나은 이유는 다음과 같다. Detection task시 고해상도 feature map은 low-level feature를 가지지만 객체의 위치에 대한 정보를 상대적으로 보존하고 있다. 이는 저해상도 feature map에 비해 덜 downsample 됐기 때문이다. 이러한 고해상도 feature map의 특징을 element-wise addition을 통해 저해상도 feature map에 전달하기 때문에 (c) 방식보다 작은 객체를 더 잘 detect 한다.




### 3. Training ResNet + Faster R-CNN with FPN


![10](https://user-images.githubusercontent.com/77332628/217481840-02abedf4-4c67-4bd1-be47-f0d2350932b5.png)

논문에서는 ResNet을 backbone network로 사용하는 Faster R-CNN에 FPN을 적용해서 학습시키는 과정을 설명한다. 

1) **Build Feature Pyramid by FPN**

FPN은 그 자체로 객체 탐지기가 아니라 객체 탐지기와 함께 작동하는 feature 추출기이다. FPN은 feature map을 추출하고 나중에 객체 탐지를 위해 탐지기에 공급한다. 먼저 FPN으로 feature 추출을 해보자.

ResNet 기반의 FPN에 원본 입력 이미지를 입력하고 Bottom-up pathway를 거쳐서 원본 이미지의 1/4, 1/8, 1/16, 1/32 크기에 해당하는 feature map {c2, c3, c4, c5}을 출력하고, 이를 Top-down pathway 과정을 통해서 1x1 conv를 적용해서 channel 수를 256으로 맞추고 크기를 2배 upsampling한다. 마지막으로 Lateral connections를 통해 각 feature map을 바로 아래 pyramid level에 존재하는 feature map과 element-wise addition을 수행하고 3x3 conv를 적용해서  {p2, p3, p4, p5} feature map을 출력한다.

* Input : single-scale image
* Process : build feature pyramid by FPN
* Output : multi-scale feature map {p2, p3, p4, p5} 

2) **Class score and Bounding box by RPN**

![11](https://user-images.githubusercontent.com/77332628/217481847-b5044668-1478-4d7c-9f70-280846a1a6e8.jpeg)

(1)과정에서 얻은 feature map {p2, p3, p4, p5}를 RPN(Region Proposal Network)에 입력한다. 각 pyramid level의 feature map에 대해 3x3 conv 연산이 적용된 다음 class score과 bounding box regressor 출력을 위해 별도의 1x1 conv 연산이 적용된다. 이러한 3x3 및 1x1 conv layer를 RPN *head*라고 한다. 이후 Non maximum suppression 알고리즘을 적용해서 class score가 높은 상위 1000개의 region proposal만을 출력한다.

* Input :  multi-scale feature map {p2, p3, p4, p5}
* Process : region proposal and Non maximum suppression
* Output : 1000 region proposals

3) **Max pooling by RoI pooling**

(1)에서 얻은 multi-scale feature map {p2, p3, p4, p5}와 (2)에서 얻은 1000개의 region proposals를 사용해서 RoI pooling을 수행한다. FPN을 적용한 Faster R-CNN은 multi-scale feature map을 사용하기 때문에 **RoI를 어떤 scale의 feature map과 매칭**시킬지를 결정해야한다.

![12](https://user-images.githubusercontent.com/77332628/217481858-064929c7-d1e5-4341-9147-2b043b744d91.png)

논문에서는 위와 같은 공식을 사용해서 RoI를 $k$번째 feature map과 매칭한다. 위 식에서 $w, h$는 각각 RoI의 너비, 높이에 해당하고, $k$는 pyramid level의 index, $k_0$은 target level을 의미한다. ($k_0=4$로 논문에서는 지정) 직관적으로 봤을 때 RoI의 scale이 작아질수록 낮은 pyramid level, 즉 해상도가 높은 feature map에 할당됨을 알 수 있다.

* Input : multi-scale feature map {p2, p3, p4, p5}, 1000 region proposals
* Process : RoI pooling
* Output : fixed sized feature maps


4) Train Faster R-CNN 

RoI pooling을 통해 얻은 fixed sized feature maps를 Fast R-CNN에 입력한 후 전체 네트워크를 multi-task loss function을 통해 학습시킨다. 손실함수와 위의 RoI pooling에 대한 내용은 [<U>**Fast R-CNN 논문 리뷰**</U>](https://hamin-chang.github.io/cv-objectdetection/frcnn/)를 참고하면 된다.


### 4. Detection & 결론

실제 inference 시에는 Fast R-CNN의 마지막 예측에 Non maximum suppression을 적용해서 최적의 예측 결과만을 출력한다.

ResNet을 backbone network로 사용한 Faster R-CNN에 FPN을 결합시켰을 때, FPN을 사용하지 않았을 때보다 AP 값이 8% 이상 향상되었다고 한다. 이외에도 FPN은 end-to-end로 학습이 가능하며, 학습 및 테스트 시간이 일정하여 메모리 사용량이 적다는 장점을 가지고 있다. 
 
FPN 논문 리뷰를 통해서 feature map의 역할과 특징을 다시 한번 짚고 넘어갈 수 있어서 좋았다. FPN의 lateral-connection 부분이 ResNet의 skip connection의 개념과 비스한 느낌이 들었다. 

출처 및 참고문헌 

FPN 논문 (https://arxiv.org/pdf/1612.03144.pdf)

개인 블로그 (http://herbwood.tistory.com/18)

wikidocs (https://wikidocs.net/162976)



