---
title : '[IC/Pytorch] ÌååÏù¥ÌÜ†ÏπòÎ°ú WRN Íµ¨ÌòÑÌïòÍ∏∞ üõ£Ô∏è' 
layout: single
toc: true
toc_sticky: true
categories:
  - ICCode
---

## PytorchÎ°ú WRN Íµ¨ÌòÑÌïòÍ∏∞

Ïù¥Î≤à Í∏ÄÏóêÏÑúÎäî Ïã§Ï†ú ÌååÏù¥ÌÜ†Ïπò ÏΩîÎìúÎ°ú WRN Î™®Îç∏ÏùÑ Íµ¨ÌòÑÌï¥Î≥∏Îã§. WRN Î™®Îç∏Ïóê ÎåÄÌïú ÏÑ§Î™ÖÏùÄ [**<U>WRN ÎÖºÎ¨∏ Î¶¨Î∑∞</U>**](https://hamin-chang.github.io/)Î•º Ï∞∏Í≥†ÌïòÍ∏∏ Î∞îÎûÄÎã§. Ïù¥Î≤à Í∏ÄÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî ÏΩîÎìúÎäî [**<U>weiaicunzaiÏùò repository</U>**](https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/wideresidual.py)Ïùò ÏΩîÎìúÎ•º ÏÇ¨Ïö©ÌñàÎã§.



Î®ºÏ†Ä residual unitÏùÑ Íµ¨ÌòÑÌïúÎã§.

WRNÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî residual unitÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§.

![1](https://user-images.githubusercontent.com/77332628/236633668-f9d77970-d8fc-45cb-8c07-f30e7ff04703.png)


```python
import torch
import torch.nn as nn

class WiderBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super().__init__()
        
        self.residual = nn.Sequential(nn.BatchNorm2d(in_channels),
                                     nn.ReLU(),
                                     nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
                                     nn.BatchNorm2d(out_channels),
                                     nn.ReLU(),
                                     nn.Dropout(),
                                     nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1))
        
        self.shortcut = nn.Sequential()
    
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=stride, padding=0)
    
    def forward(self, x):
        x_shortcut = self.shortcut(x)
        x_res = self.residual(x)
        return x_shortcut + x_res
```

Ïù¥Ï†ú residual blockÏùÑ Ïù¥Ïö©Ìï¥ÏÑú Ï†ÑÏ≤¥ Î™®Îç∏ÏùÑ Íµ¨Ï∂ïÌïòÎäîÎç∞, depth=40, $k$=10ÏúºÎ°ú ÏÑ§Ï†ïÌïúÎã§.

Ï†ÑÏ≤¥Ï†ÅÏù∏ Íµ¨Ï°∞Îäî Îã§Ïùå ÌëúÏôÄ Í∞ôÎã§.

![2](https://user-images.githubusercontent.com/77332628/236633669-22b3a8b4-158d-42bd-b8ff-e4a5bbb365ab.png)



```python
class WRN(nn.Module):
    def __init__(self, depth=40, k=10, num_classes=10, init_weights = True):
        super().__init__()
        N = int((depth-4)/6)
        self.in_channels = 16
        
        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)
        self.conv2 = self._make_layer(16*k, N, 1)
        self.conv3 = self._make_layer(32*k, N, 2)
        self.conv4 = self._make_layer(64*k, N, 2)
        self.bn = nn.BatchNorm2d(64*k)
        self.relu = nn.ReLU()
        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(64*k, num_classes)
        
        # Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
        if init_weights:
            self._weights_initialize()
            
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x
    
    def _make_layer(self, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks-1)
        layers = []
        
        for stride in strides:
            layers.append(WiderBlock(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)
    
    # Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
    def _weights_initialize(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
                
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight,0,0.01)
                nn.init.constant_(m.bias, 0)
                
def WRN_40_10():
    return WRN(40, 10)
```

Ïù¥Ï†ú Î™®Îç∏Ïù¥ Ïûò ÎßåÎì§Ïñ¥Ï°åÎäîÏßÄ ÌôïÏù∏Ìï¥Î≥¥Ïûê. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î°ú ÎûúÎç§Ìïú Í∞íÏùÑ ÎÑ£Ïñ¥ÏÑú ÌôïÏù∏Ìï¥Î≥¥Ïûê.



```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.randn(3,3,64,64).to(device)
model = WRN_40_10().to(device)
output = model(x)
print(output.size())
```

    torch.Size([3, 10])


ÎßàÏßÄÎßâÏúºÎ°ú Î™®Îç∏ summaryÎ•º Ï∂úÎ†•ÌïòÍ≥† Í∏ÄÏùÑ ÎßàÏπúÎã§.

Ï∞∏Í≥†Ìïú Î∏îÎ°úÍ∑∏ÏóêÎäî STL10 Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú Ïã§Ï†ú Î™®Îç∏ ÌõàÎ†®ÍπåÏßÄ ÏãúÌÇ§ÏßÄÎßå Ïù¥Î≤à Í∏ÄÏùò Î™©Ï†ÅÏùÄ WRNÏùÑ ÌååÏù¥ÌÜ†ÏπòÎ°ú Íµ¨ÌòÑÌïòÎäî Í≤ÉÏù¥Í∏∞ ÎïåÎ¨∏Ïóê ÎÇòÎ®∏ÏßÄ Í≥ºÏ†ïÏùÄ ÏÉùÎûµÌïúÎã§.


```python
!pip install torchsummary
import torchsummary
torchsummary.summary(model, (3,64,64), device=device.type)
```

    Collecting torchsummary
      Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)
    Installing collected packages: torchsummary
    Successfully installed torchsummary-1.5.1
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
    [0m----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1           [-1, 16, 64, 64]             448
                Conv2d-2          [-1, 160, 64, 64]           2,720
           BatchNorm2d-3           [-1, 16, 64, 64]              32
                  ReLU-4           [-1, 16, 64, 64]               0
                Conv2d-5          [-1, 160, 64, 64]          23,200
           BatchNorm2d-6          [-1, 160, 64, 64]             320
                  ReLU-7          [-1, 160, 64, 64]               0
               Dropout-8          [-1, 160, 64, 64]               0
                Conv2d-9          [-1, 160, 64, 64]         230,560
           WiderBlock-10          [-1, 160, 64, 64]               0
          BatchNorm2d-11          [-1, 160, 64, 64]             320
                 ReLU-12          [-1, 160, 64, 64]               0
               Conv2d-13          [-1, 160, 64, 64]         230,560
          BatchNorm2d-14          [-1, 160, 64, 64]             320
                 ReLU-15          [-1, 160, 64, 64]               0
              Dropout-16          [-1, 160, 64, 64]               0
               Conv2d-17          [-1, 160, 64, 64]         230,560
           WiderBlock-18          [-1, 160, 64, 64]               0
          BatchNorm2d-19          [-1, 160, 64, 64]             320
                 ReLU-20          [-1, 160, 64, 64]               0
               Conv2d-21          [-1, 160, 64, 64]         230,560
          BatchNorm2d-22          [-1, 160, 64, 64]             320
                 ReLU-23          [-1, 160, 64, 64]               0
              Dropout-24          [-1, 160, 64, 64]               0
               Conv2d-25          [-1, 160, 64, 64]         230,560
           WiderBlock-26          [-1, 160, 64, 64]               0
          BatchNorm2d-27          [-1, 160, 64, 64]             320
                 ReLU-28          [-1, 160, 64, 64]               0
               Conv2d-29          [-1, 160, 64, 64]         230,560
          BatchNorm2d-30          [-1, 160, 64, 64]             320
                 ReLU-31          [-1, 160, 64, 64]               0
              Dropout-32          [-1, 160, 64, 64]               0
               Conv2d-33          [-1, 160, 64, 64]         230,560
           WiderBlock-34          [-1, 160, 64, 64]               0
          BatchNorm2d-35          [-1, 160, 64, 64]             320
                 ReLU-36          [-1, 160, 64, 64]               0
               Conv2d-37          [-1, 160, 64, 64]         230,560
          BatchNorm2d-38          [-1, 160, 64, 64]             320
                 ReLU-39          [-1, 160, 64, 64]               0
              Dropout-40          [-1, 160, 64, 64]               0
               Conv2d-41          [-1, 160, 64, 64]         230,560
           WiderBlock-42          [-1, 160, 64, 64]               0
          BatchNorm2d-43          [-1, 160, 64, 64]             320
                 ReLU-44          [-1, 160, 64, 64]               0
               Conv2d-45          [-1, 160, 64, 64]         230,560
          BatchNorm2d-46          [-1, 160, 64, 64]             320
                 ReLU-47          [-1, 160, 64, 64]               0
              Dropout-48          [-1, 160, 64, 64]               0
               Conv2d-49          [-1, 160, 64, 64]         230,560
           WiderBlock-50          [-1, 160, 64, 64]               0
               Conv2d-51          [-1, 320, 32, 32]          51,520
          BatchNorm2d-52          [-1, 160, 64, 64]             320
                 ReLU-53          [-1, 160, 64, 64]               0
               Conv2d-54          [-1, 320, 32, 32]         461,120
          BatchNorm2d-55          [-1, 320, 32, 32]             640
                 ReLU-56          [-1, 320, 32, 32]               0
              Dropout-57          [-1, 320, 32, 32]               0
               Conv2d-58          [-1, 320, 32, 32]         921,920
           WiderBlock-59          [-1, 320, 32, 32]               0
          BatchNorm2d-60          [-1, 320, 32, 32]             640
                 ReLU-61          [-1, 320, 32, 32]               0
               Conv2d-62          [-1, 320, 32, 32]         921,920
          BatchNorm2d-63          [-1, 320, 32, 32]             640
                 ReLU-64          [-1, 320, 32, 32]               0
              Dropout-65          [-1, 320, 32, 32]               0
               Conv2d-66          [-1, 320, 32, 32]         921,920
           WiderBlock-67          [-1, 320, 32, 32]               0
          BatchNorm2d-68          [-1, 320, 32, 32]             640
                 ReLU-69          [-1, 320, 32, 32]               0
               Conv2d-70          [-1, 320, 32, 32]         921,920
          BatchNorm2d-71          [-1, 320, 32, 32]             640
                 ReLU-72          [-1, 320, 32, 32]               0
              Dropout-73          [-1, 320, 32, 32]               0
               Conv2d-74          [-1, 320, 32, 32]         921,920
           WiderBlock-75          [-1, 320, 32, 32]               0
          BatchNorm2d-76          [-1, 320, 32, 32]             640
                 ReLU-77          [-1, 320, 32, 32]               0
               Conv2d-78          [-1, 320, 32, 32]         921,920
          BatchNorm2d-79          [-1, 320, 32, 32]             640
                 ReLU-80          [-1, 320, 32, 32]               0
              Dropout-81          [-1, 320, 32, 32]               0
               Conv2d-82          [-1, 320, 32, 32]         921,920
           WiderBlock-83          [-1, 320, 32, 32]               0
          BatchNorm2d-84          [-1, 320, 32, 32]             640
                 ReLU-85          [-1, 320, 32, 32]               0
               Conv2d-86          [-1, 320, 32, 32]         921,920
          BatchNorm2d-87          [-1, 320, 32, 32]             640
                 ReLU-88          [-1, 320, 32, 32]               0
              Dropout-89          [-1, 320, 32, 32]               0
               Conv2d-90          [-1, 320, 32, 32]         921,920
           WiderBlock-91          [-1, 320, 32, 32]               0
          BatchNorm2d-92          [-1, 320, 32, 32]             640
                 ReLU-93          [-1, 320, 32, 32]               0
               Conv2d-94          [-1, 320, 32, 32]         921,920
          BatchNorm2d-95          [-1, 320, 32, 32]             640
                 ReLU-96          [-1, 320, 32, 32]               0
              Dropout-97          [-1, 320, 32, 32]               0
               Conv2d-98          [-1, 320, 32, 32]         921,920
           WiderBlock-99          [-1, 320, 32, 32]               0
              Conv2d-100          [-1, 640, 16, 16]         205,440
         BatchNorm2d-101          [-1, 320, 32, 32]             640
                ReLU-102          [-1, 320, 32, 32]               0
              Conv2d-103          [-1, 640, 16, 16]       1,843,840
         BatchNorm2d-104          [-1, 640, 16, 16]           1,280
                ReLU-105          [-1, 640, 16, 16]               0
             Dropout-106          [-1, 640, 16, 16]               0
              Conv2d-107          [-1, 640, 16, 16]       3,687,040
          WiderBlock-108          [-1, 640, 16, 16]               0
         BatchNorm2d-109          [-1, 640, 16, 16]           1,280
                ReLU-110          [-1, 640, 16, 16]               0
              Conv2d-111          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-112          [-1, 640, 16, 16]           1,280
                ReLU-113          [-1, 640, 16, 16]               0
             Dropout-114          [-1, 640, 16, 16]               0
              Conv2d-115          [-1, 640, 16, 16]       3,687,040
          WiderBlock-116          [-1, 640, 16, 16]               0
         BatchNorm2d-117          [-1, 640, 16, 16]           1,280
                ReLU-118          [-1, 640, 16, 16]               0
              Conv2d-119          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-120          [-1, 640, 16, 16]           1,280
                ReLU-121          [-1, 640, 16, 16]               0
             Dropout-122          [-1, 640, 16, 16]               0
              Conv2d-123          [-1, 640, 16, 16]       3,687,040
          WiderBlock-124          [-1, 640, 16, 16]               0
         BatchNorm2d-125          [-1, 640, 16, 16]           1,280
                ReLU-126          [-1, 640, 16, 16]               0
              Conv2d-127          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-128          [-1, 640, 16, 16]           1,280
                ReLU-129          [-1, 640, 16, 16]               0
             Dropout-130          [-1, 640, 16, 16]               0
              Conv2d-131          [-1, 640, 16, 16]       3,687,040
          WiderBlock-132          [-1, 640, 16, 16]               0
         BatchNorm2d-133          [-1, 640, 16, 16]           1,280
                ReLU-134          [-1, 640, 16, 16]               0
              Conv2d-135          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-136          [-1, 640, 16, 16]           1,280
                ReLU-137          [-1, 640, 16, 16]               0
             Dropout-138          [-1, 640, 16, 16]               0
              Conv2d-139          [-1, 640, 16, 16]       3,687,040
          WiderBlock-140          [-1, 640, 16, 16]               0
         BatchNorm2d-141          [-1, 640, 16, 16]           1,280
                ReLU-142          [-1, 640, 16, 16]               0
              Conv2d-143          [-1, 640, 16, 16]       3,687,040
         BatchNorm2d-144          [-1, 640, 16, 16]           1,280
                ReLU-145          [-1, 640, 16, 16]               0
             Dropout-146          [-1, 640, 16, 16]               0
              Conv2d-147          [-1, 640, 16, 16]       3,687,040
          WiderBlock-148          [-1, 640, 16, 16]               0
         BatchNorm2d-149          [-1, 640, 16, 16]           1,280
                ReLU-150          [-1, 640, 16, 16]               0
    AdaptiveAvgPool2d-151            [-1, 640, 1, 1]               0
              Linear-152                   [-1, 10]           6,410
    ================================================================
    Total params: 55,856,330
    Trainable params: 55,856,330
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.05
    Forward/backward pass size (MB): 430.25
    Params size (MB): 213.07
    Estimated Total Size (MB): 643.38
    ----------------------------------------------------------------


Ï∂úÏ≤ò Î∞è Ï∞∏Í≥†Î¨∏Ìóå :

1. https://deep-learning-study.tistory.com/542
2. https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/wideresidual.py
