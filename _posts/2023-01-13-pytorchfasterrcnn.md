---
title : '[CV/Pytorch] íŒŒì´í† ì¹˜ë¡œ Faster R-CNN êµ¬í˜„í•˜ê¸° ğŸ“¦'
layout: single
toc: true
toc_sticky: true
categories:
  - pytorchOD
---

## Pytorchë¡œ Faster R-CNN êµ¬í˜„í•˜ê¸°

ì´ë²ˆ ê¸€ì—ì„œëŠ” [How FasterRCNN works and step-by-step PyTorch implementation](https://www.google.com/search?q=How+FasterRCNN+works+and+step-by-step+PyTorch+implementation&oq=How+FasterRCNN+works+and+step-by-step+PyTorch+implementation&aqs=chrome..69i57j69i61l3&client=ubuntu&sourceid=chrome&ie=UTF-8)ì— ë‚˜ì˜¨ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•œ Faster RCNN ì½”ë“œë¥¼ ë¶„ì„í•´ë³¸ë‹¤. Faster RCNNì— ëŒ€í•œ ì„¤ëª…ì€ ì´ì „ ê¸€ ([**ë§í¬**](https://hamin-chang.github.io/cv-objectdetection/ffrcnn/#faster-rcnn-%EB%85%BC%EB%AC%B8-%EC%9D%BD%EC%96%B4%EB%B3%B4%EA%B8%B0))ì„ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤. ë¨¼ì € ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¡œë“œí•˜ê³  GPUë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì½”ë“œë¥¼ ì…ë ¥í•œë‹¤.




```python
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms

import matplotlib.pyplot as plt
import numpy as np
import cv2
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

if torch.cuda.is_available():
  DEVICE = torch.device('cuda')
  print(DEVICE, torch.cuda.get_device_name(0))
else :
  DEVICE = torch.device('cpu')
  print(DEVICE)
```

    Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
    cpu


ì°¸ê³ í•œ ì½”ë“œì—ì„œëŠ” ì…ë ¥ ì´ë¯¸ì§€ë¡œ ë‹¤ìŒì˜ ì–¼ë£©ë§ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í–ˆë‹¤.


```python
img0 = cv2.imread("/content/drive/MyDrive/zebras.jpg")
img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)
print(img0.shape)
plt.imshow(img0)
plt.show()
```

    (1333, 2000, 3)



    
![1](https://user-images.githubusercontent.com/77332628/212281527-ce922d2e-541d-408c-a1dd-8f98d973722b.png)
    


ì–¼ë£©ë§ì´ ìˆëŠ” ë°•ìŠ¤ë¥¼ ì‹œê°í™”í•´ë³´ì. 


```python
bbox0 = np.array([[223, 782, 623, 1074], [597, 695, 1038, 1050], 
                  [1088, 699, 1452, 1057], [1544, 771, 1914, 1063]]) 
labels = np.array([1, 1, 1, 1]) # 0: background, 1: zebra
```


```python
img0_clone = np.copy(img0)
for i in range(len(bbox0)):
    cv2.rectangle(img0_clone, (bbox0[i][0], bbox0[i][1]), 
                              (bbox0[i][2], bbox0[i][3]),
                 color=(0, 255, 0), thickness=10)
plt.imshow(img0_clone)
plt.show()
```


    
![2](https://user-images.githubusercontent.com/77332628/212281536-90220e9e-1394-4d3c-8710-c6360312eabe.png)
    


í¸ì˜ë¥¼ ìœ„í•´ì„œ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ 800x800 í¬ê¸°ë¡œ resize í•´ì¤€ë‹¤. ì‹¤ì œ sub-sampling ratio = 1/16ìœ¼ë¡œ ì§€ì •í•´ì„œ feature extractorë¥¼ ê±°ì¹œ feature mapì˜ í¬ê¸°ëŠ” 50x50ì´ ëœë‹¤.


```python
img = cv2.resize(img0, dsize=(800,800), interpolation=cv2.INTER_CUBIC)
plt.figure(figsize=(7,7))
plt.imshow(img)
plt.show()
```


    
![3](https://user-images.githubusercontent.com/77332628/212281538-82d5fac2-4c3b-41c2-a86c-88dcb46bed66.png)


ìˆ˜ì •í•œ ì´ë¯¸ì§€ì˜ í¬ê¸°ì— ë§ê²Œ ì–¼ë£©ë§ì´ ìˆëŠ” ë°•ìŠ¤ë¥¼ ë‹¤ì‹œ ì‹œê°í™” í•´ë³´ì. ì´ ì´ë¯¸ì§€ì—ì„œì˜ ë°•ìŠ¤ë“¤ì˜ ì¢Œí‘œê°€ ground truth ê°’ì´ ë  ê²ƒì´ë‹¤. 


```python
Wratio = 800/img0.shape[1]
Hratio = 800/img0.shape[0]

ratioList = [Wratio,Hratio,Wratio,Hratio]
bbox = []

for box in bbox0:
  box = [int(a*b) for a, b in zip(box,ratioList)]
  bbox.append(box)

bbox = np.array(bbox) 
print('coordinates of ground truth boxes:')
print(bbox)

img_clone = np.copy(img)
for i in range(len(bbox)):
    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), color=(0, 255, 0), thickness=5)
plt.imshow(img_clone)
plt.show()
```

    coordinates of ground truth boxes:
    [[ 89 469 249 644]
     [238 417 415 630]
     [435 419 580 634]
     [617 462 765 637]]



    
![4](https://user-images.githubusercontent.com/77332628/212281543-bf5658f8-5da7-4a48-a008-4e9f3796accf.png)
    


### 1. Feature extraction by pre-trained VGG16
ì›ë³¸ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ feature extractionì„ ìˆ˜í–‰í•  ì‚¬ì „ í›ˆë ¨ëœ VGG16 ëª¨ë¸ì„ ì •ì˜í•œë‹¤. ê·¸ ë‹¤ìŒ ì „ì²´ ëª¨ë¸ì—ì„œ sub-sampling ratioì— ë§ê²Œ 50x50 í¬ê¸°ê°€ ë˜ëŠ” layerê¹Œì§€ë§Œ feature extractorë¡œ ì‚¬ìš©í•œë‹¤. ì´ë¥¼ ìœ„í•´ ì›ë³¸ ì´ë¯¸ì§€ì™€ í¬ê¸°ê°€ ê°™ì€ 800x800 í¬ê¸°ì˜ dummy ë°°ì—´ì„ ì…ë ¥í•´ì„œ 50x50 í¬ê¸°ì˜ feature mapì„ ì¶œë ¥í•˜ëŠ” layerë¥¼ ì°¾ëŠ”ë‹¤. ì´í›„ **faster_rcnn_feature_extractor** ë³€ìˆ˜ì— ì „ì²´ ëª¨ë¸ì—ì„œ í•´ë‹¹ layerê¹Œì§€ë§Œ ì €ì¥í•˜ê³  ì›ë³¸ ì´ë¯¸ì§€ë¥¼ **faster_rcnn_feature_extractor**ì— ì…ë ¥í•´ì„œ 50x50x512 í¬ê¸°ì˜ feature mapì„ ì–»ëŠ”ë‹¤. 


```python
# ì‚¬ì „ í›ˆë ¨ëœ VGG16 ëª¨ë¸ ë¡œë“œ
model = torchvision.models.vgg16(pretrained=True).to(DEVICE)
features = list(model.features)

dummy_img = torch.zeros((1,3,800,800)).float() # ì…ë ¥ ì´ë¯¸ì§€ì™€ ê°™ì€ í¬ê¸°ì˜ dummy ë°°ì—´

req_features = []
output = dummy_img.clone().to(DEVICE)

for feature in features:
  output = feature(output)

  if output.size()[2] < 800//16:
    break
  req_features.append(feature)
  out_channels = output.size()[1]

# í•´ë‹¹ layerê¹Œì§€ë§Œ Sequential modelë¡œ ë³€í™˜
faster_rcnn_feature_extractor = nn.Sequential(*req_features)
```

    /usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
      warnings.warn(
    /usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
      warnings.warn(msg)


ìœ„ì—ì„œ ì •ì˜í•œ feature extractorì— ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì£¼ì…í•´ì„œ Feature extractionì„ ìˆ˜í–‰í•´ë³´ì.


```python
transform = transforms.Compose([transforms.ToTensor()])
imgTensor = transform(img).to(DEVICE)
imgTensor = imgTensor.unsqueeze(0)
output_map = faster_rcnn_feature_extractor(imgTensor) # ì¶œë ¥ê°’

print(output_map.size())
```

    torch.Size([1, 512, 50, 50])


ì¶œë ¥í•œ 50x50x512 feature mapsì˜ ì²« 5ê°œì˜ ì±„ë„ì„ ì‹œê°í™”í•´ë³´ì.


```python
imgArray = output_map.data.cpu().numpy().squeeze(0)
fig = plt.figure(figsize=(12,4))
figNo = 1 

for i in range(5):
  fig.add_subplot(1,5,figNo)
  plt.imshow(imgArray[i], cmap='gray')
  figNo += 1

plt.show()
```


    
![5](https://user-images.githubusercontent.com/77332628/212281547-c0754078-005c-4882-a576-d49ec359b032.png)
    


### 2. Anchor Generation Layer
Anchor Generation Layerì—ì„œëŠ” anchor boxë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ë¨¼ì € ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ì…ë ¥ ì´ë¯¸ì§€ì— anchorì˜ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ ì‹œê°í™”í•´ë³´ì.


```python
'''ì´ë¯¸ì§€ í¬ê¸°ëŠ” 800x800ì´ê³  sub-sampling rate = 1/16ì´ê¸° ë•Œë¬¸ì—
ì´ 50x50 = 2500ê°œì˜ anchorì´ ë§Œë“¤ì–´ì§€ê³  anchor í•˜ë‚˜ë‹¹ 9ê°œì˜ anchor boxesê°€
ë§Œë“¤ì–´ì§€ê¸° ë•Œë¬¸ì— ì´ 9x2500 = 22500ê°œì˜ anchor boxes ìƒì„±'''

feature_size = 800 // 16
ctr_x = np.arange(16, (feature_size+1) * 16, 16 ) 
ctr_y = np.arange(16, (feature_size+1) * 16, 16 )
print(ctr_x)
```

    [ 16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272 288
     304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560 576
     592 608 624 640 656 672 688 704 720 736 752 768 784 800]



```python
# anchorì˜ ì¤‘ì‹¬ ì¢Œí‘œ êµ¬í•˜ê¸°

index = 0
ctr = np.zeros((2500,2))

for i in range(len(ctr_x)):
  for j in range(len(ctr_y)):
    ctr[index, 1] = ctr_x[i] - 8
    ctr[index, 0] = ctr_y[j] - 8
    index += 1
```

Anchorì˜ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ ì´ë¯¸ì§€ì— í‘œì‹œí•´ë³´ì.


```python
img_clone2 = np.copy(img)
ctr_int = ctr.astype('int32')

plt.figure(figsize=(7,7))
for i in range(ctr.shape[0]):
  cv2.circle(img_clone2, (ctr_int[i][0], ctr_int[i][1]),
             radius = 1 , color = (255,0,0), thickness=3)
plt.imshow(img_clone2)
plt.show()
```


    
![6](https://user-images.githubusercontent.com/77332628/212281552-09b92547-530f-4cf8-8584-7fff42656449.png)
    


ì´ì œ ë³¸ê²©ì ì¸ Anchor generation layerë¥¼ êµ¬í˜„í• í…ë°, ì´ë¥¼ ìœ„í•´ì„œ 16x16 ê°„ê²©ì˜ gridë§ˆë‹¤ anchorë¥¼ ìƒì„ í•˜ê³ , anchorë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì™€ ë¹„ìœ¨ì„ ê°€ì§€ëŠ” 9ê°œì˜ anchor boxë¥¼ ìƒì„±í•œë‹¤. anchor_boxes ë³€ìˆ˜ì— ì „ì²´ anchor boxì˜ ì¢Œí‘œ (x1,y1,x2,y2)ë¥¼ ì €ì¥í•œë‹¤. 


```python
ratios = [0.5,1,2]
scales = [8,16,32]
sub_sample = 16

anchor_boxes = np.zeros(((feature_size * feature_size * 9),4))
index = 0
for c in ctr :  # per anchors
  ctr_y , ctr_x = c 
  for i in range(len(ratios)): # per ratios
    for j in range(len(scales)): # per scales

      # anchor boxì˜ height, width
      h = sub_sample * scales[j] * np.sqrt(ratios[i])
      w = sub_sample * scales[j] * np.sqrt(1./ratios[i])

      # anchor boxes ë³€ìˆ˜ì— ì „ì²´ anchor boxì˜ ì¢Œí‘œ ì €ì¥
      anchor_boxes[index,1] = ctr_y - h/2
      anchor_boxes[index, 0] = ctr_x - w / 2.
      anchor_boxes[index, 3] = ctr_y + h / 2.
      anchor_boxes[index, 2] = ctr_x + w / 2.
      index += 1

```

ìƒì„±í•œ anchor boxë“¤ì„ ì›ë³¸ ì´ë¯¸ì§€ì— ì‹œê°í™” í•´ë³¼ ê±´ë°, ì´ë•Œ ì´ë¯¸ì§€ì˜ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ëŠ” anchor boxë“¤ë„ ìˆê¸° ë•Œë¬¸ì— ì›ë³¸ ì´ë¯¸ì§€ì— paddingì„ ì¶”ê°€í•œë‹¤.


```python
# padding ì¶”ê°€
img_clone3 = np.copy(img)
img_clone4 = cv2.copyMakeBorder(img_clone3,400,400,400,400,cv2.BORDER_CONSTANT,value=(255,255,255))
img_clone5 = np.copy(img_clone4)

# ëª¨ë“  anchor boxes ê·¸ë¦¬ê¸°
for i in range(len(anchor_boxes)):
  x1 = int(anchor_boxes[i][0])
  y1 = int(anchor_boxes[i][1])
  x2 = int(anchor_boxes[i][2])
  y2 = int(anchor_boxes[i][3])

  cv2.rectangle(img_clone5, (x1+400,y1+400),(x2+400,y2+400),color=(255,0,0), thickness=3)

plt.figure(figsize=(10,10))
plt.subplot(121), plt.imshow(img_clone4)
plt.subplot(122), plt.imshow(img_clone5)
plt.show()
```


    
![7](https://user-images.githubusercontent.com/77332628/212281559-44b3f290-81a3-4097-a592-8fad861bcf7d.png)
    


### 3. Anchor Target Layer
Anchor Target Layerì—ì„œëŠ” RPNì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œ ì ì ˆí•œ anchor boxë¥¼ ì„ íƒí•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. ë¨¼ì € ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¶€ì— ìˆëŠ” anchor box ë§Œì„ ì„ íƒí•˜ì.


```python
# anchor boxì˜ ì¢Œí‘œê°€ (x1,y1) >= 0 ì´ê³  (x2,y2) <= 800ì¸ ê²½ìš°ë§Œ ì„ íƒ

index_inside = np.where((anchor_boxes[:,0] >= 0)&
                        (anchor_boxes[:,1] >= 0)&
                        (anchor_boxes[:,2] <= 800)&
                        (anchor_boxes[:,3] <= 800))[0]

valid_anchor_boxes = anchor_boxes[index_inside]
print('number of valid anchor boxes : ',valid_anchor_boxes.shape[0])
```

    number of valid anchor boxes :  8940


ê·¸ ë‹¤ìŒ ì „ì²´ anchor boxì— ëŒ€í•´ì„œ ground truth boxì™€ IoUê°’ì„ êµ¬í•œë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œ êµ¬í–ˆë“¯ì´ ìœ íš¨í•œ anchor boxëŠ” 8940ê°œì´ê³  ground truth ê°’ì€ 4ê°œì´ê¸° ë•Œë¬¸ì— í•œ 8940ê°œì˜ [IoU with gt box1,IoU with gt box2,IoU with gt box3,IoU with gt box4]ì˜ ê°’ì´ ë‚˜ì™€ì•¼ í•œë‹¤.


```python
ious = np.empty((len(valid_anchor_boxes),4),dtype=np.float32)
ious.fill(0) # ì¼ë‹¨ ë”ë¯¸ ë°°ì—´ ë§Œë“¤ì–´ë†“ê¸°

# anchor box ì˜ì—­ êµ¬í•˜ê¸°
for i, anchor_box in enumerate(valid_anchor_boxes):
  xa1, ya1, xa2, ya2 = anchor_box
  anchor_area = (xa2-xa1) * (ya2-ya1)
  # gt box ì˜ì—­ êµ¬í•˜ê¸°
  for j, gt_box in enumerate(bbox):
    xb1, yb1, xb2, yb2 = gt_box
    box_area = (xb2-xb1) * (yb2 - yb1)

    # ê²¹ì¹˜ëŠ” ì˜ì—­ì˜ ì¢Œí‘œ
    inter_x1 = max([xb1,xa1])
    inter_y1 = max([yb1,ya1])
    inter_x2 = min([xb2,xa2])
    inter_y2 = min([yb2,ya2])

    if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):
      inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
      iou = inter_area / (anchor_area + box_area - inter_area)
    
    else : 
      iou = 0
    
    ious[i,j] = iou

print(ious.shape)
print(ious[8930:8940,:]) # ë§ˆì§€ë§‰ 10ê°œ ì˜ˆì‹œë¡œ ì¶œë ¥
```

    (8940, 4)
    [[0.         0.         0.         0.37780452]
     [0.         0.         0.         0.33321926]
     [0.         0.         0.         0.29009855]
     [0.         0.         0.         0.24967977]
     [0.         0.         0.         0.2117167 ]
     [0.         0.         0.         0.17599213]
     [0.         0.         0.         0.14231375]
     [0.         0.         0.         0.11051063]
     [0.         0.         0.         0.08043041]
     [0.         0.         0.         0.05193678]]


ì´ì œ ê° gt boxë“¤ê³¼ ìµœëŒ€ iouë¥¼ ê°€ì§€ëŠ” anchor boxê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ì.


```python
gt_argmax_ious = ious.argmax(axis=0)

gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]

gt_argmax_ious = np.where(ious == gt_max_ious)[0]
print(gt_argmax_ious)
```

    [1008 1013 1018 1226 1232 1238 2862 2869 2876 3108 3115 3122 3336 3343
     3350 3354 3357 3361 3364 3368 3371 3377 3383 3389 3600 3607 3614 3846
     3853 3860 5935 5942 6164 6171 6178 6181 6185 6188 6192 6198 6427 6434
     8699 8703 8707]


ê° anchor boxë§ˆë‹¤ì˜ ì–´ë–¤ gt boxì™€ì˜ iouê°€ ê°€ì¥ ë†’ì€ì§€ ì•Œì•„ë³´ê³ , ê°€ì¥ ë†’ì€ iou ê°’ì„ ì¶œë ¥í•´ë³´ì.


```python
argmax_ious = ious.argmax(axis=1)
print(argmax_ious)

max_ious = ious[np.arange(len(index_inside)), argmax_ious]
print(max_ious)
```

    [0 0 0 ... 3 3 3]
    [0.         0.         0.         ... 0.11051063 0.08043041 0.05193678]


ê·¸ ë‹¤ìŒ ê° gt boxì™€ IoUê°€ ê°€ì¥ í° anchor boxì™€ IoU ê°’ì´ 0.7 ì´ìƒì¸ anchor boxëŠ” positive sampleë¡œ, IoU ê°’ì´ 0.3 ë¯¸ë§Œì¸ anchor boxëŠ” negative samplefh ì €ì¥í•œë‹¤. ë¨¼ì € ë”ë¯¸ label ë°°ì—´ì„ ë§Œë“¤ê³  label ë³€ìˆ˜ì— positiveì¼ ê²½ìš° 1, negativeì¼ ê²½ìš° 0ìœ¼ë¡œ ì €ì¥í•œë‹¤.


```python
# ë”ë¯¸ label ë°°ì—´ ë§Œë“¤ê¸°
label = np.empty((len(index_inside),), dtype=np.int32)
label.fill(-1)

# positive, negative sample ê¸°ì¤€ IoU
pos_iou_threshold = 0.7
neg_iou_threshold = 0.3

label[gt_argmax_ious] = 1
label[max_ious >= pos_iou_threshold] = 1
label[max_ious < neg_iou_threshold] = 0
```

ì´ì œ mini-batchë¥¼ êµ¬ì„±í• ê±´ë°, í¬ê¸°ëŠ” 256, positive/negative sampleì˜ ë¹„ìœ¨ì€ 1:1ë¡œ êµ¬ì„±í•œë‹¤. ë§Œì•½ positive sampleì´ 128ê°œ ì´ìƒì´ë©´ ë‚¨ëŠ” postive sampleì— í•´ë‹¹í•˜ëŠ” label ë³€ìˆ˜ëŠ” -1ë¡œ ì§€ì •í•œë‹¤. negative sampleì— ëŒ€í•´ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ìˆ˜í–‰í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ positive samleì˜ ìˆ˜ê°€ 128ê°œ ë¯¸ë§Œì¼ ê²½ìš°, ë¶€ì¡±í•œ ë§Œí¼ì˜ sampleì„ negative sampleì—ì„œ ì¶”ì¶œí•œë‹¤.


```python
n_sample = 256
pos_ratio = 0.5
n_pos = pos_ratio * n_sample

pos_index = np.where(label==1)[0]

if len(pos_index) > n_pos :
  disable_index = np.random.choice(pos_index,
                                   size = (len(pos_index) - n_pos),
                                   replace = False)
  label[disable_index] = -1

n_neg = n_sample * np.sum(label == 1)
neg_index = np.where(label == 0)[0]

if len(neg_index) > n_neg:
    disable_index = np.random.choice(neg_index, 
                                    size = (len(neg_index) - n_neg), 
                                    replace = False)
    label[disable_index] = -1
```

ê·¸ ë‹¤ìŒì€ ê° valid anchor boxê°€ ìµœëŒ€ IoUë¥¼ ê°€ì§€ëŠ” gt objectionì˜ ì¢Œí‘œë¥¼ ì•Œì•„ë‚´ê³ , valid anchor boxì˜ ì¢Œí‘œë¥¼ [x1,y1,x2,y2] í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œë‹¤.


```python
# convert the format of valid anchor boxes [x1, y1, x2, y2]

# For each valid anchor box, find the groundtruth object which has max_iou 
max_iou_bbox = bbox[argmax_ious]
print(max_iou_bbox)

height = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]
width = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]
ctr_y = valid_anchor_boxes[:, 1] + 0.5 * height
ctr_x = valid_anchor_boxes[:, 0] + 0.5 * width

base_height = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]
base_width = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]
base_ctr_y = max_iou_bbox[:, 1] + 0.5 * base_height
base_ctr_x = max_iou_bbox[:, 0] + 0.5 * base_width

eps = np.finfo(height.dtype).eps
height = np.maximum(height, eps)
width = np.maximum(width, eps)

dy = (base_ctr_y - ctr_y) / height
dx = (base_ctr_x - ctr_x) / width
dh = np.log(base_height / height)
dw = np.log(base_width / width)

anchor_locs = np.vstack((dx, dy, dw, dh)).transpose()
print(anchor_locs)
```

    [[ 89 469 249 644]
     [ 89 469 249 644]
     [ 89 469 249 644]
     ...
     [617 462 765 637]
     [617 462 765 637]
     [617 462 765 637]]
    [[ 1.24848541  2.49973296  0.56971714 -0.03381788]
     [ 1.24848541  2.41134461  0.56971714 -0.03381788]
     [ 1.24848541  2.32295626  0.56971714 -0.03381788]
     ...
     [-0.5855728  -0.63252911  0.4917556  -0.03381788]
     [-0.5855728  -0.72091746  0.4917556  -0.03381788]
     [-0.5855728  -0.80930581  0.4917556  -0.03381788]]



```python
# First set the label=-1 and locations=0 of the 22500 anchor boxes, 
# and then fill in the locations and labels of the 8940 valid anchor boxes
# NOTICE: For each training epoch, we randomly select 128 positive + 128 negative 
# from 8940 valid anchor boxes, and the others are marked with -1

anchor_labels = np.empty((len(anchor_boxes),), dtype=label.dtype)
anchor_labels.fill(-1)
anchor_labels[index_inside] = label
print(anchor_labels.shape)
print(anchor_labels)

anchor_locations = np.empty((len(anchor_boxes),) + anchor_boxes.shape[1:], dtype=anchor_locs.dtype)
anchor_locations.fill(0)
anchor_locations[index_inside, :] = anchor_locs
print(anchor_locations.shape)
print(anchor_locations[:10, :])
```

    (22500,)
    [-1 -1 -1 ... -1 -1 -1]
    (22500, 4)
    [[0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]
     [0. 0. 0. 0.]]


### 4. RPN (Region Proposal Network)

![11111](https://user-images.githubusercontent.com/77332628/212281576-d088d853-2f95-4215-b473-5ce0addb5626.png)

ì´ì œ RPNì„ ì •ì˜í• ê±´ë°, 1.Feature extractionì„ í†µí•´ ìƒì„±ëœ feature mapì— 3x3 conv ì—°ì‚°ì„ ì ìš©í•˜ëŠ” layerë¥¼ ì •ì˜í•˜ê³ , 1x1 conv ì—°ì‚°ì„ ì ìš©í•´ì„œ 9x4(anchor box ì¢…ë¥˜ ìˆ˜ x bounding box ì¢Œí‘œ)ê°œì˜ channelì„ ê°€ì§€ëŠ” feature mapì„ ë°˜í™˜í•˜ëŠ” Bounding box regressorë¥¼ ì •ì˜í•œë‹¤. ê·¸ë¦¬ê³  1x1 conv ì—°ì‚°ì„ ì ìš©í•´ì„œ 9x2(anchor box ìˆ˜ x object ì¡´ì¬ ì—¬ë¶€)ê°œì˜ channelì„ ê°€ì§€ëŠ” feature mapì„ ë°˜í™˜í•˜ëŠ” Classifierë¥¼ ì •ì˜í•œë‹¤.


```python
in_channels = 512
mid_channels = 512
n_anchor = 9

conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(DEVICE)
conv1.weight.data.normal_(0, 0.01)
conv1.bias.data.zero_()

# bounding box regressor
reg_layer = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0).to(DEVICE)
reg_layer.weight.data.normal_(0, 0.01)
reg_layer.bias.data.zero_()

# classifier(object or not)
cls_layer = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0).to(DEVICE)
cls_layer.weight.data.normal_(0, 0.01)
cls_layer.bias.data.zero_()
```




    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])



ì´ì œ ëª¨ë¸ì„ êµ¬ì¶•í• ê±´ë°, feature extractionì„ í†µí•´ì„œ ì–»ì€ feature mapì„ 3x3 conv layerì— ì…ë ¥í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ ì–»ì€ 50x50x512 í¬ê¸°ì˜ feature mapì„ BBR, Classifierì— ì£¼ì…í•´ì„œ bounding box coefficients (pred_anchor_locs ë³€ìˆ˜)ì™€ objectness score (pred_cls_score ë³€ìˆ˜)ë¥¼ ì–»ëŠ”ë‹¤. ì´ë¥¼ target ê°’ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ì„œ ì ì ˆí•˜ê²Œ resize í•´ì¤€ë‹¤.


```python
x = conv1(output_map.to(DEVICE)) # output_map : feature extractionì—ì„œ ì–»ì€ feature map
pred_anchor_locs = reg_layer(x) # BBR ì¶œë ¥
pred_cls_scores = cls_layer(x) # Classifier ì¶œë ¥

print(pred_anchor_locs.shape, pred_cls_scores.shape)

```

    torch.Size([1, 36, 50, 50]) torch.Size([1, 18, 50, 50])


ìœ„ ì½”ë“œì˜ ì¶œë ¥ê°’ì˜ í˜•ì‹ì„ ë°”ê¿”ì£¼ì.
* BBRì˜ ìœ„ì¹˜ : [1,36,50,50] => [1,22500(=50x50x9),4] (dy,dx,dh,dw)
* Classification : [1,18,50,50] => [1,22500,2] (1,0)



```python
pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)
print(pred_anchor_locs.shape)

pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()
print(pred_cls_scores.shape)

objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)

pred_cls_scores = pred_cls_scores.view(1, -1, 2)
print(pred_cls_scores.shape)
```

    torch.Size([1, 22500, 4])
    torch.Size([1, 50, 50, 18])
    torch.Size([1, 22500, 2])


ì´ì œ ì†ì‹¤ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ BBRê³¼ Classificationì— ëŒ€í•œ ground truth ê°’ì„ ì •ì˜í•˜ì.


```python
# RPNì˜ ì¶œë ¥ê°’
rpn_loc = pred_anchor_locs[0]
rpn_score = pred_cls_scores[0]

# ground truth ê°’
gt_rpn_loc = torch.from_numpy(anchor_locations)
gt_rpn_score = torch.from_numpy(anchor_labels)

print(rpn_loc.shape)
print(rpn_score.shape)
print(gt_rpn_loc.shape)
print(gt_rpn_score.shape)
```

    torch.Size([22500, 4])
    torch.Size([22500, 2])
    torch.Size([22500, 4])
    torch.Size([22500])


#### 4.1 Multi-task Loss
ì´ì œ RPNì˜ ì†ì‹¤ê°’ì„ ê³„ì‚°í•˜ëŠ” Multi-taks lossë¥¼ êµ¬í˜„í•´ë³´ì. Classification lossëŠ” cross entropy lossë¥¼ í™œìš©í•´ì„œ êµ¬í•œë‹¤. BBR lossëŠ” ì˜¤ì§ positiveì— í•´ë‹¹í•˜ëŠ” sampleì— ëŒ€í•´ì„œë§Œ lossë¥¼ ê³„ì‚°í•˜ë¯€ë¡œ, positive/negative ì—¬ë¶€ë¥¼ ì €ì¥í•˜ëŠ” ë°°ì—´ì¸ maskë¥¼ ìƒì„±í•˜ê³  ì´ë¥¼ í™œìš©í•´ì„œ Smooth L1 lossë¥¼ ê³„ì‚°í•œë‹¤. Classification lossì™€ BBR loss ì‚¬ì´ë¥¼ ì¡°ì •í•˜ëŠ” balancing parameter Î» = 10ìœ¼ë¡œ ì§€ì •í•˜ê³  ë‘ lossë¥¼ ë”í•´ì„œ multi-task lossë¥¼ êµ¬í˜„í•œë‹¤.


```python
rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long().to(DEVICE), ignore_index = -1)
print(rpn_cls_loss)
```

    tensor(0.6945, grad_fn=<NllLossBackward0>)



```python
# only positive samples
pos = gt_rpn_score > 0
mask = pos.unsqueeze(1).expand_as(rpn_loc)

# positive labelsë¥¼ ê°–ëŠ” bounding boxë§Œ ì‚¬ìš©
mask_loc_preds = rpn_loc[mask].view(-1, 4)
mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)

x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())
rpn_loc_loss = ((x < 1).float() * 0.5 * x ** 2) + ((x >= 1).float() * (x - 0.5))
print(rpn_loc_loss.sum())
```

    tensor(5.5637, dtype=torch.float64, grad_fn=<SumBackward0>)



```python
# combine rpn_cls_loss and rpn_reg_loss

rpn_lambda = 10
N_reg = (gt_rpn_score > 0).float().sum()
rpn_loc_loss = rpn_loc_loss.sum() / N_reg
rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)
print(rpn_loss)
```

    tensor(1.9309, dtype=torch.float64, grad_fn=<AddBackward0>)


### 5. Proposal Layer
Proposal Layerì—ì„œëŠ” Anchor generation layerì—ì„œ ìƒì„±ëœ anchor boxesì™€ RPNì—ì„œ ë°˜í™˜ëœ class scoresì™€ bounding box regressorë¥¼ ì‚¬ìš©í•´ì„œ region proposalsë¥¼ ì¶”ì¶œí•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. ë¨¼ì € score ë³€ìˆ˜ì— ì €ì¥ëœ objectness scoreë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„ objectness score ìƒìœ„ N(n_train_pre_nms=1200)ê°œì˜ anchor boxì— ëŒ€í•˜ì—¬ Non Maximum Suppression ì•Œê³ ë¦¬ì¦˜ì„ ìˆ˜í–‰í•œë‹¤. ë‚¨ì€ anchor box ì¤‘ ìƒìœ„ N(n_train_post_nms=2000)ê°œì˜ region proposalsë¥¼ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•œë‹¤. 

ë¨¼ì € anchor boxë¥¼ RPNì—ì„œ êµ¬í•œ ê°’ë“¤ì„ ì´ìš©í•´ì„œ ë³€í™˜ì‹œì¼œì„œ RoIë¥¼ êµ¬í•˜ì.


```python
nms_thresh = 0.7
n_train_pre_nms = 12000
n_train_post_nms = 2000
n_test_pre_nms = 6000
n_test_post_nms = 300
min_size = 16

# anchor boxë¥¼ [x1,y1,x2,y2]ì—ì„œ [ctrx,ctry,w,h]ë¡œ ë³€í™˜
anc_height = anchor_boxes[:,3] - anchor_boxes[:,1]
anc_width = anchor_boxes[:,2] - anchor_boxes[:,0]
anc_ctrx = anchor_boxes[:,1] + 0.5 * anc_height
anc_ctry = anchor_boxes[:,0] + 0.5 * anc_width

# 22500ê°œì˜ anchor boxes locationê³¼ objectness scoreë¥¼ numpyë¡œ ë³€í™˜
pred_anchor_locs_numpy = pred_anchor_locs[0].cpu().data.numpy()
objectness_score_numpy = objectness_score[0].cpu().data.numpy()

dy = pred_anchor_locs_numpy[:,1::4]
dx = pred_anchor_locs_numpy[:,0::4]
dh = pred_anchor_locs_numpy[:,3::4]
dw = pred_anchor_locs_numpy[:,2::4]

# BBRì„ ì´ìš©í•´ì„œ RoIì˜ ctr_x, ctr_y, h, w êµ¬í•˜ê¸°
ctr_x = dx * anc_height[:, np.newaxis] + anc_ctrx[:, np.newaxis]
ctr_y = dy * anc_width[:, np.newaxis] + anc_ctry[:, np.newaxis]
h = np.exp(dh) * anc_height[:, np.newaxis]
w = np.exp(dw) * anc_width[:, np.newaxis]

# RoIì˜ ì¢Œí‘œ
roi = np.zeros(pred_anchor_locs_numpy.shape, dtype = anchor_locs.dtype)
roi[:, 0::4] = ctr_x - 0.5 * w 
roi[:, 1::4] = ctr_y - 0.5 * h
roi[:, 2::4] = ctr_x + 0.5 * w
roi[:, 3::4] = ctr_y + 0.5 * h
```

ê·¸ë¦¬ê³  êµ¬í•œ RoIë“¤ì„ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ì¶°ì„œ ìµœì†Œ ìµœëŒ“ê°’ì„ ì œí•œí•´ì£¼ì.


```python
img_size = (800,800)
roi[:,slice(0,4,2)] = np.clip(roi[:,slice(0,4,2)],0,img_size[0]) # x1,x2ê°’
roi[:,slice(1,4,2)] = np.clip(roi[:,slice(1,4,2)],0,img_size[1]) # y1,y2ê°’

# heightë‚˜ widthê°€ min_size = 16 ë³´ë‹¤ ì‘ì€ predicted boxëŠ” ë²„ë¦°ë‹¤.
hs = roi[:,3] - roi[:,1]
ws = roi[:,2] - roi[:,0]

keep = np.where((hs >= min_size) & (ws >= min_size))[0]
roi = roi[keep,:]
score = objectness_score_numpy[keep]
print(keep.shape, roi.shape, score.shape)
```

    (22490,) (22490, 4) (22490,)


ìƒìœ„ 12000ê°œì˜ objectness scoreì¸ anchor boxesë§Œ ì‚¬ìš©í•œë‹¤.


```python
# ë‚´ë¦¼ì°¨ìˆœ
order = score.ravel().argsort()[::-1]

order = order[:n_train_pre_nms] # ìƒìœ„ 12000ê°œ
roi = roi[order, :]
```

ì´ì œ 2000ê°œì˜ bounding boxë¥¼ ì„ íƒí•˜ëŠ” Non maximun suppressionì„ ìˆ˜í–‰í•˜ì.


```python
x1 = roi[:,0]
y1 = roi[:,1]
x2 = roi[:,2]
y2 = roi[:,3]

areas = (x2-x1 +1) * (y2-y1 +1)
order = order.argsort()[::-1]
keep = []

while (order.size>0):
  i = order[0]
  keep.append(i)

  xx1 = np.maximum(x1[i], x1[order[1:]])
  yy1 = np.maximum(y1[i], y1[order[1:]])
  xx2 = np.minimum(x2[i], x2[order[1:]])
  yy2 = np.minimum(y2[i], y2[order[1:]])

  w = np.maximum(0.0, xx2 - xx1 + 1)
  h = np.maximum(0.0, yy2 - yy1 + 1)  

  inter = w * h
  ovr = inter / (areas[i] + areas[order[1:]] - inter)
  inds = np.where(ovr <= nms_thresh)[0]
  order = order[inds + 1]

keep = keep[:n_train_post_nms]
roi = roi[keep]
print(len(keep), roi.shape)
```

    2000 (2000, 4)


### 6. Proposal Target Layer
Proposal target layerì˜ ëª©í‘œëŠ” proposal layerì—ì„œ ë‚˜ì˜¨ region proposals ì¤‘ì—ì„œ Fast RCNN ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ìœ ìš©í•œ sampleì„ ì„ íƒí•˜ëŠ” ê²ƒì´ë‹¤. í•™ìŠµì„ ìœ„í•´ì„œ 128ê°œì˜ sampleì„ mini-batchë¡œ êµ¬ì„±í•œë‹¤. ì´ë•Œ Proposal layerì—ì„œ ì–»ì€ anchor box ì¤‘ gt boxì™€ì˜ IoU ê°’ì´ 0.5 ì´ìƒì¸ boxë¥¼ positive sampleë¡œ, 0.5 ë¯¸ë§Œì¸ boxë¥¼ negative sampleë¡œ ì§€ì •í•œë‹¤. ì „ì²´ mini-batch sample ì¤‘ 25%, ì¦‰ 32ê°œê°€ positive sampleì´ ë˜ë„ë¡ êµ¬ì„±í•œë‹¤. positive sampleì´ 32ê°œ ë¯¸ë§Œì¸ ê²½ìš°ì—ëŠ” ë¶€ì¡±í•œ sampleì„ negative sampleì—ì„œ êµ¬í•œë‹¤.




```python
n_sample = 128 
pos_ratio = 0.25 
pos_iou_thresh = 0.5
neg_iou_thresh_hi = 0.5 
neg_iou_thresh_lo = 0.0
```

ë¨¼ì € IoUë¥¼ ê³„ì‚°í•œë‹¤.


```python
ious = np.empty((len(roi), bbox.shape[0]), dtype = np.float32)
ious.fill(0)

for num1, i in enumerate(roi):
  ya1, xa1, ya2, xa2 = i
  anchor_area = (ya2-ya1) * (xa2-xa1)

  for num2 , j in enumerate(bbox):
    yb1, xb1, yb2, xb2 = j
    box_area = (yb2-yb1) * (xb2-xb1)
    inter_x1 = max([xb1,xa1])
    inter_y1 = max([yb1,ya1])
    inter_x2 = min([xb2,xa2])
    inter_y2 = min([yb2,ya2])

    if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):
      inter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)
      iou = inter_area / (anchor_area + box_area - inter_area)
    else :
      iou = 0
    ious[num1, num2] = iou

print(ious.shape)
```

    (2000, 4)


ê·¸ë¦¬ê³  ê° region proposalì— ì–´ë–¤ gt boxê°€ ë†’ì€ IoUë¥¼ ê°€ì§€ëŠ”ì§€ ì°¾ê³  ì´ë¥¼ ì‚¬ìš©í•´ì„œ ê° region proposalì— ë ˆì´ë¸”ì„ ë‹¬ì•„ì¤€ë‹¤.


```python
gt_assignment = ious.argmax(axis=1)
max_iou = ious.max(axis=1)

print(gt_assignment)

gt_roi_label = labels[gt_assignment]
print(gt_roi_label)
```

    [0 3 0 ... 0 0 0]
    [1 1 1 ... 1 1 1]


ì´ì œ positive sampleê³¼ negative sampleì„ ë¶„ë¥˜í•´ë³´ì.


```python
pos_roi_per_image = 32
pos_index = np.where(max_iou >= pos_iou_thresh)[0]
pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))

if pos_index.size > 0:
  pos_index = np.random.choice(
      pos_index, size=pos_roi_per_this_image, replace=False)
  
print(pos_roi_per_this_image)
print(pos_index)
```

    32
    [ 712  662  821  851  917  805  656  803  701  964  704  732  774  740
      779  822 1151  651  713  691  784  847  752  690  900  857  710  693
      620  836  835  716]



```python
neg_index = np.where((max_iou < neg_iou_thresh_hi) &
                     (max_iou >= neg_iou_thresh_lo))[0]
neg_roi_per_this_image = n_sample - pos_roi_per_this_image
neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))

if neg_index.size > 0:
  neg_index = np.random.choice(
    neg_index, size = neg_roi_per_this_image, replace=False)
  
print(neg_roi_per_this_image)
print(neg_index)
```

    96
    [1858 1782 1339  388 1503  589 1090 1046  738  441 1053 1067 1989 1289
     1588 1110 1777  433  954 1292 1960 1159 1123  166  966  175 1387   38
     1196 1486  161 1526 1412 1183 1555  322  177 1611 1126 1332 1710 1089
     1810 1504 1038  543  977  658 1263  485  190 1482 1621 1803  168  508
     1180  232 1568 1887 1644  213  812   96 1282 1811 1661  898 1417 1351
       52 1873 1235 1169 1331  689 1688  367 1844 1634  444  121 1466  965
     1115  220 1998 1484  563  807 1676 1897 1264  141  275 1522]


ì´ì œ positive sampleê³¼ negative sampleì„ ì›ë³¸ ì´ë¯¸ì§€ì— ê°ê° ì‹œê°í™”í•´ë³´ì. 


```python
# display RoI samples with positive

img_clone = np.copy(img)

for i in range(pos_roi_per_this_image):
  x1, y1, x2, y2 = roi[pos_index[i]].astype(int)
  cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255,255,255),
                thickness=3)
  
for i in range(len(bbox)):
  cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), 
                color = (0, 255, 0), thickness=3)

plt.imshow(img_clone)
plt.show()
```


    
![8](https://user-images.githubusercontent.com/77332628/212281563-ad109df8-5135-4b86-a1e8-349c71ace133.png)
    



```python
# display RoI samples with negative

img_clone = np.copy(img)

plt.figure(figsize=(9, 6))

for i in range(neg_roi_per_this_image):
  x1, y1, x2, y2 = roi[neg_index[i]].astype(int)
  cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255, 255, 255),
                thickness=3)
  
for i in range(len(bbox)):
  cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), 
                color = (0, 255, 0), thickness=3)
  
plt.imshow(img_clone)
plt.show()
```


    
![9](https://user-images.githubusercontent.com/77332628/212281566-54a50258-f073-4346-a785-a34394dc3732.png)
    


ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ positiveì™€ negative sampleì„ í•©ì³ì„œ ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ë§Œë“¤ì.


```python
keep_index = np.append(pos_index, neg_index)
gt_roi_labels = gt_roi_label[keep_index]
gt_roi_labels[pos_roi_per_this_image:] = 0 # negative labelì€ 0ìœ¼ë¡œ ë§Œë“ ë‹¤.
sample_roi = roi[keep_index]
print(sample_roi.shape)
```

    (128, 4)



```python
bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]

width = sample_roi[:, 2] - sample_roi[:, 0]
height = sample_roi[:, 3] - sample_roi[:, 1]
ctr_x = sample_roi[:, 0] + 0.5 * width
ctr_y = sample_roi[:, 1] + 0.5 * height

base_width = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]
base_height = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]
base_ctr_x = bbox_for_sampled_roi[:, 0] + 0.5 * base_width
base_ctr_y = bbox_for_sampled_roi[:, 1] + 0.5 * base_height 

# transform anchor boxes

eps = np.finfo(height.dtype).eps
height = np.maximum(height, eps)
width = np.maximum(width, eps)

dx = (base_ctr_x - ctr_x) / width
dy = (base_ctr_y - ctr_y) / height
dw = np.log(base_width / width)
dh = np.log(base_height / height)

gt_roi_locs = np.vstack((dx, dy, dw, dh)).transpose()
print(gt_roi_locs.shape)
```

    (128, 4)


### 7. RoI Pooling
Feature extractorë¥¼ í†µí•´ì„œ ì–»ì€ feature mapê³¼ Proposal Target Layerì—ì„œ ì–»ì€ region proposalsë¥¼ ì´ìš©í•´ì„œ RoI Poolingì„ ìˆ˜í–‰í•˜ëŠ”ë°, output feature mapì˜ í¬ê¸°ê°€ 7x7ì´ ë˜ë„ë¡ í•œë‹¤.

ë¨¼ì € labelsì™€ bbox ì¢Œí‘œë¥¼ í•©ì¹˜ì.


```python
rois = torch.from_numpy(sample_roi).float()
roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)
roi_indices = torch.from_numpy(roi_indices).float()
print(rois.shape, roi_indices.shape)
```

    torch.Size([128, 4]) torch.Size([128])



```python
indices_rois = torch.cat([roi_indices[:,None],rois],dim=1)
xy_indices_rois = indices_rois[:,[0,2,1,4,3]]
indices_rois = xy_indices_rois.contiguous()
print(xy_indices_rois.shape)
```

    torch.Size([128, 5])


ê·¸ë¦¬ê³  RoI Poolingì„ êµ¬í˜„í•´ë³´ì.


```python
size = (7,7)
adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0],size[1])

output = []
rois = indices_rois.data.float()
rois[:, 1:].mul_(1/16.0) # sub-sampling ratio
rois = rois.long()
num_rois = rois.size(0)

for i in range(num_rois):
  roi = rois[i]
  im_idx = roi[0]
  im = output_map.narrow(0, im_idx, 1)[..., roi[1]:(roi[3]+1), roi[2]:(roi[4]+1)]
  tmp = adaptive_max_pool(im)
  output.append(tmp[0])

output = torch.cat(output, 0)

print(output.size())
```

    torch.Size([128, 512, 7, 7])


### 8. Fast R-CNN

![2222](https://user-images.githubusercontent.com/77332628/212281573-4a37c71a-cb0e-4da7-bef5-077bd264203d.png)


ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ RoI Poolingì„ í†µí•´ì„œ ì–»ì€ 7x7 í¬ê¸°ì˜ feature mapì„ ë°›ì„ fc layerë¥¼ ì •ì˜í•œë‹¤. ê·¸ë¦¬ê³  class ë³„ë¡œ bounding box coefficientsë¥¼ ì˜ˆì¸¡í•˜ëŠ” Bounding Box Regreesorì™€ class scoreë¥¼ ì˜ˆì¸¡í•˜ëŠ” Classifierë¥¼ ì •ì˜í•œë‹¤.


```python
# feed forward layerì— ì „ë‹¬í•  ìˆ˜ ìˆë„ë¡ outputì„ reshapeí•´ì¤€ë‹¤.
k = output.view(output.size(0),-1)

# 7x7x512 í¬ê¸°ì˜ feature mapì´ fc layerì— ì „ë‹¬ëœë‹¤.
roi_head_classifier = nn.Sequential(*[nn.Linear(25088,4096), nn.Linear(4096,4096)])
cls_loc = nn.Linear(4096, 2*4) # 1 class, 1 ë°°ê²½, 4 ì¢Œí‘œ
cls_loc.weight.data.normal_(0,0.01)
cls_loc.bias.data.zero_()

score = nn.Linear(4096,2) # 1 class, 1 ë°°ê²½

k = roi_head_classifier(k.to(DEVICE)) 
roi_cls_loc = cls_loc(k)
roi_cls_score = score(k)

```

ë‹¤ìŒìœ¼ë¡œ Fast RCNNì—ì„œì˜ classification lossì™€ Regression lossë¥¼ êµ¬í•˜ê³  ê²°í•©í•´ì„œ Multi-task lossë¥¼ ì •ì˜í•œë‹¤.


```python
# ì˜ˆì¸¡ê°’
print(roi_cls_loc.shape)
print(roi_cls_score.shape)

# ì •ë‹µê°’
print(gt_roi_locs.shape)
print(gt_roi_labels.shape)

gt_roi_labels
```

    torch.Size([128, 8])
    torch.Size([128, 2])
    (128, 4)
    (128,)





    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])




```python
# Classification Loss

# Converting ground truth to torch variable
gt_roi_loc = torch.from_numpy(gt_roi_locs)
gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()
print(gt_roi_loc.shape, gt_roi_label.shape)

#Classification loss
roi_cls_loss = F.cross_entropy(roi_cls_score.cpu(), gt_roi_label.cpu(), ignore_index=-1)
```

    torch.Size([128, 4]) torch.Size([128])



```python
# regression loss

n_sample = roi_cls_loc.shape[0]
roi_loc = roi_cls_loc.view(n_sample, -1, 4)

roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]

# for regression we use smooth l1 loss as defined in the Fast R-CNN paper
pos = gt_roi_label > 0
mask = pos.unsqueeze(1).expand_as(roi_loc)

# take those bounding boxes which have positive labels
mask_loc_preds = roi_loc[mask].view(-1, 4)
mask_loc_targets = gt_roi_loc[mask].view(-1, 4)

x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())
roi_loc_loss = ((x < 1).float() * 0.5 * x ** 2) + ((x >= 1).float() * (x - 0.5))
print(roi_loc_loss.sum())
```

    tensor(6.7210, dtype=torch.float64, grad_fn=<SumBackward0>)



```python
# Multi-task loss
roi_lambda = 10.
roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)
print(roi_loss)

total_loss = rpn_loss + roi_loss
print(total_loss)
```

    tensor([[1.0936, 0.9071, 3.3268, 0.7143],
            [1.1238, 0.7561, 1.3800, 0.7753],
            [0.7255, 0.7369, 1.5322, 1.0111],
            [0.7884, 1.1029, 1.7753, 0.7045],
            [0.7651, 1.3447, 2.1244, 0.8515],
            [1.2455, 0.6956, 0.7916, 1.5698],
            [0.7303, 2.9759, 0.9521, 1.9188],
            [0.8895, 1.2778, 1.2332, 1.5378],
            [1.3119, 1.4879, 0.7367, 1.3184],
            [0.6888, 0.9747, 0.7605, 2.4373],
            [0.8053, 1.0531, 0.7241, 1.7428],
            [1.6073, 1.8577, 0.9956, 0.6962],
            [0.7959, 0.7438, 1.0073, 3.1008],
            [0.7297, 0.7043, 1.9854, 0.7268],
            [1.9234, 1.1130, 0.7196, 0.7216],
            [1.0215, 0.9468, 1.7787, 0.8716],
            [1.1643, 0.7128, 1.1576, 1.2314],
            [0.8628, 0.8774, 0.6940, 1.9987],
            [0.7526, 0.7147, 1.5985, 0.6927],
            [0.7461, 0.9348, 1.0082, 1.9449],
            [0.7094, 0.8767, 1.0298, 1.3036],
            [0.9790, 0.8008, 2.4844, 0.8239],
            [0.6896, 0.7068, 3.3081, 0.6885],
            [0.7091, 0.8563, 0.7847, 1.6985],
            [0.6904, 0.7272, 3.3711, 0.8233],
            [0.9068, 0.6919, 0.7404, 0.8501],
            [0.8393, 0.7360, 0.7358, 0.7666],
            [0.8143, 3.5515, 1.8522, 1.0700],
            [0.7370, 2.3878, 1.1776, 2.2519],
            [1.0304, 0.8789, 0.7123, 0.7005],
            [1.1016, 0.7949, 2.5251, 4.2220],
            [1.1834, 0.7261, 1.5839, 1.1725]], dtype=torch.float64,
           grad_fn=<AddBackward0>)
    tensor([[3.0245, 2.8380, 5.2578, 2.6453],
            [3.0547, 2.6870, 3.3109, 2.7063],
            [2.6565, 2.6678, 3.4631, 2.9420],
            [2.7194, 3.0338, 3.7062, 2.6354],
            [2.6960, 3.2756, 4.0553, 2.7824],
            [3.1764, 2.6265, 2.7225, 3.5007],
            [2.6612, 4.9068, 2.8831, 3.8497],
            [2.8204, 3.2087, 3.1641, 3.4687],
            [3.2428, 3.4188, 2.6676, 3.2493],
            [2.6197, 2.9056, 2.6915, 4.3682],
            [2.7362, 2.9840, 2.6550, 3.6738],
            [3.5382, 3.7887, 2.9266, 2.6272],
            [2.7268, 2.6748, 2.9383, 5.0317],
            [2.6606, 2.6352, 3.9163, 2.6577],
            [3.8543, 3.0439, 2.6505, 2.6525],
            [2.9524, 2.8777, 3.7096, 2.8025],
            [3.0952, 2.6438, 3.0886, 3.1623],
            [2.7937, 2.8083, 2.6250, 3.9296],
            [2.6835, 2.6456, 3.5294, 2.6236],
            [2.6771, 2.8657, 2.9391, 3.8758],
            [2.6403, 2.8076, 2.9607, 3.2345],
            [2.9099, 2.7318, 4.4153, 2.7548],
            [2.6205, 2.6377, 5.2390, 2.6195],
            [2.6400, 2.7872, 2.7156, 3.6294],
            [2.6213, 2.6581, 5.3020, 2.7542],
            [2.8377, 2.6228, 2.6714, 2.7810],
            [2.7702, 2.6670, 2.6668, 2.6976],
            [2.7452, 5.4824, 3.7831, 3.0009],
            [2.6679, 4.3187, 3.1085, 4.1828],
            [2.9613, 2.8099, 2.6432, 2.6314],
            [3.0325, 2.7259, 4.4561, 6.1529],
            [3.1143, 2.6570, 3.5148, 3.1034]], dtype=torch.float64,
           grad_fn=<AddBackward0>)


ì´ë ‡ê²Œ í•´ì„œ Faster RCNNì„ ì‹¤ì œë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´¤ë‹¤. êµ‰ì¥íˆ ì½”ë“œë„ ê¸¸ê³ , ì„¤ëª…ë„ ìˆì—ˆê¸° ë•Œë¬¸ì— ê¸´ ê¸€ì´ ë˜ì—ˆì§€ë§Œ ê·¸ë˜ë„ Faster RCNNì´ ë™ì‘í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì¡°ê¸ˆì€ ìì„¸í•˜ê²Œ ì•Œê²Œ ëœê²ƒ ê°™ì•„ì„œ ì˜ë¯¸ ìˆëŠ” ì‹¤ìŠµì´ì—ˆë‹¤.

ì¶œì²˜

* https://www.google.com/search?q=How+FasterRCNN+works+and+step-by-step+PyTorch+implementation&oq=How+FasterRCNN+works+and+step-by-step+PyTorch+implementation&aqs=chrome..69i57j69i61l3&client=ubuntu&sourceid=chrome&ie=UTF-8 ì›ë³¸ ì½”ë“œ ì˜ìƒ

* https://herbwood.tistory.com/11 ì½”ë“œì˜ ì„¤ëª… ì°¸ê³ 
