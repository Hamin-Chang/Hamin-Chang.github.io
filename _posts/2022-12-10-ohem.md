---
title : '[OD/개념] 객체 탐지 - OHEM ⛏️ '
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## Online Hard Example Mining (OHEM) 논문 읽어보기

### 0. 들어가며

이번 글은 Online Hard Example Mining (OHEM)을 다룬 논문을 리뷰해보겠다. 일반적으로 Object Detection시, 객체를 포함하는 region proposals보다 배경 영역에 해당하는 region proposals가 더 많기 때문에 클래스 불균형이 발생한다. 이를 해결하기 위해 Hard Negative Mining을 사용했었는데, 이로 인해서 속도저하와 성능 향상에 한계가 발생하는 문제가 있다. 이러한 문제를 해결하기 위해 OHEM 방법이 고안되었다.

### 1. Hard Negative Mining

Hard Negative Mining은 모델이 잘못 예측한, 즉 어려운(hard) sample을 추출하는 방법이다. Object Detection 모델에서 positive sample은 객체가 있는 영역이고 negative sample은 배경 영역이다. 여기서 모델이 예측하기 어려운 sample은 주로 False Positive sample이다. False Negative sample은 모델의 객체가 포함된 영역만 detect하는 모델의 특성상 고려되지 않는다.

기존에는 Hard Negative Mining을 적용해서 어려운 sample을 추출하고 이를 학습 데이터에 포함시켜서 모델이 Fasle Positive 오류에 강건해지도록 학습시켰다. 이는 다음과 같이 동작한다.

![ohem1](https://user-images.githubusercontent.com/77332628/206851662-51dbcbe6-ec70-4a42-9aad-7da0591b1d36.jpeg)

1. 전체 region proposals 중에서 positive/negative sample을 적절히 섞어서 mini-batch로 구성하고 모델에 주입해서 모델을 학습시킨다.
2. Hard Negative Mining을 적용해서 학습된 모델이 False Positive로 판단한 sample들을 학습 데이터에 추가한다. 
3. 다음 epoch부터 모델은 False Positive sample이 ㅊ가된 mini-batch를 입력받아 학습한다.
4. 마지막 epoch까지 1~3 과정을 반복한다.

위의 이미지에서 빨간색은 positive, 파란색은 negative sample이다. 학습이 거듭될수록 점차 mini-batch에 파란색 박스들이 더해지면서 객체라고 판단할만큼 어려운 negative sample들이 추가된다.

#### 1.1.2 Hard Negative Mining의 문제점

첫번째 문제점은 모델이 False Positive를 판단하고 이를 학습 데이터에 추가하고 mini-batch를 구성하는 과정이 끝날 때까지 모델을 update할 수 없기 때문에 학습이 느려진다.

또한 positive/negative을 특정 비율에 맞춰서 mini-batch를 구성하고, positive와 negative를 구분하는 IoU 같은 특정 지표도 정의했는데, 논문의 저자는 지정해줘야 하는 하이퍼 파라미터가 많아서 실험자의 개입과 시행착오가 많이 필요하다고 지적했다.

### 2. OHEM
OHEM은 Hard Negative Mining과 달리 이미지에서 추출한 **모든** RoI(Region of Interest)들을 forward pass해서 loss를 계산하고, 높은 loss를 가지는 RoI들에 대해서만 backward pass를 수행하는 기법이다. 논문에서 설명한 Fast RCNN에 OHEM 기법을 적용해서 성능을 높이는 과정은 다음과 같다.

![ohem2](https://user-images.githubusercontent.com/77332628/206851665-8ec140c0-d7b1-4bf6-97ba-f9395d090617.png)

1. 이미지를 사전 훈련된 conv layer(=VGG16)에 주입해서 feature map을 얻는다.
2. Selective search로 얻은 모든 RoI와 (1)에서 얻은 feature map을 사용해서 RoI Pooling을 수행한다.
3. fc layer, Classifier, BBR을 거쳐 각각의 RoI별로 loss를 계산한다.
4. 상위 N개의 loss인 sample만 선택하고 이를 backward pass한다.

실제로 OHEM을 구현하기 위해선 두가지의 RoI Network를 구성한다. 
1. readonly network : forward pass시에만 메모리를 할당하고, 각 iteration마다 feature map이 주어지면 모든 RoI에 대해 loss를 계산한다.
2. Hard RoI Sampler : hard example만을 추출해서 일반적인 RoI Network에 입력한다. 오직 hard example에 대해서만 forward, backward pass를 수행하여 gradient를 축적해서 ConvNet에 전달한다.

### 3. Training Fast RCNN with OHEM

![ohem3](https://user-images.githubusercontent.com/77332628/206851669-7fb9db2b-8bf3-401d-8c32-c099275bc38a.png)

1) Selective Search

* Input : 원본 이미지
* Process : Selective search
* Output : Region Proposals

2) pre-trained VGG16
* Input : 원본 이미지
* Process : feature extraction by VGG16
* Output : feature maps

3) RoI Pooling

Selective Search를 통해 추출한 **모든** RoIs와 featuremap을 사용해서 RoI Pooling을 하면 RoI 수만큼의 feature map이 생성된다.

* Input : feature maps, **ALL** region proposals
* Process : RoI pooling
* Output : feature maps

4) readonly RoI Network

fc layer, classification , BBR로 구성된 readonly network는 forward pass를 통해 RoI들에 대한 loss를 계산한다.

* Input : feature maps
* Process : Calculate loss
* Output : RoI losses

5) Hard RoI sampler

Non Maximum Supperssion(NMS)를 통해 중복되는 sample을 제거하고 loss를 내림차순으로 정렬하고 상위 N개의 RoI만 선택한다.

* Input : RoIs , RoI losses
* Process : hard example sampling
* Output : hard examples(mini-batch)

6) RoI Pooling

(5)에서 얻은 hard example과 (2)에서 얻은 feature map을 사용해서 RoI Pooling을 수행한다. 이를 통해 hard example 수만큼의 feature map을 추출한다.
* Input : hard examples(mini-batch)
* Process : RoI Pooling
* Output : feature maps

7) Training Standard RoI Network

feature map을 입력받아서 기존 Fast RCNN의 fc layer , BBR, Classifier를 거쳐서 loss를 계산하고 backward pass를 통해 모델을 학습시키는데, 오직 hard example에 해당하는 RoI만이 학습에 이용된다.

* Input : feature maps
* Process : calculate loss 
* Output : losses

### 4. 마치며
OHEM 기법을 적용하면, 학습 도중에 sampling을 필요로하지 않기 때문에 모델의 학습 속도가 빨라진다는 장점이 있다. 또한 positive/negative의 비율을 조절하기 위한 하이퍼파라미터를 필요로하지 않다. 만약 특정 class의 sample이 backwardpass되지 않는다면 loss는 상승할것이다. 이는 다음 iteration때 이 sample이 hard example로 선택되어 backward pass되 가능성이 높아진다는 것이다. 또한 논문에서도 OHEM을 적용했을 때 성능이 약 3% 가량 높아졌다고 한다.

참고자료

[OHEM 논문](https://arxiv.org/pdf/1604.03540.pdf)

개인 블로그 (https://herbwood.tistory.com/12)
