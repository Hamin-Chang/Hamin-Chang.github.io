---
toc : true
title : '[DL/CV] 이미지 분류 - VGGNet 👇 '
layout : single
toc: true
toc_sticky: true
categories:
  - cv-imageclassification
---

## VGGNet 논문 리뷰

이번 글에서는 [<U>VGGNet 논문</U>](https://arxiv.org/pdf/1409.1556.pdf)(Very Deep Convolutional Networks for Large-Scale Image Recognition)을 리뷰해보도록 하겠다.

저자가 Abstract를 통해서 강조하듯 이 VGGNet 논문의 핵심은 깊이가 깊은 convolutional network의 성능이라고 볼 수 있다. 본 논문에서 VGGNet의 구조에 대해 설명하고 training & classification test 작업 방식에 대해 설명한다.

### 1. ConvNet Architecture

VGG-16의 구조는 다음과 같다.

이미지1

**1) Input image**
* 224x224로 size를 고정
* Input image에 대한 preprocessing은 RGB mean value를 빼주는 작업만 수행

**2) Conv layer**
* 3x3 Conv filter 사용 (이미지 요소의 left, right, up, down등을 파악할 수 있는 receptive field의 최소 크기) 
* 1x1 Conv filter도 사용 (차원 줄이고, non-linearity 증가 목적)
* Conv filter의 stride=1, padding=1

**3) Pooling layer**
* Conv layer 다음에 적용
* 2x2 size, stride=2인 총 5개의 max pooling layer로 구성

**4) FC layer**
* 처음 두 FC layer는 4,096 채널
* 마지막 FC layer는 1,000 채널 (ILSVRC에 사용된 class 수)

**6)** 마지막으로 **soft-max layer** 적용

etc) 
* 모든 hidden layerdp ReLU activation function 적용
* 아래 내용에서 다루겠지만 LRN은 효과가 없고 연산량만 늘리기 때문에 사용하지 않음

#### 1.1 3x3 Conv 사용 이유?

VGGNet 이전에 Convolutional Network를 활용해서 이미지 분류에서 좋은 성능을 낸 모델들은 비교적 큰 Receptive field를 갖는 11x11 필터나 7x7 필터를 포함한다. 그러면 왜 작은 receptive field의 3x3 Conv filter를 사용할까?

다음은 7x7 Conv filter를 사용했을 때이고,

이미지2

다음은 3x3 Conv filter를 사용했을 때이다.

이미지3

Stride=1 일 때, 3차례의 3x3 Conv filter를 적용한 feature map은 한 픽셀이 원본 이미지의 7x7 Receptive field의 효과를 볼 수 있다. 그럼 3번이나 Conv filter를 반복 적용하면 어떠한 장점들이 있을까?

1. 결정 함수의 비선형성 증가 : 각 Conv 연산은 ReLU 함수를 포함하기 때문에 7x7 Conv filter를 한번 적용한 것보다 3x3 Conv filter를 3번 적용하면 ReLU 함수가 더 많이 적용되어서 모델의 비선형성이 증가하게 되고 이는 모델의 특징 식별성 증가로 이어진다.

2. 학습 파라미터 수 감소 : 7x7 filter 1개의 학습 파라미터의 수는 7x7(=49)이고 3개의 3x3 filter의 학습 파라미터의 수는 3x3x3(=27)로 파라미터 수가 크게 감소한다.

물론 이 기법을 활용해서 무작정 네트워크의 깊이를 깊게 만드는 것이 항상 좋은 성능을 내지는 못한다. 깊은 모델을 거쳐 만들어진 feature map은 동일한 Receptive Field에 대해 더 추상적인 정보를 담게 되는데, 때로는 더 선명한 feature map이 필요할 때도 있기 때문이다.








### 2. Configurations

논문에서 저자는 depth에 따라 6개의 다른 모델로 실험을 했다고 한다. 모델명은 A ~ E라고 정의했고, 각각 layer가 16 ~ 19를 이루고 있다고 설명한다. 추가로 모델의 depth가 깊어짐에도 더 큰 conv layer를 사용한 얕은 신경망보다 오히려 파라미터 수가 줄어들었다고 설명했다.

이미지4

3x3 Conv filter를 사용했음에도 불구하고 Table 2에 나타나듯 상당수의 파라미터가 존재하고 있다. 이는 VGGNet에 마지막에 있는 3개의 FC layer 때문인데 이는 VGGNet의 단점이라고 볼 수 있다.

### 3. Training
먼저 VGGNet Training을 위해서 hyer-parameter를 다음과 같이 설정했다고 한다.

* Cost Function : Multinominal logistic regression objective (=Cross Entropy)
* Mini batch : 256 size
* Optimizer : Momentum 0.9
* Weight Decay : L2 Norm
* Dropout : 0.5
* Learning rate : $10^{-2}$로 시작해서 validation error rate가 높아질수록 $10^{-1}$씩 감소

이때, AlexNet보다 더 깊고 더 많은 parameter를 갖지만, 더 적은 epoch를 기록하였는데, 이는 다음 2가지 기법 덕분이다.

1. Implicit regularisation : 앞서 말했듯이 7x7 filter 1개 대신 3x3 filter 3개를 사용해서 파라미터 수가 줄어드는 효과가 있다.
2. Pre-initialization : Table 2에서 가장 간단한 모델인 A 모델을 먼저 학습시키고, 다음 모델을 구성할 때 A 모델에 학습된 layer를 사용해서 최적의 초기값을 설정해서 더 안정적인 모델 initialization을 꾀한다.

#### 3.1 Training image size

VGGNet을 학습시킬 때 가장 먼저하는 것은 training image를 VGG 모델의 input size에 맞게 image size를 조정해야 한다는 것이다. 사이즈 조정은 isotropically-rescaled training image를 224x224 size로 random하게 crop해서 수행한다.

이미지6

논문에서는 training scale을 'S'로 나타내는데, 원본 이미지의 넓이 높이 중에서 더 작은 쪽을 S로 줄여주는데, 이때  다음 이미지처럼 aspect ratio를 유지하며 다른 쪽도 rescaling 해주는데 이를 isotropically-rescaled했다고 한다.

이미지5

S를 설정하는데 두가지 방법을 사용한다.

1. Single-scale training : S를 256 or 384로 고정시키는 방법인데, S=384인 네트워크의 경우, 학습 속도를 높이기 위해 S=256로 설정하여 학습시킨 가중치값들을 기반으로 S=384로 설정해서 다시 학습시킨다. 이때, 256일 때 이미 많은 학습이 진행되었기 때문에 S=384일때는 learning rate를 줄이고 학습시킨다.

2. Multi-scale training : S를 256과 512 사이의 값으로 설정한다. 즉 256과 512 사이의 scale을 정할 수 있기 때문에 다양한 크기에 대한 대응이 가능해서 모델의 성능이 올라간다. 학습시킬 때는 S=384로 미리 학습시킨 후 S를 무작위로 선택해가며 fine tuning을 한다. S를 무작위로 학습시킨다고 하여 이것을 scale jittering이라고 한다. 이를 통해서 모델이 overfitting에 빠지는 것을 최대한 방지하려고 노력했다고 한다.

이미지7

(이미지를 256x256 크기로 변환 후 224x224 크기로 sampling)

이미지8

(이미지를 512x512 크기로 변환 후 224x224 크기로 sampling)

이를 통해서 다음의 효과를 얻을 수 있다.

1. 한정적인 데이터의 수를 늘릴 수 있다. (Data augmentation)
2. 하나의 object에 대한 다양한 측면을 학습 시 반영시킬 수 있다. 변환된 이미지가 작을수록 개체의 전체적인 측면을 학습할 수 있고, 변환된 이미지가 클수록 개체의 특정한 부분을 학습에 반영할 수 있다.

위 두가지 모두 Overfitting을 방지하는 데 도움을 준다.

실제로 VGG 연구팀의 실험 경과에 따르면 multi-scale training이 single-scale training보다 높은 분류 정확도를 나타냈다고 한다.

### 4. Testing

이미지9 

(이미지 출처 : https://www.geeksforgeeks.org/vgg-16-cnn-model/)

Training을 완료한 모델의 test 과정에서는 모델의 구조를 약간 바꿔서 사용한다. 위 이미지처럼 신경망의 마지막 FC layers를 Convolution으로 변환하여 사용한다. 첫 번째 FC layer는 7x7 Conv로, 마지막 두 FC layer는 1x1 Conv로 변환했다. 이런식으로 변환된 신경망을 Fully-Convolutional Networks라고 부른다. Training시에는 input image를 crop해야 하지만, Test시에는 conv layer를 사용하기 때문에 Uncropped image를 사용한다.

FC layer는 MLP(다중 퍼셉트론) 개념으로 입력 노드가 hyperparameter로 정해져 있기 때문에 항상 입력 이미지의 크기가 정해져 있지만, conv 연산에서는 이 부분에서 자유롭다. 이에 따라 하나의 입력 이미지를 다양한 스케일로 사용한 결과들을 앙상블하여 이미지 분류 정확도를 개선하는 것도 가능해진다. 
 
이미지10

위 이미지처럼 Test시에는 입력 이미지의 크기가 다양하기 때문에 입력 image size에 따라 output feature map size가 달라지는데, 여기서 1x1 사이즈가 아닌 feature map을 class score map이라고 한다. 만약 class score map가 7x7x1000인 경우, 다음 이미지처럼 sum-pooled(즉 mean or average pooling)을 적용한다. 이후, softmax를 거치고 flipped image와 original image의 평균값을 통해 최종 score를 출력한다. AlexNet과 비교하면 AlexNet은 10 augmented image를 사용하고 10개 이미지에 대한 평균을 취하기 때문에 속도가 매우 느려지지만 FC layer를 1x1 conv layer로 바꾸고 약간의 큰 사이즈의 이미지를 넣고 horizontal flipping만 적용했기 때문에 속도도 빠르고 성능마저 좋은 효과를 얻었다고 한다.

이미지11




### 5. Classification Experiments & 결론

이 부분에서는 실제 classification 실험 결과를 보여주는데, Dataset으로는 ILSVRC를 사용했고, 분류 성능은 top-1과 top-5 error 방식을 사용했는데, 이는 각각 multi-class classification error이고, top-5 error는 ILSVRC에서 요구하는 test 기준을 사용했다고 한다.

*또한 대부분의 실험에서 validation set을 test set으로 사용했다고 하는데, validation set은 이미 학습에 사용되었기 때문에 bias가 있을텐데도 test set으로 사용해도 되는지 의문이 들었다.*

1) Single scale Evaluation : test시 image size가 고정되어 있는 것을 의미하는데, Single-scale training인 경우에는 S=Q 사이즈로 test image size가 고정되고, Multi-scale training인 경우에는 Q=0.5(256+512)=384로 고정된다. [*여기서 Q는 test image scale*]

이미지12

위 표를 보면 더 깊은 모델일수록 더 좋은 성능을 내는 것을 알 수 있다. 또한 1x1 conv filter를 사용하는 C 모델보다 3x3 conv filter를 사용한 D 모델의 성능이 더 좋게 나왔는데, 이는 1x1 conv filter를 사용하면 non-linearity를 더 잘 표현할 수 있지만, 3x3 conv filter가 spatial context(공간 정보)의 특징을 더 잘 뽑아주기 때문이라고 한다.

2) Multi scale Evaluation : S가 고정된 경우는 {S-32,S,S+32}로 Q값을 변화시키면서 test를 진행하고 학습에 scale jittering을 적용한 경우는 Q를 {256,384,512}로 변화시키며 evaluation을 진행하는 방법이다. 아래 표를 보면 scale jittering을 적용했을 때 더 좋은 성능을 내고, single-scale보다는 multi-scale test의 성능이 더 높게 나온 것을 알 수 있다.

이미지13

3) Multi crop Evaluation

(1),(2)에서 언급한 dense evaluation 보다 multi-crop evaluation으로 validation을 진행한 결과 더 좋은 성능을 낸다는 것을 설명한다. 또한 두 evaluation은 상보적인 특성을 가지기 때문에 같이 적용을 해서 평균을 내면 더 좋은 성능을 내는 것을 아래표를 보면 알 수 있다.

이미지14

VGGNet 논문은 여러가지 기법을 통해서 모델의 깊이를 상당히 늘릴 수 있다는 것을 설명했고, 이를 통해 더욱 높은 분류 성능을 내는 모델을 만들 수 있다는 것을 알았다.

출처 및 참고문헌 :

VGGNet 논문 (https://arxiv.org/pdf/1409.1556.pdf)

개인 블로그 

(https://codebaragi23.github.io/machine%20learning/1.-VGGNet-paper-review/)

(https://velog.io/@d9249/VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION)

(https://phil-baek.tistory.com/entry/1-Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition-VGGNet-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)

(https://www.geeksforgeeks.org/vgg-16-cnn-mode)


