---
title : '[DL/Pytorch] 파이토치로 신경망 - torch.nn 🧠'
layout: single
toc: true
toc_sticky: true
categories:
  - pytorchBasic
---
## 파이토치로 신경망 만들기

### 1. 인공신경망
신경망은 복잡한 함수를 단순한 함수들의 합성으로 표현할 수 있는 딥러닝의 핵심적인 수학적 엔티티다. 복잡한 함수의 기본 빌딩 블럭은 뉴런이다. 뉴런은 단순히 입력에 대한 선형 변환과 **활성 함수**라고 부르는 고정된 비선형 함수를 적용하는 역할을 한다. 수학적으로는 $y = f(w*x+b)$로 표현할 수 있다. $f$는 활성함수, $x$와 $y$는 단순 스칼라값의 입력과 출력이고 $w$와 $b$는 가중치와 편향값(or 오프셋)이다. 여러 차원으로 가중치와 편향값을 가진 여러 개의 뉴런을 타나내는 경우에는 이런 표현식을 **뉴런 계층(layer)**이라고 한다.

![neuron1](https://user-images.githubusercontent.com/77332628/209757194-6ff30cdf-5111-4257-8809-fe3d01a18e1c.png)

출처(https://www.gttkorea.com/news/articleView.html?idxno=3135)

이제 다중 계층 신경망을 합성할 때는 다음과 같이 여러 뉴런으로 구성된 계층의 출력은 이어지는 다음 계층의 입력으로 사용된다. 

x_1 = f(w_0 * x + b_0)
x_2 = f(w_1 * x_1 + b_1)
...
y = f(w_n * x_n + b_n)

#### 1.1 오차함수
딥러닝과 선형모델의 주요한 차이점 중 하나는 오차 함수의 모양이다. 딥러닝 신경망의 오차 함수의 모양은 선형 모델과 다르게 볼록 형태가 아니다! 딥러닝 모델이 근사하려는 각 파라미터는 정해진 정답은 없고, 모든 뉴런이 협력해서 유용한 출력을 만들기 위해서 **근사**하는 과정을 거치는데, 이는 어느 정도의 불완전함을 뜻한다. 어디에서 어떻게 불완전성이 나타나는 것은 임의적이기 때문에, 출력은 제어하는 파라미터 역시 임의적인 측면이 있다. 그래서 신경망 훈련이 기계적인 파라미터 추정과 유사해 보이지만 이론적인 기반을 전혀 다르다는 사실을 기억하자.

#### 1.2 활성함수
활성함수는 다음의 두가지 중요한 역할을 한다.

1. 활성 함수는 출력 함수가 값마다 다른 기울기를 가지도록 만들고, 신경망이 여러 출력에 대해 각기 기울기가 다른 특성을 절묘하게 핪어해서 여러 복잡한 함수에 근사할 수 있게 된다.

2. 신경망의 마지막 계층은 이전의 선형 연산 출력을 주어진 범위로 모아준다. 예를 들어서 분류 문제를 다룰 때 '높은 점수'가 어느 정도인지 정의해야 하는데, 선형 연산만으로는 출력값이 알아서 특정 범위로 제한되지 않기 때문에 활성 함수의 역할이 중요하다.

* 출력 범위 제한하기
예를 들어서 10점 만점의 점수를 기준으로 잡고 모델을 만들면 출력들이 10점을 넘어서 12점이 나오거나 음의 값이 점수로 출력되는 경우가 없도록 해야한다. 이는 출력의 범위를 제한하면 되는데, **torch.nn.Hardtanh** 활성 함수를 사용해서 0 이하는 무조건 0으로 출력하고 10이상은 무조건 10으로 출력하도록 해야한다.

![neuron2](https://user-images.githubusercontent.com/77332628/209757199-23063ad7-0be2-490d-bce8-86539cbf5527.png)

출처(https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html)

* 출력 범위 줄이기
위와 비슷하고 잘 동작하는 활성 함수 중 하나로 **torch.nn.Sigmoid**가 있는데, 이는 입력값이 음의 무한대로 가면 0으로 가까워지고 양의 무한대로 가면 1에 가까워지는 곡선을 갖고, 입력값이 0일 때는 상수의 기울기를 갖기 때문에 뉴런에 민감하게 반응하는 함수의 중앙 영역에 선형 함수가 존재하고 다른 영역은 바로 경계값에 가까워지기 때문에 잘 동작한다.

![neuron3](https://user-images.githubusercontent.com/77332628/209757202-20321c0d-6e2f-453b-a033-a9a7cf21bb99.png)

출처(https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg)

따라서 점수가 굉장히 낮은 것들은 쉽게 0으로 분류되고 점수가 애매한(0에 가까운) 것들은 sigmoid 내의 선형 함수에서 민감하게 분류되고(점수의 작은 변화로도 결과가 달라짐) 점수가 굉장히 높은 것들은 쉽게 1로 분류된다.

이외에도 여러가지 활성함수가 존재한다.

![neuron4](https://user-images.githubusercontent.com/77332628/209757203-2ba55d95-d482-4962-b664-c50f0d2daba4.png)

(출처:https://velog.io/@jeewoo1025/%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98)

실제 연구에서 효과가 증명된 다양한 활성 함수가 위의 이미지보다 더 많기 때문에 활성함수는 흥미로운 영역이다. 활성 함수의 일반적인 특성은 다음과 같다.

1. 비선형이다. 활성함수의 비선형성이 전체 신경망이 더 복잡한 함수로 근사하도록 해준다.
2. 미분 가능하기 때문에 기울기 계산이 가능하다. (Hardtanh나 ReLU의 불연속점은 큰 문제가 되지 않는다.)
3. 최소한 하나의 민감한 구간을 가지며 입력에서 중요 범위가 변경되는 일치하는 출력 영역에서도 중요한 변화가 생기는데, 이 부분이 훈련에서 중요한 부분이다.
4. 대부분이 둔감한 구간이며, 이 구간에서는 입력의 변화가 출력에 거의 영향을 주지 않는다.

역전파가 어떻게 일어나는지를 생각해보면 3,4번의 특성이 입력이 응답 범위에 있으 때 오차가 활성 단계를 통해서 더 효과적으로 전파되고 반대로 포화된 입력은 오차가 뉴런에 큰 영향을 미치지 않는다는 것을 알 수 있다.

이제 우리는 어떻게 해서 많은 선형 + 활성 유닛 조합이 병렬로 연결된 후, 스택처럼 하나씩 쌓여서 복잡하고 정교한 함수에 근사할 수 있는 수학적 객체가 되는지를 좀 더 깊게 이해해보고자 한다. 

#### 1.3 신경망에서 학습의 의미
심층 신경망의 매력적인 부분은 데이터를 표현하는 정확한 함수에 대해서 크게 고민할 필요가 없다는 점이다. 심층 신경망은 고수준의 비선형적인 현상에 대해 명시적인 모델 없이 근사할 수 있는 능력이 있다. 명시적인 모델 대신 훈련되지 않은 일반 모델에서 출발하며 우리느 여러 입출력쌍 예제와 역전파할 손실 함수를 제공해서 일반 모델을 특정 작업에 최적화 하는데, 이를 **학습**이라고 한다. 

예를 들어서 이전글([**링크**](https://hamin-chang.github.io/pytorchbasic/pytrain1/))에서 다룬 온도 문제에서는 선형 모델이라는 명시적인 모델을 두고 입출력 함수의 형태를 하드코딩 했다. 그렇기 때문에 선 근처에 나열되지 않는 데이터 포인트는 근사할 수 없었다. 이런 문제는 문제가 더 복잡해지면 더 심각하게 발생할 것이다. 반대로 심층 신경망은 현상을 설명할 수 있는 모델을 만들어달라고 하는 대신, 충분히 다양한 입출력 관계에 대해 근사할 수 있는 다양한 함수군을 구성되기 때문에 현상을 설명하는 모델을 고민하는 일은 내려놓고, 얼마나 더 복잡한 문제들을 풀 수 있는지 고민할 여유를 준다.


### 2. 파이토치 nn 모듈
이제 파이토치에서 신경망을 구현해보자! 파이토치에는 torch.nn이라는 신경망 전용 서브 모듈이 존재한다. 이 모듈에는 모든 신경망 아키텍처를 만들 수 있는 **모듈**이라고 불리는 빌딩 블럭이 들어있다. 모듈은 하나 이상의 Parameter 객체를 인자로 받는데, 이는 텐서 타입이고 훈련 과정을 통해서 값이 최적화된다. 

nn.Linear 서브 클래스를 사용해서 저번 글([**링크**](https://hamin-chang.github.io/pytorchbasic/pytrain1/))에서 구현한 것처럼 입력에 선형 모델을 통해서 아핀 변환을 적용해보자.

#### 2.1 선형 모델에 nn 적용하기
nn.Linear 생성자는 3개의 인자를 받는다. 입력 피처 수와 출력 피처 수 그리고 편향 값을 포함하는지 여부(기본값 True)를 받는다.


```python
%matplotlib inline
import numpy as np
import torch
import torch.optim as optim

torch.set_printoptions(edgeitems=2, linewidth=75)
t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c).unsqueeze(1) # <1>
t_u = torch.tensor(t_u).unsqueeze(1) # <1>

t_u.shape
torch.Size([11, 1])
n_samples = t_u.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indices = torch.randperm(n_samples)

train_indices = shuffled_indices[:-n_val]
val_indices = shuffled_indices[-n_val:]

t_u_train = t_u[train_indices]
t_c_train = t_c[train_indices]

t_u_val = t_u[val_indices]
t_c_val = t_c[val_indices]

t_un_train = 0.1 * t_u_train
t_un_val = 0.1 * t_u_val

```


```python
import torch.nn as nn

linear_model = nn.Linear(1,1) # 입출력 텐서 크기 모두 1, 편향값 포함(기본값 True)
linear_model(t_un_val)
```




    tensor([[0.1468],
            [0.0940]], grad_fn=<AddmmBackward0>)



이제 하나의 입력 피처와 하나의 출력 피처를 가진 nn.Linear 인스턴스를 생성했다. 이 인스턴스는 하나의 가중치와 편향값을 요구한다.


```python
linear_model.weight
```




    Parameter containing:
    tensor([[0.0458]], requires_grad=True)




```python
linear_model.bias
```




    Parameter containing:
    tensor([-0.1301], requires_grad=True)



이제 입력값으로 모듈을 호출하면 모듈이 가지고 있는 가중치와 편향값을 이용해서 출력값을 계산한다.


```python
x = torch.ones(1)
linear_model(x)
```




    tensor([-0.0843], grad_fn=<AddBackward0>)



#### 2.2 배치 최적화
배치(Batch)를 수행하는 여러 이유중 주요한 이유 하나는 연산량을 충분히 크게 만들어서 준비할 자원을 최대한 활용하기 위함이다. 병렬 연산에 최적화된 GPU에 작은 모델을 넣고 입력을 하나만 수행하면 대부분의 자원을 놀게 되기 때문에 여러 입력을 묶어서 하나를 배치로 한번에 실행하면, 놀고 있는 다른 유닛도 계산에 사용할 수 있도록 만들 수 있다. 

두개의 1차원 텐서 t_u와 t_c로 만들어진 크기가 B인 저번 글([**링크**](https://hamin-chang.github.io/pytorchbasic/pytrain1/))에서 다룬 온도 문제 데이터로 돌아가보자. B개의 입력을 B x Nin으로 reshape해서 B개의 배치를 만들면 Nin은 1로 설정이 된다. 이는 unsqueeze로 쉽게 구현이 가능하다. 




```python
t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c).unsqueeze(1) # <1> 1번 축에 여분의 차원을 추가
t_u = torch.tensor(t_u).unsqueeze(1) # <1>

t_u.shape
```




    torch.Size([11, 1])



이제 저번글([**링크**](https://hamin-chang.github.io/pytorchbasic/pytrain1/))에서 작성한 학스보드를 업데이트하자. 직접 작성했던 모델을 nn.Linear(1,1)로 바꾸고 옵티마이저에서 선형 모델 파라미터를 전달한다.


```python
linear_model = nn.Linear(1,1)
optimizer = optim.SGD(linear_model.parameters(), # [params] 부분 대신
                      lr = 1e-2)
```


```python
list(linear_model.parameters())
```




    [Parameter containing:
     tensor([[0.0187]], requires_grad=True), Parameter containing:
     tensor([0.2354], requires_grad=True)]



parameters 매소드를 호출하면 모듈의 init 생성자에 정의된 서브 모듈까지 재귀적으로 호출하며 만나는 모든 파라미터 리스트를 담은 리스트를 반환한다. 옵티마이저에 텐서 리스트가 전달되는데, 텐서는 Parameters를 가지며 경사하강으로 최적화될 것이기 때문에 requires_grad = True를 기본으로 가진다. 따라서 training_loss.backward()가 호출되면 옵티마이저에 전달된 grad가 그래프의 말단 노드들에 누적된다. 이제 훈련 루프를 다시한번 정의해보자.


```python
def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,t_c_train,t_c_val):
  for epoch in range(1,n_epochs+1):
    t_p_train = model(t_u_train) # 개별 파라미터 대신 모델 전달
    loss_train = loss_fn(t_p_train,t_c_train)

    t_p_val = model(t_u_val) # 개별 파라미터 대신 모델 전달
    loss_val = loss_fn(t_p_val,t_c_val)

    optimizer.zero_grad()
    loss_train.backward()
    optimizer.step()

    if epoch ==1 or epoch % 1000 ==0 :
      print(f'Epoch {epoch}, Training Loss {loss_train.item():.4f} , Val Loss {loss_val.item():.4f}')
```

torch.nn에서 손실 계산 또한 유용하게 할 수 있는데, nn 자체에 일반적인 손실 함수가 이미 들어있다. MSE는 nn.MSELoss로 구현할 수 있다. 직접 작성한 loss_fn을 바꿔서 훈련 루프를 돌려보자.


```python
linear_model = nn.Linear(1,1)
optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)

training_loop(n_epochs=3000,
              optimizer=optimizer,
              model = linear_model,
              loss_fn = nn.MSELoss(), # 직접 만든 손실 함수 사용 X
              t_u_train = t_un_train,
              t_u_val = t_un_val, 
              t_c_train = t_c_train,
              t_c_val = t_c_val)

print()
print(linear_model.weight)
print(linear_model.bias)
```

    Epoch 1, Training Loss 168.4553 , Val Loss 84.7645
    Epoch 1000, Training Loss 3.4812 , Val Loss 4.1016
    Epoch 2000, Training Loss 2.8648 , Val Loss 3.9766
    Epoch 3000, Training Loss 2.8541 , Val Loss 3.9681
    
    Parameter containing:
    tensor([[5.4236]], requires_grad=True)
    Parameter containing:
    tensor([-17.2466], requires_grad=True)


### 3. 심층 신경망 구현하기
마지막으로 위의 선형 모델 대신 신경망을 근사 함수로 사용하도록 바꿔보자. 온도 문제는 데이터 자체가 선형 함수를 따르기 때문에 신경망 모델을 사용한다고 결과가 더 좋아지지는 않을 것이다. 하지만 간단한 예제를 신경망으로 구현한다는 것에 의미를 두고 구현해보자.

위의 훈련 루프에서 모든 것은 그대로 사용하고 model만 다시 정의한다. 선형 모듈 뒤에 활성 함수를 달고 다른 선형 모듈에 연결하자. 첫번째 선형 + 활성층은 **은닉층 (hidden layer)**이라고 불린다. 계층의 출력을 직접 관찰하는 대신 출력층의 입력으로 넣어줬기 때문이다. 마지막 선형 계층은 활성 함수의 출력을 받아서 선형적으로 결합한 후 출력값을 만든다. 우리의 신경망을 간단하게 나타내면 다음 이미지와 같다.

![neuron5](https://user-images.githubusercontent.com/77332628/209757204-291872d8-7971-472b-84de-1d8726590e36.jpg)

출처(https://techvidvan.com/tutorials/artificial-neural-network/)

nn은 nn.Sequential 컨테이너를 통해서 모듈을 간단하게 결합시킨다.



```python
seq_model = nn.Sequential(nn.Linear(1,13), # 이 층의 출력과 
                          nn.Tanh(),
                          nn.Linear(13,1)) # 이 층의 입력의 크기가 같아야 한다.

seq_model
```




    Sequential(
      (0): Linear(in_features=1, out_features=13, bias=True)
      (1): Tanh()
      (2): Linear(in_features=13, out_features=1, bias=True)
    )



정의한 모델을 보면, 모델은 한개의 입력 피처로부터 13개의 은닉된 피처로 펼쳐지며 , 결과값을 tanh 활성 함수로 넘겨서 결과로 나온 13개의 숫자를 하나의 입력 피처로 만들기 위해 선형적으로 결합한다.

model.parameters()를 호출하면 첫번째와 두번째 선형 모듈에서 weight와 bias를 반환한다. 파라미터를 살펴보기 위해서 차원 정보를 출력하는 것이 좋다.


```python
[param.shape for param in seq_model.parameters()]
```




    [torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]



여러개의 서브 모듈로 만들어진 모델의 파라미터를 추적할 때, 파라미터를 이름으로 식별할 수 있게 하면 매우 편리하다. 이를 위해 named_parameters 메소드가 존재한다.


```python
for name, param in seq_model.named_parameters():
  print(name,param.shape)
```

    0.weight torch.Size([13, 1])
    0.bias torch.Size([13])
    2.weight torch.Size([1, 13])
    2.bias torch.Size([1])


Sequentail은 OrderedDict라는 인자도 받는데, 여기에 각 모듈의 이름을 정해서 전달해도 이름을 지정할 수 있다.


```python
from collections import OrderedDict

namedseq_model = nn.Sequential(OrderedDict([
    ('hidden_linear', nn.Linear(1, 12)),
    ('hidden_activation', nn.Tanh()),
    ('output_linear', nn.Linear(12 , 1))
]))

namedseq_model
```




    Sequential(
      (hidden_linear): Linear(in_features=1, out_features=12, bias=True)
      (hidden_activation): Tanh()
      (output_linear): Linear(in_features=12, out_features=1, bias=True)
    )



이러면 서브 모듈을 속성처럼 사용해서 특정 Parameter에 접근할 수도 있다.


```python
namedseq_model.output_linear.bias
```




    Parameter containing:
    tensor([-0.0247], requires_grad=True)



이런 방법을 사용해서 은닉층의 선형 영역에서 weight의 기울기를 출력하고 싶다면 새로운 신경망 모델을 위한 훈련 루프를 돌리고 마지막 에포크까지 끝난 후 만들어진 기울기를 보면 된다.


```python
optimizer = optim.SGD(namedseq_model.parameters(), lr=1e-3)
training_loop(n_epochs=5000,
              optimizer = optimizer,
              model = namedseq_model,
              loss_fn = nn.MSELoss(),
              t_u_train = t_un_train,
              t_u_val = t_un_val,
              t_c_train = t_c_train,
              t_c_val = t_c_val)

print('output',namedseq_model(t_un_val))
print('answer', t_c_val)
print('hidden', namedseq_model.hidden_linear.weight.grad)
```

    Epoch 1, Training Loss 1.5079 , Val Loss 3.9692
    Epoch 1000, Training Loss 1.4923 , Val Loss 4.0185
    Epoch 2000, Training Loss 1.4769 , Val Loss 4.0696
    Epoch 3000, Training Loss 1.4614 , Val Loss 4.1238
    Epoch 4000, Training Loss 1.4456 , Val Loss 4.1819
    Epoch 5000, Training Loss 1.4292 , Val Loss 4.2453
    output tensor([[15.8510],
            [ 7.3981]], grad_fn=<AddmmBackward0>)
    answer tensor([[13.],
            [ 8.]])
    hidden tensor([[ 0.0170],
            [ 0.0094],
            [-0.0018],
            [-0.0026],
            [ 0.0016],
            [ 0.0008],
            [-0.0119],
            [ 0.0108],
            [ 0.0012],
            [-0.0315],
            [-0.0059],
            [ 0.0116]])


#### 3.1 선형 모델과 비교하기
그래프에 O는 입력데이터, X는 출력데이터, 연속된 선은 샘플 사이의 모델의 행동을 출력해보면,



```python
from matplotlib import pyplot as plt

t_range = torch.arange(20.,90.).unsqueeze(1)

fig= plt.figure(dpi=200)
plt.xlabel('Fahrenheit')
plt.ylabel('Celcius')
plt.plot(t_u.numpy(),t_c.numpy(),'o')
plt.plot(t_range.numpy(),namedseq_model(0.1*t_range).detach().numpy(),'c-')
plt.plot(t_u.numpy(), namedseq_model(0.1*t_u).detach().numpy(),'kx')
```




    [<matplotlib.lines.Line2D at 0x7fdabf31ee50>]




    
![ddd](https://user-images.githubusercontent.com/77332628/209757410-84fcded6-77ec-46af-8dbc-e08f64f05a8e.png)
    


신경망이 노이즈까지 포함해서 측정값을 과도하게 따라가는 과적합을 보인다. 아마 굉장히 작은 신경망이더라도 측정값이 얼마 안되기 때문에 적합에 필요한 양보다 많은 파라미터를 가지고 있어서 그런것 같지만, 그래도 결과는 나쁘지 않다고 볼 수 있다. 이번 글에서는 저번글에서 다룬 온도 문제를 신경망으로 푸는 과정을 통해 파이토치로 신경망을 구현하는 법에 대해서 배웠다.


[<파이토치 딥러닝 마스터:모의암 진단 프로젝트로 배우는 신경망 모델 구축부터 훈련,튜닝,모델 서빙까지>(책만, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.]  도서보기: [https://www.gilbut.co.kr/book/view?bookcode=BN003496](https://www.onlybook.co.kr/entry/pytorch)


```python

```
