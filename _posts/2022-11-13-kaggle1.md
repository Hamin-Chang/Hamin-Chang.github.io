---
layout: single
title : '[IC/Kaggle] Intel image classification - 기본 모델로 문제 해결 🏞️' 
toc: true
toc_sticky: true
categories:
  - kaggle-imageclassification
---

## 1. Intel Image Classification

### 1.0 데이터 살펴보기
캐글에 있는 실제 데이터들을 다뤄보자. Intel Image Classification이라는 데이터를 다룰 것인데, [이 링크](https://www.kaggle.com/datasets/puneet6060/intel-image-classification)로 들어가면 데이터를 다운 받을 수 있다. 데이터에 대한 설명을 살펴보자. Intel Image Classification 데이터는 150x150 크기의 6개의 카테고리로 나누어진 2만 5천여개의 자연 경관에 대한 사진들을 포함하고 있다. 6가지 카테고리는 다음과 같다.
* 'buildings' -> 0
* 'forest' -> 1
* 'glacier' -> 2
* 'mountain' -> 3
* 'sea' -> 4
* 'street' -> 5

또한 훈련 데이터는 약 1만 4천개, 검증 데이터는 약 3천개, 테스트 데이터로는 약 7천개의 데이터가 각각 zip 파일에 들어있다고 설명하고 있다. 

### 1.1 기본 CNN 모델로 문제 풀기

#### 1.1.0 필요한 libraries import 하기
1.1에서는 Sequential을 이용한 기본적인 CNN 모델을 구축한 후, 사진 분류를 진행할 것이다. 기본 모델이기 때문에 성능이 조금 낮을 수 있겠지만, 기본적인 모델로 시작하는 것이 좋을 것 같다. 1.1의 코드는 [이 캐글러](https://www.kaggle.com/code/ahmadjaved097/multiclass-image-classification-using-cnn)의 코드를 참고했다.
먼저 필요한 library들을 import 한다.


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

import cv2
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.models import Sequential
from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
```

#### 1.1.1 데이터 로드하기
먼저 데이터의 파일 경로를 지정하고, 데이터 이미지들의 크기가 150x150이기 때문에 이미지 너비와 높이를 지정해주고, 배치 크기도 32로 지정해준다.


```python
train_dataset_path = '/kaggle/input/intel-image-classification/seg_train/seg_train/'
validation_dataset_path = '/kaggle/input/intel-image-classification/seg_test/seg_test/'

IMG_WIDTH = 150
IMG_HEIGHT = 150
BATCH_SIZE = 32
```

[**이전 글**](https://hamin-chang.github.io/small/)에서 다뤘듯이, 훈련의 과대적합을 최소화하기 위해 **이미지 증식**을 이용해서 훈련 데이터와 검증 데이터를 로드하겠다.


```python
train_datagen = ImageDataGenerator(rescale = 1.0/255,
                                  zoom_range = 0.2,
                                  width_shift_range = 0.2,
                                  height_shift_range = 0.2,
                                  fill_mode = 'nearest')

train_generator = train_datagen.flow_from_directory(train_dataset_path,
                                                   target_size= (IMG_HEIGHT,IMG_HEIGHT),
                                                   batch_size= BATCH_SIZE,
                                                   class_mode= 'categorical',
                                                   shuffle = True)

validation_datagen = ImageDataGenerator(rescale= 1.0/255)
validation_generator= validation_datagen.flow_from_directory(validation_dataset_path,
                                                           target_size=(IMG_WIDTH,IMG_HEIGHT),
                                                           batch_size= BATCH_SIZE,
                                                           class_mode = 'categorical',
                                                           shuffle = True)

```

    Found 14034 images belonging to 6 classes.
    Found 3000 images belonging to 6 classes.


위의 코드에서 훈련 데이터에는 총 6개의 클래스를 가진 14034개의 이미지가, 검증 데이터에는 총 6개의 클래스를 가진 3000개의 이미지가 있다는 것을 알 수 있다.

그 다음 어떤 클래스들이 있는지 class_indices를 이용해서 클래스의 이름들을 출력해보자.


```python
labels = {value : key for key, value in train_generator.class_indices.items()}
print("Label Mappings for classes present in the training and validation datasets\n")
for key , value in labels.items():
    print(f'{key} : {value}')
```

    Label Mappings for classes present in the training and validation datasets
    
    0 : buildings
    1 : forest
    2 : glacier
    3 : mountain
    4 : sea
    5 : street


#### 1.1.2 훈련 데이터 샘플 이미지 시각화하기
다음 코드로 훈련 데이터 중 몇가지 샘플 이미지를 시각화 해보자.


```python
fig, ax = plt.subplots(nrows=2,ncols=5,figsize=(15,12))
idx = 0

for i in range(2):
    for j in range(5):
        label = labels[np.argmax(train_generator[0][1][idx])]
        ax[i, j].set_title(f'{label}')
        ax[i, j].imshow(train_generator[0][0][idx][:,:,:])
        ax[i, j].axis('off')
        idx += 1
        
plt.tight_layout()
plt.suptitle('Sample Training Images', fontsize=21)
plt.show()
```


    
![kaggle1_1](https://user-images.githubusercontent.com/77332628/201599062-d4bf63df-a572-4730-a5e3-4dc0c20e64fe.png)
    


#### 1.1.3 CNN 모델 훈련하기
이제 훈련 데이터가 준비가 되었으니 간단한 CNN 모델을 구축한 후 훈련 이미지 데이터들로 훈련해보자.


```python
def creat_model():
    model = Sequential([
        Conv2D(filters=128, kernel_size=(5, 5), padding='valid',input_shape=(IMG_WIDTH,IMG_HEIGHT,3)),
        Activation('relu'),
        MaxPooling2D(pool_size=(2,2)),
        BatchNormalization(),
        
        Conv2D(filters=64, kernel_size=(3, 3), padding = 'valid', kernel_regularizer=l2(0.00005)),
        Activation('relu'),
        MaxPooling2D(pool_size=(2, 2)),
        BatchNormalization(),
        
        Conv2D(filters=32, kernel_size=(3, 3), padding='valid', kernel_regularizer=l2(0.00005)),
        Activation('relu'),
        MaxPooling2D(pool_size=(2, 2)),
        BatchNormalization(),
        
        Flatten(),
        
        Dense(units=256, activation='relu'),
        Dropout(0.5),
        Dense(units=6, activation='softmax')
    ])
    
    return model
```


```python
cnn_model = creat_model()
print(cnn_model.summary())
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 146, 146, 128)     9728      
    _________________________________________________________________
    activation (Activation)      (None, 146, 146, 128)     0         
    _________________________________________________________________
    max_pooling2d (MaxPooling2D) (None, 73, 73, 128)       0         
    _________________________________________________________________
    batch_normalization (BatchNo (None, 73, 73, 128)       512       
    _________________________________________________________________
    conv2d_1 (Conv2D)            (None, 71, 71, 64)        73792     
    _________________________________________________________________
    activation_1 (Activation)    (None, 71, 71, 64)        0         
    _________________________________________________________________
    max_pooling2d_1 (MaxPooling2 (None, 35, 35, 64)        0         
    _________________________________________________________________
    batch_normalization_1 (Batch (None, 35, 35, 64)        256       
    _________________________________________________________________
    conv2d_2 (Conv2D)            (None, 33, 33, 32)        18464     
    _________________________________________________________________
    activation_2 (Activation)    (None, 33, 33, 32)        0         
    _________________________________________________________________
    max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         
    _________________________________________________________________
    batch_normalization_2 (Batch (None, 16, 16, 32)        128       
    _________________________________________________________________
    flatten (Flatten)            (None, 8192)              0         
    _________________________________________________________________
    dense (Dense)                (None, 256)               2097408   
    _________________________________________________________________
    dropout (Dropout)            (None, 256)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 6)                 1542      
    =================================================================
    Total params: 2,201,830
    Trainable params: 2,201,382
    Non-trainable params: 448
    _________________________________________________________________
    None


ReduceLROnPlateau라는 콜백함수를 정의해준다. ReduceLROnPlateau 콜백함수는 지정한 지표가 최적의 방향으로 더 이상 가지 않을 때 learning_rate(학습률)을 낮추는 역할을 한다.


```python
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = np.sqrt(0.1), patience= 5)
```

옵티마이저는 Adam으로 정의해준 후, 모델을 컴파일 하고 훈련을 시작한다.


```python
optimizer = Adam(learning_rate=0.001)
cnn_model.compile(optimizer=optimizer,loss=CategoricalCrossentropy(),metrics=['accuracy'])

history = cnn_model.fit(train_generator, epochs=50, validation_data=validation_generator,
                       verbose = 2, callbacks=[reduce_lr])
```

    Epoch 1/50
    439/439 - 164s - loss: 1.9695 - accuracy: 0.5043 - val_loss: 0.9947 - val_accuracy: 0.6487
    Epoch 2/50
    439/439 - 97s - loss: 1.0539 - accuracy: 0.6252 - val_loss: 0.8171 - val_accuracy: 0.6927
    Epoch 3/50
    439/439 - 96s - loss: 0.9064 - accuracy: 0.6704 - val_loss: 1.0334 - val_accuracy: 0.6870
    Epoch 4/50
    439/439 - 96s - loss: 0.8291 - accuracy: 0.6971 - val_loss: 0.9408 - val_accuracy: 0.6500
    Epoch 5/50
    439/439 - 97s - loss: 0.7846 - accuracy: 0.7198 - val_loss: 0.7990 - val_accuracy: 0.6777
    Epoch 6/50
    439/439 - 95s - loss: 0.7495 - accuracy: 0.7316 - val_loss: 0.9228 - val_accuracy: 0.6257
    Epoch 7/50
    439/439 - 96s - loss: 0.7199 - accuracy: 0.7449 - val_loss: 1.3041 - val_accuracy: 0.6343
    Epoch 8/50
    439/439 - 96s - loss: 0.6787 - accuracy: 0.7597 - val_loss: 0.8333 - val_accuracy: 0.7040
    Epoch 9/50
    439/439 - 96s - loss: 0.6629 - accuracy: 0.7637 - val_loss: 0.9845 - val_accuracy: 0.6673
    Epoch 10/50
    439/439 - 96s - loss: 0.6458 - accuracy: 0.7752 - val_loss: 1.0875 - val_accuracy: 0.6523
    Epoch 11/50
    439/439 - 96s - loss: 0.5519 - accuracy: 0.8065 - val_loss: 0.5719 - val_accuracy: 0.8080
    Epoch 12/50
    439/439 - 98s - loss: 0.5079 - accuracy: 0.8264 - val_loss: 0.5859 - val_accuracy: 0.8073
    Epoch 13/50
    439/439 - 98s - loss: 0.4976 - accuracy: 0.8283 - val_loss: 0.5601 - val_accuracy: 0.8227
    Epoch 14/50
    439/439 - 98s - loss: 0.4858 - accuracy: 0.8348 - val_loss: 0.4887 - val_accuracy: 0.8400
    Epoch 15/50
    439/439 - 98s - loss: 0.4773 - accuracy: 0.8358 - val_loss: 0.4390 - val_accuracy: 0.8583
    Epoch 16/50
    439/439 - 98s - loss: 0.4643 - accuracy: 0.8360 - val_loss: 0.4203 - val_accuracy: 0.8653
    Epoch 17/50
    439/439 - 97s - loss: 0.4681 - accuracy: 0.8380 - val_loss: 0.4737 - val_accuracy: 0.8463
    Epoch 18/50
    439/439 - 97s - loss: 0.4585 - accuracy: 0.8445 - val_loss: 0.3976 - val_accuracy: 0.8750
    Epoch 19/50
    439/439 - 98s - loss: 0.4523 - accuracy: 0.8420 - val_loss: 0.4146 - val_accuracy: 0.8680
    Epoch 20/50
    439/439 - 97s - loss: 0.4456 - accuracy: 0.8477 - val_loss: 0.6212 - val_accuracy: 0.8157
    Epoch 21/50
    439/439 - 98s - loss: 0.4305 - accuracy: 0.8489 - val_loss: 0.4339 - val_accuracy: 0.8580
    Epoch 22/50
    439/439 - 97s - loss: 0.4262 - accuracy: 0.8536 - val_loss: 0.4181 - val_accuracy: 0.8643
    Epoch 23/50
    439/439 - 98s - loss: 0.4251 - accuracy: 0.8521 - val_loss: 0.4194 - val_accuracy: 0.8727
    Epoch 24/50
    439/439 - 98s - loss: 0.3876 - accuracy: 0.8677 - val_loss: 0.3593 - val_accuracy: 0.8897
    Epoch 25/50
    439/439 - 99s - loss: 0.3781 - accuracy: 0.8703 - val_loss: 0.5618 - val_accuracy: 0.8010
    Epoch 26/50
    439/439 - 98s - loss: 0.3765 - accuracy: 0.8711 - val_loss: 0.3758 - val_accuracy: 0.8817
    Epoch 27/50
    439/439 - 97s - loss: 0.3706 - accuracy: 0.8701 - val_loss: 0.3647 - val_accuracy: 0.8830
    Epoch 28/50
    439/439 - 98s - loss: 0.3682 - accuracy: 0.8749 - val_loss: 0.4075 - val_accuracy: 0.8707
    Epoch 29/50
    439/439 - 98s - loss: 0.3747 - accuracy: 0.8690 - val_loss: 0.3665 - val_accuracy: 0.8827
    Epoch 30/50
    439/439 - 98s - loss: 0.3598 - accuracy: 0.8752 - val_loss: 0.3667 - val_accuracy: 0.8767
    Epoch 31/50
    439/439 - 97s - loss: 0.3518 - accuracy: 0.8798 - val_loss: 0.3539 - val_accuracy: 0.8830
    Epoch 32/50
    439/439 - 96s - loss: 0.3456 - accuracy: 0.8807 - val_loss: 0.3468 - val_accuracy: 0.8867
    Epoch 33/50
    439/439 - 96s - loss: 0.3607 - accuracy: 0.8756 - val_loss: 0.3449 - val_accuracy: 0.8890
    Epoch 34/50
    439/439 - 96s - loss: 0.3436 - accuracy: 0.8821 - val_loss: 0.3526 - val_accuracy: 0.8847
    Epoch 35/50
    439/439 - 97s - loss: 0.3429 - accuracy: 0.8804 - val_loss: 0.3536 - val_accuracy: 0.8870
    Epoch 36/50
    439/439 - 96s - loss: 0.3451 - accuracy: 0.8789 - val_loss: 0.3575 - val_accuracy: 0.8863
    Epoch 37/50
    439/439 - 97s - loss: 0.3422 - accuracy: 0.8811 - val_loss: 0.3483 - val_accuracy: 0.8897
    Epoch 38/50
    439/439 - 97s - loss: 0.3455 - accuracy: 0.8809 - val_loss: 0.3429 - val_accuracy: 0.8937
    Epoch 39/50
    439/439 - 96s - loss: 0.3404 - accuracy: 0.8823 - val_loss: 0.3679 - val_accuracy: 0.8833
    Epoch 40/50
    439/439 - 96s - loss: 0.3396 - accuracy: 0.8838 - val_loss: 0.3618 - val_accuracy: 0.8860
    Epoch 41/50
    439/439 - 97s - loss: 0.3411 - accuracy: 0.8835 - val_loss: 0.3478 - val_accuracy: 0.8877
    Epoch 42/50
    439/439 - 96s - loss: 0.3420 - accuracy: 0.8816 - val_loss: 0.3507 - val_accuracy: 0.8900
    Epoch 43/50
    439/439 - 97s - loss: 0.3418 - accuracy: 0.8781 - val_loss: 0.3511 - val_accuracy: 0.8890
    Epoch 44/50
    439/439 - 97s - loss: 0.3360 - accuracy: 0.8834 - val_loss: 0.3472 - val_accuracy: 0.8870
    Epoch 45/50
    439/439 - 97s - loss: 0.3277 - accuracy: 0.8868 - val_loss: 0.3477 - val_accuracy: 0.8903
    Epoch 46/50
    439/439 - 96s - loss: 0.3324 - accuracy: 0.8865 - val_loss: 0.3458 - val_accuracy: 0.8897
    Epoch 47/50
    439/439 - 97s - loss: 0.3317 - accuracy: 0.8862 - val_loss: 0.3466 - val_accuracy: 0.8897
    Epoch 48/50
    439/439 - 96s - loss: 0.3342 - accuracy: 0.8836 - val_loss: 0.3515 - val_accuracy: 0.8883
    Epoch 49/50
    439/439 - 97s - loss: 0.3379 - accuracy: 0.8847 - val_loss: 0.3472 - val_accuracy: 0.8887
    Epoch 50/50
    439/439 - 96s - loss: 0.3312 - accuracy: 0.8830 - val_loss: 0.3514 - val_accuracy: 0.8887


#### 1.1.4 훈련 결과 시각화하기
훈련 데이터로 모델을 훈련한 결과들 즉 train_accuracy, val_accuracy, train_loss, val_loss 등을 그래프로 시각화하는 과정이다.


```python
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

train_loss = history.history['loss']
val_loss = history.history['val_loss']

learning_rate = history.history['lr']

fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12,10))

ax[0].set_title('Training Accuracy vs. Epochs')
ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')
ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].legend(loc='best')

ax[1].set_title('Training/Validation Loss vs. Epochs')
ax[1].plot(train_loss, 'o-', label='Train Loss')
ax[1].plot(val_loss, 'o-', label='Validation Loss')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].legend(loc='best')

ax[2].set_title('Learning Rate vs. Epochs')
ax[2].plot(learning_rate, 'o-', label='Learning Rate')
ax[2].set_xlabel('Epochs')
ax[2].set_ylabel('Loss')
ax[2].legend(loc='best')

plt.tight_layout()
plt.show()
```


![kaggle1_2](https://user-images.githubusercontent.com/77332628/201599068-9e0ea075-af87-4fba-ae24-636429f60614.png)


#### 1.1.5 테스트 데이터세트에서 성능 평가하기
마지막으로 훈련한 모델을 테스트 데이터 세트에서 평가한다. 먼저 훈련 데이터와 검증 데이터와 마찬가지로 테스트 데이터 세트를 데이터 증식 기법인 ImageDataGenerator로 생성한다.


```python
test_dataset = '/kaggle/input/intel-image-classification/seg_test/seg_test/'

test_datagen = ImageDataGenerator(rescale=1.0/255)
test_generator = test_datagen.flow_from_directory(test_dataset,
                                                 shuffle=False,
                                                 batch_size=BATCH_SIZE,
                                                 target_size=(IMG_WIDTH,IMG_HEIGHT),
                                                 class_mode='categorical')
```

    Found 3000 images belonging to 6 classes.


위의 코드로 테스트 데이터 세트가 준비되었으니 6개의 클래스와 매칭이 되어있지 않은 테스트 데이터의 이미지들이 6개의 클래스 중 어디에 해당하는지 매칭을 해보도록 한다.


```python
predictions = cnn_model.predict(test_generator)

fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(12,10))
idx = 0

for i in range(2):
    for j in range(5):
        predicted_label = labels[np.argmax(predictions[idx])]
        ax[i, j].set_title(f'{predicted_label}')
        ax[i, j].imshow(test_generator[0][0][idx])
        ax[i, j].axis('off')
        idx += 1
        
plt.tight_layout()
plt.suptitle('Test Dataset Predictions',fontsize=20)
plt.show()
```


    
![kaggle1_3](https://user-images.githubusercontent.com/77332628/201599071-098accab-0a4e-44a4-8300-181796ab0e1a.png)
    


테스트 데이터 세트에 대한 예측을 진행한 후, test 손실과 test 정확도를 구한다.


```python
test_loss, test_accuracy = cnn_model.evaluate(test_generator, batch_size=BATCH_SIZE)
print(f'테스트 손실 : {test_loss}')
print(f'테스트 정확도 : {test_accuracy}')
```

    94/94 [==============================] - 5s 57ms/step - loss: 0.3514 - accuracy: 0.8887
    테스트 손실 : 0.3514269292354584
    테스트 정확도 : 0.8886666893959045


#### 1.1.6 Confusion Matrix 사용해서 정확도 시각화하기
Confusion Maxtrix를 이용해서 각 클래스에 대한 예측이 어떻게 이루어졌는지 살펴보자.


```python
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

cf_mtx = confusion_matrix(y_true, y_pred)

group_counts = ["{0:0.0f}".format(value) for value in cf_mtx.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cf_mtx.flatten()/np.sum(cf_mtx)]
box_labels = [f"{v1}\n({v2})" for v1, v2 in zip(group_counts, group_percentages)]
box_labels = np.asarray(box_labels).reshape(6, 6)

plt.figure(figsize = (12, 10))
sns.heatmap(cf_mtx, xticklabels=labels.values(), yticklabels=labels.values(),
           cmap="YlGnBu", fmt="", annot=box_labels)
plt.xlabel('Predicted Classes')
plt.ylabel('True Classes')
plt.show()

print(classification_report(y_true, y_pred, target_names=labels.values()))
```


    
![kaggle1_4](https://user-images.githubusercontent.com/77332628/201599074-60a49b0e-bf7f-4052-bf2b-0ea5657b6bd6.png)
    


                  precision    recall  f1-score   support
    
       buildings       0.83      0.90      0.86       437
          forest       0.96      0.98      0.97       474
         glacier       0.85      0.84      0.84       553
        mountain       0.87      0.81      0.84       525
             sea       0.91      0.93      0.92       510
          street       0.92      0.89      0.90       501
    
        accuracy                           0.89      3000
       macro avg       0.89      0.89      0.89      3000
    weighted avg       0.89      0.89      0.89      3000
    


#### 1.1.7 잘못된 예측값 살펴보기
이제 각 클래스마다 얼마나 정확히 예측했고 얼마나 잘못 예측했는지 알아봤으니, 모델이 어느 부분에서 잘못된 예측을 했는지 알아보고, 잘못 예측한 이미지의 정답은 무엇인지 알아보자.


```python
errors = (y_true - y_pred != 0)
y_true_errors = y_true[errors]
y_pred_errors = y_pred[errors]

test_images = test_generator.filenames
test_img = np.asarray(test_images)[errors]

fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(12, 10))
idx = 0

for i in range(2):
    for j in range(5):
        idx = np.random.randint(0, len(test_img))
        true_index = y_true_errors[idx]
        true_label = labels[true_index]
        predicted_index = y_pred_errors[idx]
        predicted_label = labels[predicted_index]
        ax[i, j].set_title(f"True Label: {true_label} \n Predicted Label: {predicted_label}")
        img_path = os.path.join(test_dataset, test_img[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        ax[i, j].imshow(img)
        ax[i, j].axis("off")

plt.tight_layout()
plt.suptitle('Wrong Predictions made on test set', fontsize=20)
plt.show()
```


    

![kaggle1_5](https://user-images.githubusercontent.com/77332628/201599077-8e75cf76-76b2-441c-9f84-91c877ff2abc.png)
    

