---
title : '[CV/Pytorch] íŒŒì´í† ì¹˜ë¡œ SSD300 êµ¬í˜„í•˜ê¸° ğŸ”«'
layout: single
toc: true
toc_sticky: true
categories:
  - pytorchCV
---

## Pytorchë¡œ SSD(Single Shot Detector) êµ¬í˜„í•˜ê¸°

ì´ë²ˆ ê¸€ì—ì„œëŠ” SSD ëª¨ë¸ì„ ì§ì ‘ pytorchë¡œ êµ¬í˜„í•´ë³´ë„ë¡ í•˜ê² ë‹¤. SSD ëª¨ë¸ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ì´ì „ ê¸€([**ë§í¬**](https://hamin-chang.github.io/cv-objectdetection/ssd/))ë¥¼ ì°¸ê³ í•˜ë©´ ëœë‹¤. ëª¨ë¸ì˜ ì…ë ¥ê°’ì„ í¬ê¸°ê°€ 300x300ì¸ RGB ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” SSD300ì„ êµ¬í˜„í•´ë³¼ ê²ƒì´ë‹¤.

### 1. Base Conv Network

SSD300ì€ VGG16 networkë¥¼ base conv networkë¡œ ì‚¬ìš©í•œë‹¤. ì—¬ê¸°ì„œëŠ” torchvisionì—ì„œ ì œê³µí•˜ëŠ” vgg16(pretrained=True) ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ì´ì „ ê¸€([**ë§í¬**](https://hamin-chang.github.io/cv-objectdetection/ssd/))ì—ì„œ ì„¤ëª…í–ˆë“¯ì´, SSD300ì—ì„œëŠ” VGG16ì˜ fc6,fc7 layerë¥¼ conv6, conv7ë¡œ ìˆ˜ì •í•œ ëª¨ë¸ VGGBase ëª¨ë“ˆì„ ì‚¬ìš©í•œë‹¤.


![ìŠ¤í¬ë¦°ìƒ·, 2023-01-31 18-54-24](https://user-images.githubusercontent.com/77332628/215727269-518e78ec-4c11-480f-8644-fbe0d8c845f4.png)

[ì¶œì²˜ ë§í¬](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)

ë˜í•œ ìˆ˜ì •í•œ conv6ì—ì„œ ì—°ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ artous convolutionì„ ì ìš©í•˜ëŠ”ë°, ì´ë•Œ dilation ê°’ì„ 6ìœ¼ë¡œ ì§€ì •í•œë‹¤. artous convolutionì€ ë‹¤ìŒ ì´ë¯¸ì§€ì²˜ëŸ¼ ì‘ë™í•˜ëŠ” conv networkì´ë‹¤.

![2](https://user-images.githubusercontent.com/77332628/215726739-06534b3d-3793-4919-af10-dc29d0105762.gif)

[ì¶œì²˜ ë§í¬](https://better-tomorrow.tistory.com/entry/Atrous-Convolution)

ìˆ˜ì •ëœ VGGBase ëª¨ë“ˆì€ self.load_pretrained_layers()ì— ì˜í•´ torchvisionì—ì„œ ì œê³µí•˜ëŠ” ImageNetì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ weight ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ëœë‹¤. ì°¸ê³ ë¡œ, 2ê°œì˜ fc layerë¥¼ convë¡œ ëŒ€ì²´í–ˆê¸° ë•Œë¬¸ì— ì¼ë¶€ ì—°ì‚° shapeì´ ë°”ë€Œì§€ë§Œ parameter ìˆ˜ëŠ” ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ì—°ì‚° shape ë³€í™˜ì„ í†µí•´ weight ì´ˆê¸°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.




```python
import torch.nn as nn
import torch.nn.functional as F
from utils import *
import torchvision

class VGGBase(nn.Module):
  def __init__(self):
    super(VGGBase, self).__init__()

    # VGG16 ëª¨ë¸ì˜ ê¸°ì¡´ conv layers
    self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    self.conv1_2 = nn.Conv2d(64,64, kernel_size=3, padding=1)
    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
    self.conv2_2 = nn.Conv2d(128,128, kernel_size=3, padding=1)
    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True) # ceiling(ì†Œìˆ˜ì  ì˜¬ë¦¼)

    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)

    # fc6, fc7 layerë¥¼ conv layerë¡œ ìˆ˜ì •
    self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # artous conv ì ìš©
    self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)

    # ì‚¬ì „ í›ˆë ¨ëœ layers ë¡œë“œ
    self.load_pretrained_layers()
    
  def forward(self, image):
    # image ë³€ìˆ˜ : ì…ë ¥ images, (N, 3, 300, 300) shapeì˜ tensor

    out = F.relu(self.conv1_1(image)) # (N, 64, 300, 300)
    out = F.relu(self.conv1_2(out)) # (N, 64, 300, 300)
    out = self.pool1(out) # (N, 64, 150, 150)

    out = F.relu(self.conv2_1(out)) # (N, 128, 150, 150)
    out = F.relu(self.conv2_2(out)) # (N, 128, 150, 150)
    out = self.pool2(out) # (N, 128, 75, 75)

    out = F.relu(self.conv3_1(out)) # (N, 256, 75, 75)
    out = F.relu(self.conv3_2(out)) # (N, 256, 75, 75)
    out = F.relu(self.conv3_3(out)) # (N, 256, 75, 75)
    out = self.pool3(out) # (N, 256, 38, 38) # ceiling modeì´ê¸° ë•Œë¬¸ì— 38

    out = F.relu(self.conv4_1(out)) # (N, 512, 38, 38)
    out = F.relu(self.conv4_2(out)) # (N, 512, 38, 38)
    out = F.relu(self.conv4_3(out)) # (N, 512, 38, 38)
    conv4_3_feats = out # (N, 512, 38, 38)
    out = self.pool4(out) # (N, 512, 19, 19)

    out = F.relu(self.conv5_1(out)) # (N, 512, 19, 19)
    out = F.relu(self.conv5_2(out)) # (N, 512, 19, 19)
    out = F.relu(self.conv5_3(out)) # (N, 512, 19, 19)
    out = self.pool5(out) # (N, 512, 19, 19) # pool5ëŠ” stride=1 ë•Œë¬¸ì— dimension reduce X

    out = F.relu(self.conv6(out)) # (N, 1024, 19, 19)
    conv7_feats = F.relu(self.conv7(out)) # (N, 1024, 19, 19)

    # 6ê°œì˜ multi-scale feature maps ì¤‘ 2ê°œì˜ feature map ì¶”ì¶œ
    return conv4_3_feats, conv7_feats

  def load_pretrained_layers(self):
    # ImageNetì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ VGG16 ëª¨ë¸ì„ ë¡œë“œí• ê±´ë°,
    # ìš°ë¦¬ê°€ ì‚¬ìš©í•  VGG16ì€ fc6, fc7 ëŒ€ì‹  convê°€ ë“¤ì–´ê°”ìœ¼ë‹ˆ ì´ ë¶€ë¶„ì€ utilsì˜ decimateë¡œ ì²˜ë¦¬

    # í˜„ì¬ baseì˜ ìƒíƒœ
    state_dict = self.state_dict()
    param_names = list(state_dict.keys())

    # Pre-trained VGG base
    pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()
    pretrained_param_names = list(pretrained_state_dict.keys())

    # ì‚¬ì „ í›ˆë ¨ëœ paramsë¥¼ í˜„ì¬ ëª¨ë¸ì— ì „ë‹¬
    for i, param in enumerate(param_names[:-4]): # conv6, conv7 ì¼ë‹¨ ì œì™¸
      state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]

    # fc6, fcyë¥¼ conv layerë¡œ ë³€í™˜ (decimation(=down sampling)ì„ ì´ìš©í•´ì„œ)
    # fc6 -> conv6
    conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7) 
    conv_fc6_bias = pretrained_state_dict['classifier.0.bias'] # (4096,)
    state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3]) # (1024, 512, 3, 3)
    state_dict['convt.bias'] = decimate(conv_fc6_bias, m=[4]) # (1024,)

    self.load_state_dict(state_dict)

    print('\nLoaded base model.\n')

```

    /usr/lib/python3.8/utils.py:546: SyntaxWarning: "is" with a literal. Did you mean "=="?
      if d.__name__ is 'adjust_hue':


### 2. Auxiliary Convolution Network

SSD ëª¨ë¸ì€ Base VGG network ë§ˆì§€ë§‰ layerì—ì„œ ì¶”ì¶œí•œ 2ê°œì˜ feature map ì´ì™¸ì— Auxiliary Networkì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ 4ê°œì˜ feature mapì„ ì¶”ì¶œí•´ì„œ ì‚¬ìš©í•œë‹¤. Auxiliary networkì—ì„œ ê° layerì—ì„œëŠ” strideë¥¼ í†µí•´ì„œ poolingì„ ì§„í–‰í•œë‹¤. íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”ë¡œëŠ” ì—¬ëŸ¬ ë°©ë²• ì¤‘ Xavierë¥¼ ì‚¬ìš©í•œë‹¤. (Xavierì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì•„ì§ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ìƒëµí•œë‹¤.)

![3](https://user-images.githubusercontent.com/77332628/215726748-13d4bd9f-0569-40b8-ad2e-ecc5bec71495.jpg)

[ì¶œì²˜ ë§í¬](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)




```python
class AuxiliaryConvolutions(nn.Module):

  def __init__(self):
    super(AuxiliaryConvolutions, self).__init__()

    self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)
    self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1) # stride í†µí•´ downsampling

    self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)
    self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # stride í†µí•´ downsampling

    self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)
    self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # padding=0 í†µí•´ downsampling

    self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)
    self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # padding=0 í†µí•´ downsampling

    # convolution parameter ì´ˆê¸°í™”
    self.init_conv2d()

  def init_conv2d(self):
    # convolution parameter ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
    for c in self.children():
      if isinstance(c, nn.Conv2d):
        nn.init.xavier_uniform_(c.weight)
        nn.init.constant_(c.bias, 0.)
  
  def forward(self, conv7_feats):
    '''Base VGG ë§ˆì§€ë§‰ outputì¸ conv7_featsë¥¼ ì…ë ¥ìœ¼ë¡œ ìˆœì „íŒŒí•´ì„œ 
    ì¶”ê°€ 4ê°œì˜ feature maps ì¶”ì¶œ'''
    out = F.relu(self.conv8_1(conv7_feats)) # (N, 256, 19, 19)
    out = F.relu(self.conv8_2(out)) # (N, 512, 10, 10)
    conv8_2_feats = out # 3ë²ˆì§¸ feature map ì¶”ì¶œ

    out = F.relu(self.conv9_1(out)) # (N, 128, 10, 10)
    out = F.relu(self.conv9_2(out)) # (N, 256, 5, 5)
    conv9_2_feats = out # 4ë²ˆì§¸ feature map ì¶”ì¶œ

    out = F.relu(self.conv10_1(out)) # (N, 128, 5, 5)
    out = F.relu(self.conv10_2(out)) # (N, 256, 3, 3)
    conv10_2_feats = out # 5ë²ˆì§¸ feature map ì¶”ì¶œ

    out = F.relu(self.conv11_1(out)) # (N, 128, 3, 3)
    out = F.relu(self.conv11_2(out)) # (N, 256, 1, 1)
    conv11_2_feats = out # 6ë²ˆì§¸ feature map ì¶”ì¶œ

    return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats
```

### 3. Prediction Convolution Network

ì´ì œ ë³´ì¡° Conv networkì—ì„œ ì¶”ì¶œëœ 6ê°œì˜ multi-scale feature mapì„ í™œìš©í•´ì„œ bounding boxì˜ offsetì„ êµ¬í•˜ëŠ” **Localization task**ì™€ íƒì§€ëœ ê°ì²´ì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” **Classification task**ë¥¼ ìˆ˜í–‰í•˜ëŠ” Prediction Convolution Networkë¥¼ êµ¬í˜„í•´ë³´ì. Localizationì€ 4ê°œì˜ offsetì„ ì¶œë ¥í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— $(g^c_x,g^c_y,g_w,g_h)$ prior boxì— 4ë¥¼ ê³±í•´ì¤€ outputì„ ë°˜í™˜í•˜ê³ , Classificationì€ ê° í´ë˜ìŠ¤ë³„ scoreë¥¼ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ì „ì²´ ë¼ë²¨ì˜ ê°¯ìˆ˜ë¥¼ ê³±í•´ì£¼ë©´ ëœë‹¤.




```python
class PredictionConvolutions(nn.Module):
  '''
  ë¨¼ì € localizationì„ ìˆ˜í–‰í•  ë•Œ, bounding boxë“¤ì€ ì´ 8732ê°œì˜ offset ê°’ìœ¼ë¡œ ì˜ˆì¸¡ëœë‹¤.
  bounding boxì˜ offsetì˜ encodingì€ utilsì˜ cxcy_to_gcxgcyë¡œ ìˆ˜í–‰í•œë‹¤.

  ê·¸ë¦¬ê³  class scoreì€ ê°ê°ì˜ 8732ê°œì˜ bounding boxë“¤ì˜ scoreë¥¼ í‘œí˜„í•œë‹¤.
  '''
  def __init__(self, n_classes):
    super(PredictionConvolutions, self).__init__()
    self.n_classes = n_classes

    # feature mapë§ˆë‹¤ ë‹¤ë¥¸ ê°œìˆ˜ì˜ (ë¹„ìœ¨ì´ ë‹¤ë¥¸)prior-box ì ìš©
    n_boxes = {'conv4_3'  : 4,
               'conv7'    : 6,
               'conv8_2'  : 6,
               'conv9_2'  : 6,
               'conv10_2' : 4,
               'conv11_2' : 4}

    # Localization prediction convolution (ê° prior boxì˜ offset ì˜ˆì¸¡)
    self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)
    self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv_7'] * 4, kernel_size=3, padding=1)
    self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)
    self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)
    self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)
    self.loc_conv11_2 = nn.Conv2d(512, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)
    
    # Class prediction convolution (localization boxì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡)
    self.c1_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv7 = nn.Conv2d(512, n_boxes['conv_7'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv9_2 = nn.Conv2d(512, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv10_2 = nn.Conv2d(512, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)
    self.c1_conv11_2 = nn.Conv2d(512, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)

    # convolution param ì´ˆê¸°í™”
    self.init_conv2d()

  def init_conv2d(self):
    # convolution param ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
    for c in self.children():
      if isinstance(c, nn.Conv2d):
        nn.init.xavier_uniform_(c.weight)
        nn.init.constant_(c.bias, 0.)
  
  def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):
    # 6ê°œì˜ multiscale feature mapë¥¼ ìˆœì „íŒŒí•´ì„œ 8732ê°œì˜ locationê³¼ class score ì¶œë ¥
    batch_size = conv4_3_feats.size(0)

    # Localization bounding boxes (as offsets of prior boxes)
    l_conv4_3 = self.loc_conv4_3(conv4_3_feats) # (N, 16, 38, 38)
    l_conv4_3 = l_conv4_3.permute(0,2,3,1).contiguous() # (N, 38, 38, 16) 
    l_conv4_3 = l_conv4_3.view(batch_size, -1, 4) # (N, 5776, 4) ì´ 5776ê°œì˜ boxes ì¡´ì¬

    l_conv7 = self.loc_conv7(conv7_feats) # (N, 24, 19, 19)
    l_conv7 = l_conv7.permute(0,2,3,1).contiguous() # (N, 19, 19, 24) 
    l_conv7 = l_conv7.view(batch_size, -1, 4) # (N, 2166, 4) ì´ 2166ê°œì˜ boxes ì¡´ì¬
    
    l_conv8_2 = self.loc_conv8_2(conv8_2_feats) # (N, 24, 10, 10)
    l_conv8_2 = l_conv8_2.permute(0,2,3,1).contiguous() # (N, 10, 10, 24) 
    l_conv8_2 = l_conv8_2.view(batch_size, -1, 4) # (N, 600, 4) ì´ 600ê°œì˜ boxes ì¡´ì¬

    l_conv9_2 = self.loc_conv9_2(conv9_2_feats) # (N, 24, 5, 5)
    l_conv9_2 = l_conv9_2.permute(0,2,3,1).contiguous() # (N, 5, 5, 24) 
    l_conv9_2 = l_conv9_2.view(batch_size, -1, 4) # (N, 150, 4) ì´ 150ê°œì˜ boxes ì¡´ì¬

    l_conv10_2 = self.loc_conv10_2(conv10_2_feats) # (N, 16, 3, 3)
    l_conv10_2 = l_conv10_2.permute(0,2,3,1).contiguous() # (N, 3, 3, 16) 
    l_conv10_2 = l_conv10_2.view(batch_size, -1, 4) # (N, 36, 4) ì´ 36ê°œì˜ boxes ì¡´ì¬

    l_conv11_2 = self.loc_conv11_2(conv11_2_feats) # (N, 16, 1, 1)
    l_conv11_2 = l_conv11_2.permute(0,2,3,1).contiguous() # (N, 1, 1, 16) 
    l_conv11_2 = l_conv11_2.view(batch_size, -1, 4) # (N, 4, 4) ì´ 4ê°œì˜ boxes ì¡´ì¬

    # localization boxesì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡
    c_conv4_3 = self.cl_conv4_3(conv4_3_feats) # (N, 4 * n_classes, 38, 38)
    c_conv4_3 = c_conv4_3.permute(0,2,3,1).contiguous() # (N, 38, 38, 4 * n_classes)
    c_conv4_3 = c_conv4_3.view(batch_size, -1, self.n_classes) # (N, 5776, n_classes)

    c_conv7 = self.cl_conv7(conv7_feats) # (N, 6 * n_classes, 19, 19)
    c_conv7 = c_conv7.permute(0,2,3,1).contiguous() # (N, 19, 19, 6 * n_classes)
    c_conv7 = c_conv7.view(batch_size, -1, self.n_classes) # (N, 2166, n_classes)

    c_conv8_2 = self.cl_conv8_2(conv8_2_feats) # (N, 6 * n_classes, 10, 10)
    c_conv8_2 = c_conv8_2.permute(0,2,3,1).contiguous() # (N, 10, 10, 6 * n_classes)
    c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes) # (N, 600, n_classes)

    c_conv9_2 = self.cl_conv9_2(conv9_2_feats) # (N, 6 * n_classes, 5, 5)
    c_conv9_2 = c_conv9_2.permute(0,2,3,1).contiguous() # (N, 5, 5, 6 * n_classes)
    c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes) # (N, 150, n_classes)

    c_conv10_2 = self.cl_conv10_2(conv10_2_feats) # (N, 4 * n_classes, 3, 3)
    c_conv10_2 = c_conv10_2.permute(0,2,3,1).contiguous() # (N, 3, 3, 4 * n_classes)
    c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes) # (N, 36, n_classes)

    c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)
    c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)
    c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)

    # ìœ„ì˜ ì¶œë ¥ê°’ë“¤ì„ prior boxesì˜ ìˆœì„œì— ë§ê²Œ í•©ì³ì¤€ë‹¤.
    locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1) # (N, 8732, 4)
    classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1) # (N, 8732, n_classes)

    return locs, classes_scores
    
```

ìœ„ì˜ PredictionConvolutionì—ì„œ forward ë¶€ë¶„ì—ì„œ ì´ë¤„ì§€ëŠ” contiguous() í•¨ìˆ˜ì— ëŒ€í•´ì„œ ì„¤ëª…ì„ ì¶”ê°€í•˜ìë©´, view í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ í…ì„œì˜ shapeë¥¼ ë³€ê²½í•´ì¤˜ì•¼ í•˜ëŠ”ë°, Pytorchì—ì„œëŠ” í•­ìƒ contiguousí•œ ë©”ëª¨ë¦¬ ì—°ì‚°ì„ ë³´ì¥í•˜ì§€ ì•Šê¸° ëŒ€ë¬¸ì— ì•„ë˜ ì´ë¯¸ì§€ì²˜ëŸ¼ ì—°ì†ì ì¸ ê°’ë“¤ì´ í•˜ë‚˜ì˜ gridë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì†ì„±ì„ í‘œí˜„í•œë‹¤ëŠ” ë³´ì¥ì´ ì—†ë‹¤. ì´ë•Œ contiguous() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê²Œ ë˜ë©´ ë³€í˜•ëœ í…ì„œê°€ ì›ë³¸ Elementì˜ ìˆœì„œì™€ ê°™ì€ ëª¨ì–‘ì˜ í…ì„œê°€ ë§Œë“¤ì–´ì§„ë‹¤.

![4](https://user-images.githubusercontent.com/77332628/215726770-f1508770-1cbd-4e11-84a2-dc0f0f6170b2.jpg)

[ì¶œì²˜ ë§í¬](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)

### 4. Training SSD300
ì´ì œ Base, Auxiliary, Prediction Networkë¥¼ ì´ìš©í•´ì„œ SSD300 ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œë³´ì. Base -> Auxiliary -> Prediction ìˆœì„œë¡œ í›ˆë ¨ì„ ì‹œí‚¤ëŠ” ê²ƒì„ êµ¬í˜„í•´ ë³¼ ê²ƒì´ë‹¤. ì½”ë“œ ì¤‘ì—ì„œ ì£¼ëª©í•  ë¶€ë¶„ì€, self.baseì—ì„œ ë‚˜ì˜¨ conv4_3_featsì˜ outputì˜ feature valueëŠ” Auxiliary Networkì—ì„œ ì¶”ì¶œí•œ 4ê°œì˜ feature mapì— ë¹„í•´ scale ì ì¸ ì¸¡ë©´ì—ì„œ ì°¨ì´ê°’ì´ í¬ê¸° ë•Œë¬¸ì—, Prediction Networkë¡œ ì…ë ¥ ì „ ì´ë¥¼ L2-normalizationì„ ì ìš©í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ (0,1) ì‚¬ì´ë¡œ scaling í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë¹„ì •ìƒì ì¸ feature value ì°¨ì´ë¡œ ì¸í•œ loss explodingì„ ë°©ì§€í•œë‹¤.



```python
from math import sqrt
class TrainSSD300(nn.Module):

  def __init__(self, n_classes):
    super(TrainSSD300, self).__init__()
    self.n_classes = n_classes

    self.base = VGGBase()
    self.aux_convs = AuxiliaryConvolutions()
    self.pred_convs = PredictionConvolutions(n_classes)

    # conv4_3_featsê°€ ìƒëŒ€ì ìœ¼ë¡œ í° scaleì„ ê°€ì§€ê³  ìˆê¸° ë–„ë¬¸ì— L2 normê³¼ rescale ì ìš©
    self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))
    nn.init.constant_(self.rescale_factors, 20)

    # Prior boxse
    self.priors_cxcy = self.create_prior_boxes()

  def forward(self, image):
    # (N, 3, 300, 300) í¬ê¸°ì˜ image tensorë¥¼ ìˆœì „íŒŒí•´ì„œ 8732ê°œì˜ locationê³¼ class score ì¶œë ¥

    conv4_3_feats, conv7_feats = self.base(image) # (N,512,38,38), (N,1024,19,19)

    # L2 ì •ê·œí™”, rescaling conv4_3_featsì— ì ìš©
    norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt() # (N, 1, 38, 38)
    conv4_3_feats = conv4_3_feats / norm # (N, 512, 38, 38)
    conv4_3_feats = conv4_3_feats * self.rescale_factors 

    conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \
    self.aux_convs(conv7_feats) # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)

    locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)
    # (N, 8732, 4) , (N, 8732, n_classes)

    return locs, classes_scores
  
  def create_prior_boxes(self):
    # ì´ 8732ê°œì˜ prior boxë¥¼ ìƒì„±í•œë‹¤. (center-size coordinates í˜•íƒœë¡œ)
    fmap_dims = {'conv4_3': 38,
                 'conv7': 19,
                 'conv8_2': 10,
                 'conv9_2': 5,
                 'conv10_2': 3,
                 'conv11_2': 1}
    
    obj_scales = {'conv4_3': 0.1,
                  'conv7': 0.2,
                  'conv8_2': 0.375,
                  'conv9_2': 0.55,
                  'conv10_2': 0.725,
                  'conv11_2': 0.9}

    aspect_ratios = {'conv4_3': [1., 2., 0.5],
                     'conv7': [1., 2., 3., 0.5, .333],
                     'conv8_2': [1., 2., 3., 0.5, .333],
                     'conv9_2': [1., 2., 3., 0.5, .333],
                     'conv10_2': [1., 2., 0.5],
                     'conv11_2': [1., 2., 0.5]}

    fmaps = list(fmap_dims.keys())

    prior_boxes = []

    for k, fmap in enumerate(fmaps):
      for i in range(fmap_dims[fmap]):
        for j in range(fmap_dims[fmap]):
          cx = (j + 0.5) / fmap_dims[fmap]
          cy = (i + 0.5) / fmap_dims[fmap]

          for ratio in aspect_ratios[fmap]:
            prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])

            '''
            aspect ratio = 1ì¼ ë•Œ, í˜„ì¬ feature mapì˜ scaleê³¼ ë‹¤ìŒ feature mapì˜ scaleì˜
            ê¸°í•˜ í‰ê· ì˜ scaleì„ ê°€ì§€ëŠ” prior ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©
            '''
            if ratio == 1.:
              try :
                  additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k+1]])
                # ë§Œì•½ ë§ˆì§€ë§‰ feature mapì¼ ê²½ìš°, ë„˜ì–´ê°„ë‹¤.
              except IndexError:
                additional_scale = 1.
              prior_boxes.append([cx, cy, additional_scale, additional_scale])

    prior_boxes = torch.FloatTensor(prior_boxes).to(device) # (8732, 4)
    prior_boxes.clamp_(0,1)

    return prior_boxes

  def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):
    '''
    ê° classì— ëŒ€í•´ì„œ NMSë¥¼ ìˆ˜í–‰í•´ì„œ ìƒìœ„ top_kê°œì˜ box, label, scoreë¥¼ ë°˜í™˜í•œë‹¤.
    predicted_locs : 8732ê°œì˜ prior boxesì— ëŒ€í•œ ì˜ˆì¸¡í•œ ìœ„ì¹˜ (N, 8732, 4)
    predicted_scores : 8732ê°œì˜ prior boxesì— ëŒ€í•œ í´ë˜ìŠ¤ ì˜ˆì¸¡ (N, 8732, n_classes)
    min_score : NMSì˜ threshold ê°’
    max_overlap : 2ê°œì˜ boxê°€ ê²¹ì¹  ìˆ˜ ìˆëŠ” ìµœëŒ“ê°’
    '''
    batch_size : predicted_locs.size(0)
    n_priors = self.priors_cxcy.size(0)
    predicted_scores = F.softmax(predicted_scores, dim=2) # (N, 8732, n_classes)

    # ìµœì¢… ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ boxes, labels, scoresë¥¼ ë‹´ì„ list
    all_images_boxes = list()
    all_images_labels = list()
    all_images_scores = list()

    assert n_priors == predicted_locs.size(0) == predicted_scores.size(1)

    for i in range(batch_size):
      # ì˜ˆì¸¡í•œ boxì˜ ì¢Œí‘œ í˜•ì‹ì„ x,y ê°’ìœ¼ë¡œ ë³€í™˜
      decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy)) # (8732, 4)

      # í•˜ë‚˜ì˜ í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ ê²°ê³¼ê°’
      image_boxes = list()
      image_labels = list()
      image_scores = list()

      max_scores, best_labels = predicted_scores[i].max(dim=1) #(8732,)
    
      # Class í•˜ë‚˜í•˜ë‚˜ ì²´í¬
      for c in range(1, self.n_classes):
        # minimum score ë³´ë‹¤ ë†’ì€ boxì™€ scoreë§Œ ê°€ì§€ê³  ê°„ë‹¤.
        class_scores = predicted_scores[i][:, c] # (8732,)
        score_above_min_score = class_scores > min_score
        n_above_min_score = score_above_min_score.sum().item()
        if n_above_min_score == 0:
          continue
        class_scores = class_scores[score_above_min_score] # (n_qualified)
        class_decoded_locs = decoded_locs[score_above_min_score] # (n_qualified, 4)

        # ì˜ˆì¸¡í•œ boxesì™€ scoresë¥¼ scoresì— ë”°ë¼ ë¶„ë¥˜
        class_scoers, sort_ind = class_scoers.sort(dim=0, descending=True) # (n_qualified), (n_min_score)
        class_decoded_locs = class_decoded_locs # (n_min_score, 4)

        # predicted boxesë¼ë¦¬ ê²¹ì¹˜ëŠ” ê°’ ì°¾ê¸°
        overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)

        # Non-Maximum Suppression êµ¬í˜„
        # suppress => 1, suppress X => 0 
        suppress = torch.zeros((n_above_min_score)).bool().to(device) # (n_qualified)
        for box in range(class_decoded_locs.size(0)):
          # ë§Œì•½ í•´ë‹¹ boxê°€ ì´ë¯¸ suppressë¡œ ë˜ì—ˆë‹¤ë©´ 
          if suppress[box] == 1:
            continue
          
          # maximum overlapë³´ë‹¤ ë§ì´ overlap ëœ boxë“¤ì€ suppress
          suppresss = suppress | (overlap[box] > max_overlap)

        # í•´ë‹¹ í´ë˜ìŠ¤ì—ì„œ suppress ë˜ì§€ ì•Šì€ boxë“¤ë§Œ ê°€ì§€ê³  ê°„ë‹¤.
        image_boxes.append(class_decoded_locs[~suppress])
        image_labels.append(torch.LongTensor((~suppress).sum().item() * [c]).to(device))

      # ë§Œì•½ í•´ë‹¹ í´ë˜ìŠ¤ì— ëŒ€í•´ ì•„ë¬´ objectë„ ì—†ìœ¼ë©´ 'ë°°ê²½' í´ë˜ìŠ¤ë¡œ ì²˜ë¦¬í•œë‹¤.
      if len(image_boxes) == 0:
        image_boxes.append(torch.FloatTensor([[0.,0.,1.,1.]]).to(device))
        image_labels.append(torch.LongTensor([0]).to(device))
        image_scores.append(torch.FloatTensor([0.]).to(device))

      # ìœ„ì˜ ê°’ë“¤ì„ ë³‘í•©í•œë‹¤.
      image_boxes = torch.cat(image_boxes, dim=0) # (n_objects, 4)
      image_labels = torch.cat(image_labels, dim=0) # (n_objects)
      image_scores = torch.cat(image_scores, dim=0) # (n_objects)
      n_objects = image_scores.size(0)

      # ìƒìœ„ top_k ê°œì˜ ê°’ë§Œ ê°€ì§€ê³  ê°„ë‹¤.
      if n_objects > top_k :
        image_scores, sort_ind = image_scores.sort(dim=0, descending= True)
        image_scores = image_scores[:top_k] # (top_k)
        image_boxes = image_boxes[sort_ind][:top_k] # (top_k, 4)
        image_labels = image_labels[sort_ind][:top_k] # (top_k)

      # ì´ì œ ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ê²°ê³¼ê°’ ë°˜í™˜
      all_images_boxes.append(image_boxes)
      all_images_labels.append(image_labels)
      all_images_scores.append(image_scores)
    
    return all_images_boxes, all_images_labels, all_images_scores 


```

### 5. MultiBox Loss

ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì •ì˜í• ê±´ë°, ì†ì‹¤í•¨ìˆ˜ëŠ” ì´ì „ ê¸€ì—ì„œë„ ì •ì˜í–ˆë“¯ì´ localization lossì™€ confidence lossë¥¼ í•©ì¹œê²ƒìœ¼ë¡œ êµ¬í˜„í•œë‹¤. MultiBoxLoss ëª¨ë“ˆì—ì„œëŠ” localization lossì™€ confidence lossë¥¼ êµ¬í˜„í•˜ê¸° ì „ì— ë¨¼ì € Matching Strategyë¥¼ êµ¬í˜„í•œë‹¤.




```python
class MultiBoxLoss(nn.Module):
  def __init__(self, priors_cxcy, threshold = 0.5, neg_pos_ratio = 3, alpha = 1.):
    super(MultiBoxLoss, self).__init__()
    self.priors_cxcy = priors_cxcy
    self.priors_xy = cxcy_to_xy(priors_cxcy)
    self.threshold = threshold
    self.neg_pos_ratio = neg_pos_ratio
    self.alpha = alpha

    self.smooth_l1 = nn.L1Loss()
    self.cross_entropy = nn.CrossEntropyLoss(reduce=False)

  def forward(self, predicted_locs, predicted_scores, boxes, labels):
    '''
    predicted_locs : 8732ê°œì˜ prior boxì— ëŒ€í•œ predicted locations (N, 8732, 4)
    predicted_scores : predicted locationsì— ëŒ€í•œ class score (N, 8732, n_classes)
    boxes : true bounding boxes in boundary coordinates
    labels : true object labels
    ==> return multibox loss
    '''
    batch_size = predicted_locs.size(0)
    n_priors = self.priors_cxcy.size(0)
    n_classes = predicted_scores.size(2)

    assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)

    true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device) # (N, 8732, 4)
    true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device) # (N, 8732)

    # for each image
    for i in range(batch_size):
      n_objects = boxes[i].size(0)
      overlap = find_jaccard_overlap(boxes[i], self.priors_xy) # (n_objects, 8732)

      # prior boxë§ˆë‹¤ ê°€ì¥ overlap ê°’ì´ í° objectë¥¼ ì°¾ëŠ”ë‹¤.
      overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)

      '''
      ê·¸ ì „ì—, 2ê°€ì§€ ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ìƒí™©ì´ ìˆëŠ”ë°, 

      1. í•´ë‹¹ objectê°€ ëª¨ë“  prior boxesì— ëŒ€í•´ ìµœì ì˜ ë¬¼ì²´ê°€ ì•„ë‹ìˆ˜ë„ ìˆë‹¤.
      2. thresholdì˜ ê°’ì´ 0.5ì´ê¸° ë•Œë¬¸ì— ê°ì²´ê°€ ì—†ëŠ” ë°°ê²½(background)ì™€ í• ë‹¹ë˜ëŠ” prior boxë“¤ì´ ìˆì„ ìˆ˜ë„ ìˆë‹¤.
      ==> Matching Strategyë¡œ í•´ê²°!
      '''

      # ë¨¼ì €, ê° objectì— ëŒ€í•´ maximum overlapì„ ê°€ì§€ëŠ” priorë¥¼ ì°¾ëŠ”ë‹¤.
      _, prior_for_each_object = overlap.max(dim=0)

      # ê·¸ë¦¬ê³  ê° ë¬¼ì²´ì— í•´ë‹¹í•˜ëŠ” maximum-overlap-priorë¥¼ í• ë‹¹í•œë‹¤. (ì›ì¹˜ ì•ŠëŠ” ìƒí™© 1 í•´ê²°!)
      object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)

      # ì¸ìœ„ì ìœ¼ë¡œ í•´ë‹¹ prior boxì˜ overlapì„ 0.5ë³´ë‹¤ í° ê°’ì„ ì¤€ë‹¤. (ì›ì¹˜ ì•ŠëŠ” ìƒí™© 2 í•´ê²°!)
      overlap_for_each_prior[prior_for_each_object] = 1.

      # ê° prior boxì˜ label 
      labels_for_each_prior = labels[i][object_for_each_prior] # (8732)

      # objectê³¼ì˜ overlapì´ thresholdë³´ë‹¤ ì‘ì€ prior boxëŠ” ë°°ê²½(no object)ì— í• ë‹¹í•œë‹¤.
      labels_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)

      # êµ¬í•œ ê°’ë“¤ ì €ì¥
      true_classes[i] = labels_for_each_prior

      # ì°¾ì€ prior boxë“¤ì„ center-size object ì¢Œí‘œë¡œ encode í•œë‹¤.
      true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy) # (8732, 4)

    # positive(non-background) priorsë¥¼ ì •ì˜í•œë‹¤.
    positive_priors = true_classes != 0 # (N, 8732)

    # ì´ì œ Localization Lossë¥¼ êµ¬í˜„í• ê±´ë°, ì´ëŠ” positive priorsì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•œë‹¤.
    loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])

    '''
    Confidence Lossë¥¼ êµ¬í˜„í• ê±´ë°, ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ ë‚´ì—ì„œ ë°°ê²½ì— í•´ë‹¹í•˜ëŠ” boxì˜ ë¹„ìœ¨ì´ ë†’ê¸° ë•Œë¬¸ì— 
    negative sampleì´ positive sampleë³´ë‹¤ í›¨ì”¬ ë§ì•„ì„œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì¼ì–´ë‚˜ëŠ”ë°,
    ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Hard Negative Mining ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤.

    ê° imageì— ëŒ€í•´ ê°€ì¥ í° lossë¥¼ ê°€ì§€ëŠ” (neg_pos_ratio * n_positives)ì¸ 
    hardest negative prior boxesë¥¼ sampleì— ì¶”ê°€í•´ì„œ ëª¨ë¸ì´ hardest negative priorsì— ì§‘ì¤‘í•´ì„œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ìµœì†Œí™”í•˜ë„ë¡ í•œë‹¤.
    '''
    n_positives = positive_priors.sum(dim=1)
    n_hard_negatives = self.neg_pos_ratio * n_positives # negative, postive ë¹„ìœ¨ì€ 3:1ë¡œ í•œë‹¤.

    # ì „ì²´ prior boxì— ëŒ€í•´ loss ê³„ì‚°
    conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))
    conf_loss_all = conf_loss_all.view(batch_size, n_priors) # (N, 8732)

    conf_loss_pos = conf_loss_all[positive_priors]

    # hard negative priorsë¥¼ ì°¾ì„ê±´ë°, ì´ë¥¼ ìœ„í•´ì„œ negative priors ì¤‘ì—ì„œ ìƒìœ„ n_hard_negativesê°œì˜ priorsë¥¼ ì¶”ì¶œí•œë‹¤.
    conf_loss_neg = conf_loss_all.clone()
    conf_loss_neg[positive_priors] = 0. # positive priorì˜ lossëŠ” ì œì™¸
    conf_loss_neg, _ = conf_loss_neg.sort(dim=1, desceding = True)
    hardness_rank = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(deivce)
    hard_negatives = hardness_rank < n_hard_negatives.unsqueeze(1)
    conf_loss_hard_neg = conf_loss_neg[hard_negatives]

    # ë…¼ë¬¸ì—ì„œì²˜ëŸ¼ hard negativeì™€ positive priorsì˜ lossì˜ í•©ì„ positive priorsì˜ ìˆ˜ë¡œë§Œ ë‚˜ëˆˆë‹¤.
    conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()

    # ìµœì¢… loss (alpha ê°’ì€ ë‘ loss ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ëŠ” balancing parameter)
    return conf_loss + self.alpha * loc_loss
      
```

êµ‰ì¥íˆ ì½”ë“œê°€ ê¸¸ì—ˆì§€ë§Œ, ì´ê²ƒìœ¼ë¡œ SSD ëª¨ë¸ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œì•„ë³´ì•˜ë‹¤.

ì¶œì²˜, ì°¸ê³ ë¬¸í—Œ:

[ì›ë³¸ github ì½”ë“œ](https://github.com/Jeffkang-94/pytorch-SSD300/blob/6cb6b3ce0cb98e3f5d2a9fa805cd9b9273a7714b/model.py#L372)

[ê°œì¸ ë¸”ë¡œê·¸](https://rain-bow.tistory.com/entry/Object-Detection-Object-Detection-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-Implementation)
