---
title: '[DL/KERAS] 머신러닝의 목표 - 최적화 vs 일반화 🎯'
layout : single
toc: true
toc_sticky: true
categories:
  - kerasBasic
---

# 4. 머신러닝의 목표 (K겹 교차검증, 훈련 성능 향상, 일반화 성능 향상)

### 4.1 일반화 : 머신러닝의 목표
머신러닝의 근본적인 이슈는 **최적화**와 **일반화** 사이의 줄다리기이다. 최적화는 훈련 데이터에서 최고의 성능을 얻으려는 과정이고 , 일반화는 모델이 처음보는 데이터에서 좋은 성능을 얻으려는 과정이다.

#### 4.1.1 과소적합과 과대적합
훈련 초기에 최적화와 일반화는 상호 연관되어 있다. 훈련 데이터의 손실이 낮아지면 테스트 데이터의 손실도 낮아진다. 이런 상황을 모델이 **과소적합** 되었다고 한다. 즉,모델이 훈련 데이터의 모든 패턴을 학습하지 못한 상태이다. 하지만 훈련을 반복하다보면 일반화 성능이 더 이상 높아지지 않고 검증 세트의 성능이 감소하기 시작한다. 즉, 모델이 **과대적합** 되었다고 한다. 이 상태는 새로운 데이터와 관련이 적고 잘못된 판단을 하게 만든다.

(*과대적합은 잡음,불확실성,드문 특성이 포함되어 있을 때 발생할 가능성이 높다*.)

### 4.2 머신러닝 모델 평가
#### 4.2.1 훈련, 검증, 테스트 데이터
훈련 데이터와 테스트 데이터만 사용하는 것이 아닌 검증 데이터도 사용하는 이유는 뭘까? 검증 데이터를 통해 모델의 **하이퍼파라미터**를 튜닝하기 때문이다. 결국 검증 데이터로 직접 훈련하지 않더라도 **정보 누설** 때문에 모델이 검증 데이터에 빠르게 과대적합 될 수 있다. 따라서 테스트 데이터는 모델을 출시하기 직전에 닥 한번만 사용해야한다.

1. 단순 홀드아웃 검증 : 데이터의 일정량을 테스트 세트로 떼어놓는다. 다음 코드는 홀드아웃을 간단히 구현한 예이다.


```python
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

num_validation_samples = 10000
np.random.shuffle(data) # 데이터를 섞는 것이 일반적으로 좋다.
validation_data = data[:num_validation_samples] # 검증 세트를 떼어놓는다.
training_data = data[num_validation_samples:] # 검증 세트를 떼어놓고 남은 훈련데이터 세트

model = keras.Sequential([
    layers.Dense(512,activation='relu'),
    layers.Dense(10,activation='softmax')
])
model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(training_data,...)
validation_score = model.evaluate(validation_data,...)
.
. # 모델을 튜닝 , 훈련 , 평가 반복
.
model = keras.Sequential([
    layers.Dense(512,activation='relu'),
    layers.Dense(10,activation='softmax')
])
model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(np.concatenate([training_data,validation_data]),...) #하이퍼파라미터 튜닝 후 테스트 데이터를 제외한 모든 데이터 사용해서 다시 훈련
test_score = model.evaluate(test_data,...) # 최종적으로 테스트 데이터로 손실 확인
```

단순 홀드아웃 검증은 너무 단순하기 때문에 데이터가 적을 때는 검증 세트와 테스트 세트의 샘플이 너무 적어서 주어진 전체 데이터를 통계적으로 대표하지 못한다는 단점이 있다.

그래서 다음에 나오는 K-겹 교차 검증과 반복 K-겹 교차 검증이 이 문제를 해결할 수 있다.

#### 4.2.2 K-겹 교차 검증
이 방식에서는 데이터를 동일한 크기의 K개의 분할로 나눈다. 

![kfold](https://user-images.githubusercontent.com/77332628/195969010-71cd7dac-5950-4caf-9ace-736b014c84ac.png)

위의 사진처럼 각 분할 i에 대해서 남은 K-1개의 분할로 모델을 훈련하고 분할 i에서 모델을 평가한다. 이렇게 얻은 K개의 점수를 평균해서 최종 점수을 얻게 된다. 이 방법은 모델의 성능이 데이터 분할에 따라 편차가 클 때 도움이 된다.

다음은 K-겹 분할 검증을 구현한 코드이다.


```python
k = 3
num_validation_samples = len(data) / k 
np.random.shuffle(data)
validation_scores = []
for fold in range(k):
  validation_data = data[num_validation_samples * fold:
                         num_validation_samples * (fold + 1)] # 각 폴드마다 검증 데이터 선택
  training_data = np.concatenate(
      data[:num_validation_samples * fold],
      data[num_validation_samples * (fold + 1):]) # 각 폴드마다 추출한 검증 데이터를 제외한 훈련 데이터를 선택
  model = model = keras.Sequential([
    layers.Dense(512,activation='relu'),
    layers.Dense(10,activation='softmax')
  ])
  model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])  # 훈련되지 않은 새로운 모델 구축
  model.fit(training_data) # 훈련 데이터로 훈련


  validation_score = model.evaluate(validation_data,...) # 각 폴드마다 검증 점수 
  validation_scores.append(validation_score)             # 각 폴드마다 얻은 검증 점수를 validation_scores 에 저장
  validation_score = np.average(validation_scores)       # 얻은 검증 점수들의 평균 = 최종점수

model = model = keras.Sequential([
    layers.Dense(512,activation='relu'),
    layers.Dense(10,activation='softmax')])
model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',
              metrics=['accuracy']) 
model.fit(data,...) # 테스트 데이터 제외한 전체 데이터로 최종 모델 훈련
test_score = model.evaluate(test_data,...) # 테스트 데이터 점수


```

#### 4.2.3 상식 수준의 기준점 넘기

딥러닝 모델을 구축할 때 데이터셋으로 작업을 시작하기 전에 항상 넘어야 할 간단한 **기준점**을 정해야 한다. 이 기준점은 머신러닝을 사용하지 않고 생각할 수 있는 가장 간단한 방법이 될 수 있다. 아무도 해결하지 못한 문제를 머신러닝 모델로 해결하고자 할 때 참고할 수 있는 상식 수준의 기준점이 필수적이다.

#### 4.2.4 모델 평가에 대한 유의할 점


*   **데이터의 대표성** : 훈련세트와 테스트세트가 주어진 데이터에 대한 대표성을 가지고 있어야 한다. 그래서 보통 훈련 세트와 테스트 세트로 나누기 전에 무작위로 섞는 방법을 사용한다.
*   **시간의 방향** : 과거로부터 미래를 예측하려고 하면 데이터를 분할하기 전에 무작위로 섞어서는 절대 안된다. 이렇게 하면 미래의 정보가 모델에 누설되기 때문이다.
*   **데이터 중복** : 한 데이터셋에서 똑같은 데이터 포인트가 두번 등장하면 데이터 분할 후 훈련 데이터와 테스트 데이터에 똑같은 데이터가 들어있을 수도 있기 때문에 중복되는 데이터를 유의해야한다.

### 4.3 훈련 성능 향상하기
최적의 모델을 얻으려면 먼저 과대적합이 되어야 한다. 아래의 사진 처럼 최적적합의 경계가 어디인지 미리 알지 못하기 때문에 경계를 찾으려면 경계를 넘어가 봐야한다. 

![overfitting](https://user-images.githubusercontent.com/77332628/195969012-fc3f8f21-fe09-4333-93c8-773dfead74f9.png)

따라서 모델 구축의 초기 목표는 약간의 일반화 능력을 보이고 과대적합할 수 있는 모델을 얻는 것이다. 과대적합하는 모델을 구축한 후 일반화 성능을 개선하는 것에 초점을 맞추는 것이다. 과대적합을 하는 과정에서 일반적으로 다음 세가지의 문제가 발생한다.


*   시간이 지나도 훈련 손실이 줄어들지 않는다.
*   모델이 의미 있는 일반화를 달성하지 못한다. (*상식 수준의 기준점을 넘지 못한다.*)
*   손실이 모두 줄어들고 상식 수준의 기준점을 넘었지만 과대적합 되지 않는다. (*여전히 과소적합 상태이다.*)

위의 문제들을 해결하기 위한 해결책들이다.

#### 4.3.1 경사 하강법의 핵심 파라미터 튜닝하기
훈련이 시작되지 않거나 너무 일찍 중단되면 손실은 줄지 않고 멈추어 있게 된다. 이런 상황이 발생하면 경사 하강법의 파라미터인 **옵티마이저**, **모델 가중치의 초기값 분포**, **학습률**, **배치 크기**등을 튜닝해야한다. (*일반적으로는 학습률과 배치 크기만을 튜닝하는 것으로 충분하다.*)

#### 4.3.2 구조에 대해 더 나은 가정하기
모델이 훈련을 시작하지만 검증 지표가 전혀 나아지지 않는다면 모델 구축의 접근 방식에 근본적으로 잘못된 무언가가 있다는 의미이다. 예를 들어 데이터의 종류에 맞지 않는 모델을 사용했을 수도 있다. 

#### 4.3.3 모델 용량 늘리기
모델이 훈련되고 검증 지표가 향상되고 최소한의 일반화 능력을 달성한다면 모델을 과대적합 시켜야한다. 다음 코드는 MNIST 픽셀에서 훈련하는 작은 모델이다.
 









```python
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
import numpy as np

(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
model = keras.Sequential([layers.Dense(10, activation="softmax")])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
history_small_model = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    validation_split=0.2,verbose=0)
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
    11490434/11490434 [==============================] - 0s 0us/step


이 모델을 훈련하면 다음과 같은 손실 곡선을 얻는다.


```python
import matplotlib.pyplot as plt
val_loss = history_small_model.history["val_loss"]
epochs = range(1, 21)
plt.plot(epochs, val_loss, "b--",
         label="Validation loss")
plt.title("Effect of insufficient model capacity on validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```


    
![20221015first](https://user-images.githubusercontent.com/77332628/195968985-01386efa-e318-4342-9d7f-894f969e17cc.png)
    


위의 그래프에서는 검증 손실이 0.26에 도달한 후 그 지점에서 정체되어 있다.
모델을 훈련했지만 여러번의 반복 후에도 과대적합을 이뤄내지 못했다. 

**항상 과대적합이 가능하다는 것을 기억하자.** 위의 경우에는 모델의 용량이 작기 때문에 (*즉 모델의 **표현능력** 이 부족하기*) 때문에 용량이 더 큰 모델이 필요하다.
 즉, 더 많은 정보를 저장할 수 있는 모델이 필요하다. 층을 추가하거나 , 층의 크기를 늘리거나 , 층의 종류를 바꾸는 등의 방법이 있다.

다음 코드는 층의 크기를 늘리고 층의 개수도 늘린 모델로 학습한 결과이다.


```python
model = keras.Sequential([
    layers.Dense(96, activation="relu"),
    layers.Dense(96, activation="relu"),
    layers.Dense(10, activation="softmax"),
])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
history_large_model = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    verbose=0)
```


```python
import matplotlib.pyplot as plt
val_loss = history_large_model.history["val_loss"]
epochs = range(1, 21)
plt.plot(epochs, val_loss, "b--",
         label="Validation loss")
plt.title("Effect of insufficient model capacity on validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```


    
![20221015second](https://user-images.githubusercontent.com/77332628/195968994-f2337ff8-2107-4978-b8a4-c4d0734201ab.png)
    


검증 곡선이 8번째 에포크 이후에 과대적합을 이뤄낸 것을 알 수 있다.

### 4.4 일반화 성능 향상하기

모델이 최소한의 일반화 성능을 가지고 과대적합할 수 있다면 그 다음으로 모델의 일반화를 극대화 해야한다.
#### 4.4.1 데이터셋 큐레이션
적절한 데이터셋을 사용하는 것이 모델 개발에 투자하는 것보다 거의 항상 더 좋은 결과를 가져다준다.

*   입력에서 출력을 매핑하는 공간을 조밀하게 샘플링해야는 것이 중요하다. 데이터가 많을수록 좋은 모델이 만들어지기 때문에 **데이터가 충분한지 확인**해야한다.
*   입력 데이터를 시각화아여 이상치를 확인하고 레이블을 조정해서 **레이블 할당 에러를 최소화**해야한다.
*   데이터를 정제하고 누락된 값을 처리하고 , 데이터의 많은 특성 중에서 어떤 것이 유용한지 확실하지 않다면 특성 선택을 수행해야한다.

#### 4.4.2 특성 공학
특성공학은 데이터와 머신러닝 알고리즘에 관한 지식을 사용하는 단계이다. 모델에 데이터를 주입하기 전에 (*학습이 아닌*) 하드코딩된 변환을 적용해서 알고리즘이 더 잘 수행되도록 만드는 것이다. 모델이 수월하게 작업할 수 있도록 데이터의 표현 방식을 바꾼다고 생각하면 편하다.

다행히 최신 딥러닝은 특성공학이 대부분 필요하지 않다. 신경망이 자동으로 원본 데이터에서 유용한 특성을 추출하기 때문이다. 하지만 특성 공학을 사용하면 **더 적은 데이터로 문제를 풀 수 있게 된다.** 딥러닝 모델이 스스로 특성을 학습하는 능력은 가용한 훈련 데이터가 많을 때 발휘되는데 , 샘플 개수가 적다면 특성 공학이 도움이 될 것이다.

#### 4.4.3 조기 종료 사용하기
딥러닝에서는 항상 너무 많은 파라미터가 많은 모델을 사용하기 때문에 모델이 훈련을 끝까지 하면 일반화가 전혀 되지 않을 것이다. 훈련 중 일반화 성능이 가장 높은 최적접합의 지점을 찾는 것이 중요하다. 케라스에서는 일반적으로 **EarlyStopping 콜백함수**를 사용하여 이를 처리한다.

#### 4.4.4 모델 규제하기
**규제(regularization)**은 훈련 데이터에 모델을 완벽히 맞추려는 것을 적극적으로 방해하는 기법이다. 모델 규제는 항상 정확한 평가 절차를 따라야하는 과정임을 명심해야한다. 측정이 가능한 경우에만 일반화를 달성할 수 있다.

과대적합을 완화시키는 가장 간단한 방법은 모델의 크기를 줄이는 것이다. 모델의 기억 용량에 제한이 있다면 모델이 데이터를 단순히 다 외워버리진 못할것이다. 동시에 기억해야 할 것은 과소적합을 피하기 위해 충분한 파라미터를 가진 모델을 사용해야한다는 것이다.

또 다른 방법은 모델의 복잡도에 제한을 두어 가중치가 작은 값을 갖도록 규제하는 것이다. 이 방법을** 가중치 규제 (weight regularization)**이라고 하며, 두가지 종류가 있다.


1.   **L1 규제** : 가중치의 절댓값에 비례하는 비용이 추가된다. (가중치의 L1 노름(norm).)
2.   **L2 규제** : 가중치의 제곱에 비례하는 비용이 추가된다.  (가중치의 L1 노름(norm).) L2 규제는 신경망에서 가중치 감쇠라고도 부른다.

케라스에서 가중치 규제 객체를 층의 키워드 매개변수로 전달하여 가중치 규제를 추가할 수 있다. imbd 영화 리뷰 분류 모델에 가중치 규제를 적용해보자.




```python
'''여기서 부터'''
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
import numpy as np
from tensorflow.keras.datasets import imdb
(train_data, train_labels), _ = imdb.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
train_data = vectorize_sequences(train_data)
'''여기까지는 imbd 데이터를 준비하는 코드'''

from tensorflow.keras import regularizers
model = keras.Sequential([
    layers.Dense(16,
                 kernel_regularizer=regularizers.l2(0.002), #lr(0.002): 가중치 행령의 모든 원소를 제곱하고 0.002를 곱해 모델 전체 손실에 더해진다.
                 activation="relu"),
    layers.Dense(16,
                 kernel_regularizer=regularizers.l2(0.002), #lr(0.002): 가중치 행령의 모든 원소를 제곱하고 0.002를 곱해 모델 전체 손실에 더해진다.
                 activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_l2_reg = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4,
    verbose=0)

```


```python
''' 규제를 적용하지 않은 원래 모델'''
from tensorflow.keras.datasets import imdb
(train_data, train_labels), _ = imdb.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
train_data = vectorize_sequences(train_data)

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_original = model.fit(train_data, train_labels,
                             epochs=20, batch_size=512, validation_split=0.4,verbose=0)
'''규제를 적용하지 않은 일반 모델'''
```




    '규제를 적용하지 않은 일반 모델'




```python
import matplotlib.pyplot as plt
val_loss = history_original.history["val_loss"]
val_loss_l2 = history_l2_reg.history["val_loss"]
epochs = range(1, 21)

plt.plot(epochs, val_loss, "b--",
         label="Validation loss of original model")
plt.plot(epochs, val_loss_l2, "b", 
         label="Validation loss of L-2 model")      #L-2규제 모델 손실 그래프
plt.title("Effect of regularizaiton model on validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```


    
![20221015third](https://user-images.githubusercontent.com/77332628/195969009-2e36c691-f773-47ed-9c11-2d5eede366df.png)
    


위의 그래프에서 볼 수 있듯이 두 멜이 동일한 파라미터 개수를 가지고 있더라도 L2 규제를 사용한 모델이 일반 모델보다 과대적합에 잘 견디고 있다.

가중치 규제는 일반적으로 작은 딥러닝 모델에서 사용된다. 대규모 딥러닝 모델은 파라미터가 너무 많기 때문에 가중치 규제가 일반화에 큰 영향을 미치지 않는다. 

이런 경우 **드롭아웃(DropOut) **기법이 사용된다. 모델 층에 드롭아웃을 적용하면 훈련하는 동안 무작위로 층의 출력 특성을 일부 제외시킨다.(0으로 만든다.) 드롭아웃 비율은 0이 될 특성의 비율이다. 보통 0.2에서 0.5 사이의 값으로 설정한다.하지만 테스트 단계에서는 어떤 유닛도 드롭아웃 되지 않는다. 그 대신에 층의 출력을 드롭아웃 비율에 비례하여 줄여준다. 

![dropout](https://user-images.githubusercontent.com/77332628/195968901-444d1f57-eb38-49e6-a80a-5beda983514b.png)

다음은 imbd 데이터셋 모델에 2개의 Dropout 층을 추가한 코드다. Dropout을 추가함으로서 과대적합이 얼마나 줄었는지 확인해보자.


```python
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_dropout = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4,
    verbose=0)
```


```python
import matplotlib.pyplot as plt
val_loss = history_original.history["val_loss"]
val_loss_dropout = history_dropout.history["val_loss"]
epochs = range(1, 21)

plt.plot(epochs, val_loss, "r--",
         label="Validation loss of original model")
plt.plot(epochs, val_loss_dropout, "b",
         label="Validation loss of Dropout model")  #드롭아웃 모델 손실 그래프
plt.plot(epochs, val_loss_l2, "g--", 
         label="Validation loss of L-2 model")      #L-2규제 모델 손실 그래프
plt.title("Effect of regularizaiton model on validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```


    
![20221015fourth](https://user-images.githubusercontent.com/77332628/195969007-46532b8d-9a40-4fd3-8125-c9d360b786dc.png)
    


위의 그래프에서 볼 수 있듯이 드롭아웃 모델이 기본 모델보다 확실히 향상되었고 L-2 규제모델보다 더 낮은 검증 손실을 달성했기 때문에 L-2 규제모델보다도 잘 작동하는 것을 알 수 있다.

[<케라스 창시자에게 배우는 딥러닝 개정 2판>(길벗, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.] 출처: 프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496
