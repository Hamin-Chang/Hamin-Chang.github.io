---
title : '[OD/Pytorch] íŒŒì´í† ì¹˜ë¡œ Mask R-CNN êµ¬í˜„í•˜ê¸° ğŸ­'
layout: single
toc: true
toc_sticky: true
categories:
  - ODCode
---

## Pytorchë¡œ Mask R-CNN êµ¬í˜„í•˜ê¸° (FPN êµ¬í˜„ í¬í•¨)

ì´ë²ˆ ê¸€ì—ì„œëŠ” [<U>Matterportì˜ repository</U>](https://github.com/multimodallearning/pytorch-mask-rcnn)ë¥¼ ì°¸ê³ í•´ì„œ pytorchë¡œ Mask R-CNNì„ ì§ì ‘ êµ¬í˜„í•´ë³´ê² ë‹¤. Mask R-CNN ëª¨ë¸ì— ëŒ€í•œ ì„¤ëª…ì€ [<U>Mask R-CNN ë…¼ë¬¸ ë¦¬ë·°</U>](https://hamin-chang.github.io/cv-objectdetection/maskrcnn/)ë¥¼ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤.

### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ, í•„ìš”í•œ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ ì •ì˜

ë¨¼ì € ëª¨ë¸ êµ¬ì¶•ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¡œë“œí•´ì£¼ì.


```python
from google.colab import drive
drive.mount('/content/drive')
```

    Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).



```python
import datetime
import math
import os
import random
import re

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable

import utils
import visualize
nms.nms_wrapper import nms
from roialign.roi_align.crop_and_resize import CropAndResizeFunction
```

ë‹¤ìŒ ì½”ë“œëŠ” ì „ì²´ ì½”ë“œë¥¼ ëŒë ¸ì„ ë•Œ text massageê°€ ëœ¨ê²Œí•˜ëŠ” í•¨ìˆ˜ì™€ iteration ì§„í–‰ í˜„í™©ì„ ë³´ì—¬ì£¼ëŠ” ê¸°ëŠ¥ì„ í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê³¼ì •ì´ë‹¤. Mask R-CNN êµ¬í˜„ì˜ ì§ì ‘ì ì¸ ë¶€ë¶„ì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ê°€ë³ê²Œ ë³´ê³  ë„˜ì–´ê°€ë©´ ì¢‹ì„ê±° ê°™ë‹¤.


```python
# text massageë¥¼ printí•˜ëŠ” í•¨ìˆ˜, Numpy arrayê°€ ìƒì„±ë˜ë©´ arrayì˜ shape, min, max value ì¶œë ¥í•œë‹¤.
def log(text, array=None):
   if array is not None:
     text = text.ljust(25)
     text += ("shape : {:20}, min: {:10.5f} max: {:10.5f}".format(
         str(array.shape), array.min() if array.size else "",
         array.max() if array.size else ""))
     
# terminal progress bar ìƒì„±í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def printProgressbar(iteration, total, prefix='',suffix='', decimals=1, length=100, fill='â–ˆ'):
  percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
  filledLength = int(length * iteration // total)
  bar = fill * filledLength + '-' * (length - filledLength)
  print('\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\n')
  
  if iteration == total:
    print()
```

ì°¸ê³ í•œ repositoryì—ì„œëŠ” ë³¸ê²©ì ìœ¼ë¡œ Mask R-CNNì„ êµ¬í˜„í•˜ê¸° ì „ì— ì´ì— í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•œë‹¤. ì´ ë¶€ë¶„ë„ í° ë¹„ì¤‘ì„ ë‘ê³  ë³¼ í•„ìš”ëŠ” ì—†ì„ ê²ƒ ê°™ë‹¤.


```python
from typing_extensions import Required
def unique1d(tensor):
    if tensor.size()[0] == 0 or tensor.size()[0] == 1:
        return tensor
    tensor = tensor.sort()[0]
    unique_bool = tensor[1:] != tensor [:-1]
    first_element = Variable(torch.ByteTensor([True]), requires_grad=False)
    if tensor.is_cuda:
        first_element = first_element.cuda()
    unique_bool = torch.cat((first_element, unique_bool),dim=0)
    return tensor[unique_bool.data]

def intersect1d(tensor1, tensor2):
    aux = torch.cat((tensor1, tensor2),dim=0)
    aux = aux.sort()[0]

# pytorchì—ëŠ” Log2 í•¨ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì— ë§Œë“¤ì–´ì¤€ë‹¤.
def log2(x):
  ln2 = Variable(torch.log(torch.FloatTensor([2.0])), requires_grad=False)
  if x.is_cuda:
    ln2 = ln2.cuda()
  return torch.log(x) / ln2

# tensorflowì˜ padding='same'ì„ êµ¬í˜„í•˜ëŠ” í´ë˜ìŠ¤
class SamePad2d(nn.Module):
  def __init__(self, kernel_size, stride):
    super(SamePad2d, self).__init__()
    self.kernel_size = torch.nn.modules.utils._pair(kernel_size)
    self.stride = torch.nn.modules.utils._pair(stride)

  def forward(self,input):
    in_width = input.size()[2]
    in_height = input.size()[3]
    out_width = math.ceil(float(in_width)/float(self.stride[0]))
    out_height = math.ceil(float(in_height) / float(self.stride[1]))
    pad_along_width = ((out_width - 1) * self.stride[0] +
                           self.kernel_size[0] - in_width)
    pad_along_height = ((out_height - 1) * self.stride[1] +
                            self.kernel_size[1] - in_height)
    pad_left = math.floor(pad_along_width / 2)
    pad_top = math.floor(pad_along_height / 2)
    pad_right = pad_along_width - pad_left
    pad_bottom = pad_along_height - pad_top
    return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), 'constant', 0)
  
  def __repr__(self):
    return self.__class__.__name__
```

### 2. Backbone Network êµ¬í˜„

ë‹¤ìŒìœ¼ë¡œëŠ” Mask R-CNNì˜ Backbone networkë¡œ ì‚¬ìš©í•  ResNet-FPNì„ êµ¬í˜„í•˜ì. ë¨¼ì € FPNì„ êµ¬í˜„í•´ë³´ì.


```python
# FPNì—ì„œ TopDown + Lateral connection êµ¬í˜„ class
class TopDownLayer(nn.Module):
  def __init__(self, in_channels, out_channels):
    super(TopDownLayer, self).__init__()
    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)
    self.padding2 = SamePad2d(kernel_size=3, stride=1)
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1)

  def forward(self, x, y):
    y = F.upsample(y, scale_factor=2)
    x = self.conv1(x)
    return self.conv2(self.padding2(x+y))

# Lateral connection ì´í›„ 3x3 convë¥¼ ì ìš©í•´ì„œ P6~P1 feature map ì¶œë ¥
class FPN(nn.Module):
  def __init__(self, C1, C2, C3, C4, C5, out_channels):
    super(FPN, self).__init__()
    self.out_channels = out_channels
    self.C1 = C1
    self.C2 = C2
    self.C3 = C3
    self.C4 = C4
    self.C5 = C5
    self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)
    self.P5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride =1)
    self.P5_conv2 = nn.Sequential(SamePad2d(kernel_size=3, stride=1),
                                  nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1))
    self.P4_conv1 = nn.Conv2d(1024, self.out_channels, kernel_size=1, stride =1)
    self.P4_conv2 = nn.Sequential(SamePad2d(kernel_size=3, stride=1),
                                  nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1))
    self.P3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride =1)
    self.P3_conv2 = nn.Sequential(SamePad2d(kernel_size=3, stride=1),
                                  nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1))
    self.P2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride =1)
    self.P2_conv2 = nn.Sequential(SamePad2d(kernel_size=3, stride=1),
                                  nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1))
  def forward(self, x):
    x = self.C1(x)
    x = self.C2(x)
    c2_out = x
    x = self.C3(x)
    c3_out = x
    x = self.C4(x)
    c4_out = x
    x = self.C5(x)
    p5_out = self.P5_conv1(x)
    p4_out = self.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2)
    p3_out = self.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2)
    p2_out = self.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2)

    p5_out = self.P5_conv2(p5_out)
    p4_out = self.P4_conv2(p4_out)
    p3_out = self.P3_conv2(p3_out)
    p2_out = self.P2_conv2(p2_out)

    # P6ëŠ” RPNì—ì„œ 5ë²ˆì§¸ anchor boxì— ì‚¬ìš©ë˜ë©°, P5ë¥¼ 2ë°° subsamplingí•œë‹¤.
    p6_out = self.P6(p5_out)

    return [p2_out, p3_out, p4_out, p5_out, p6_out]
    
```

FPN ëª¨ë¸ ë‹¤ìŒì— ResNetì„ êµ¬í˜„í•´ë³´ì.


```python
# ResNetì— ì‚¬ìš©ë˜ëŠ” Bottleneck êµ¬ì¡°ë¥¼ ìœ„í•œ class
class Bottleneck(nn.Moduule):
  expansion = 4

  def __init__(self, inplanes, planes, stride=1, downsample=None):
    super(Bottleneck, self).__init__()
    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride)
    self.bn1 = nn.BatchNorm2d(planes, eps=0.001, momentum= 0.01)
    self.padding2 = SamePad2d(kernel_size=3, stride=1)
    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3)
    self.bn2 = nn.BatchNorm2d(planes, eps=0.001, momentum = 0.01)
    self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1)
    self.bn3 = nn.BatchNorm2d(planes * 4, eps=0.001, momentum = 0.01)
    self.relu = nn.ReLU(inplace=True)
    self.downsample = downsample
    self.stride = stride

  def forward(self, x):
    residual = x
    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)
    
    out = self.padding2(out)
    out = self.conv2(out)
    out = self.bn2(out)
    out = self.relu(out)

    out = self.conv3(out)
    out = self.bn3(out)

    if self.downsample is not None:
      residual = self.downsample(x)

    out += residual
    out = self.relu(out)

    return out

# Backbone networkë¡œ ì‚¬ìš©ë˜ëŠ” ResNet Network êµ¬í˜„
class ResNet(nn.Module):
  def __init__(self, architecture, stage5 = False):
    super(ResNet, self).__init__()
    assert architecture in ['resnet50', 'resnet101']
    self.inplanes = 64
    self.layers = [3,4,{'resnet50':6, 'resnet101':23}[architecture],3]
    self.block = Bottleneck
    self.stage5 = stage5

    self.C1 = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size = 7, stride=2, padding=3),
        nn.BatchNorm2d(64, eps=0.001, momentum=0.01),
        nn.ReLU(inplace=True),
        SamePad2d(kernel_size=3, stride=2),
        nn.MaxPool2d(kernel_size=3, stride=2)
    )
    self.C2 = self.make_layer(self.block, 64, self.layers[0])
    self.C3 = self.make_layer(self.block, 128, self.layers[1], stride=2)
    self.C4 = self.make_layer(self.block, 256, self.layers[2], stride=2)
    if self.stage5:
      self.C5 = self.make_layer(self.block, 512, self.layers[3], stride=2)
    else:
      self.C5 = None

    def stages(self):
      return [self.C1, self.C2, self.C3, self.C4, self.C5]
    
    def make_layer(self, block, planes, blocks, stride=1):
      downsample = None
      if stride != 1 or self.inplanes != planes * block.expansion:
        downsample = nn.Sequential(
            nn.Conv2d(self.inplanes, planes*block.expansion, kernel_size=1, stride=stride),
            nn.BatchNorm2d(planes * block.expansion, eps = 0.001, momentum=0.01))
      
      layers = []
      layers.append(block(self.inplanes, planes, stride, downsample))
      self.inplanes = planes * block.expansion
      for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, stride, downsample))
        
      return nn.Sequential(*layers)

```

### 3. Faster R-CNN êµ¬í˜„

ì´ì œ Faster R-CNNì˜ í•™ìŠµê³¼ì •ì„ êµ¬í˜„í• ê±´ë°, ê°œë…ì ì¸ ë¶€ë¶„ì€ [<U>Faster R-CNN ë…¼ë¬¸ ë¦¬ë·°</U>](https://hamin-chang.github.io/cv-objectdetection/ffrcnn/#4-faster-rcnn-%ED%9B%88%EB%A0%A8%ED%95%98%EA%B8%B0)ë¥¼ ì°¸ê³ í•˜ë©´ì„œ ë”°ë¼ì˜¤ë©´ ë” ì´í•´í•˜ê¸° ì‰¬ìš¸ê²ƒì´ë‹¤. (ë¬¼ë¡  ëª…ì¹­ ê°™ì€ ê²ƒë“¤ì´ ì™„ì „íˆ ë˜‘ê°™ì§€ëŠ” ì•Šë‹¤.)

![2](https://user-images.githubusercontent.com/77332628/220282137-206639c1-f562-4936-9698-43dabaf30563.png)

#### 3.1 Proposal layer

anchor scoreë¥¼ ë°›ì•„ì„œ ë‹¤ìŒ ë‹¨ê³„ì— ì „ë‹¬í•  region proposalë¥¼ ì¶”ì¶œí•œë‹¤. Non maximums suppressionì„ ì ìš©í•´ì„œ ìƒìœ„ ëª‡ê°œì˜ proposalsë§Œ ì „ë‹¬í•œë‹¤.



```python
# bboxì˜ delta ê°’ì„ bboxì— ì ìš©í•˜ëŠ” í•¨ìˆ˜
# boxes : [N,4] <= y1, x1, y2, x2
# deltas : [N,4] <= [dy, dx, log(dh), log(dw)]
def apply_box_deltas(boxes, deltas):
  # y, x, h, wë¡œ ë³€í™˜
  height = boxes[:,2] - boxes[:,0]
  width = boxes[:,3] - boxes[:,1]
  center_y = boxes[:,0] + 0.5 * height
  center_x = boxes[:,1] + 0.5 * width
  # deltas ê°’ ì ìš©
  center_y += deltas[:,0] * height
  center_x += deltas[:,1] * width
  height *= torch.exp(deltas[:,2])
  width *= torch.exp(deltas[:,3])
  # ì ìš©í•œ ê°’ì„ bbox ì¢Œí‘œ y1,x1,y2,x2ë¡œ ë‹¤ì‹œ ë³€í™˜
  y1 = center_y - 0.5 * height
  x1 = center_x - 0.5 * width
  y2 = y1 + height
  x2 = x1 + width
  result = torch.stack([y1, x1, y2, x2], dim=1)
  return result

def clip_boxes(boxes, window):
  # boxes : [N,4] <= y1,x1,y2,x2
  # window : [4] <= y1,x1,y2,x2
  boxes = torch.stack([boxes[:,0].clamp(float(window[0]), float(window[2])),
                       boxes[:,1].clamp(float(window[1]), float(window[3])),
                       boxes[:,2].clamp(float(window[0]), float(window[2])),
                       boxes[:,3].clamp(float(window[1]), float(window[3]))],1)
  return boxes


def proposal_layer(inputs, proposal_count, nms_threshold, anchors, config=None):
  '''
  Inputs : 
        rpn_probs : [batch, anchors, (bg prob, fg prob)]
        rpn_bbox : [batch, anchors, (dy, dx, log(dh), log(dw))]
  
  Returns : Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]
  '''
  inputs[0] = inputs[0].squeeze(0)
  inputs[1] = inputs[1].squeeze(0)

  # Box scores. foreground class confidence ì‚¬ìš© 
  scores = inputs[0][:,1] # [batch, num_rois,1]  

  # Box deltas <= [batch, num_rois, 4]
  deltas = inputs[1]
  std_dev = Variable(torch.from_numpy(np.reshape(config.RPN_BBOX_STD_DEV, [1,4])).float(), requires_grad=False)
  if config.GPU_COUNT:
    std_dev = std_dev.cuda()
  deltas = deltas * std_dev

  # anchor scoreì— ë”°ë¼ ìƒìœ„ ëª‡ê°œì˜ anchorë§Œ ê°€ì§€ê³  ê°„ë‹¤.
  pre_nms_list = min(6000, anchors.size()[0])
  scores, order = scores.sort(descending = True)
  order = order[:pre_nms_list]
  scores = scores[:pre_nms_list]
  deltas = deltas[order.data, :]
  anchors = anchors[order.data, :]

  # deltasë¥¼ anchorì— ì ìš©í•´ì„œ refined anchorë¥¼ ì–»ëŠ”ë‹¤.
  # [batch, N, (y1, x1, y2, x2)]
  boxes = apply_box_deltas(anchors,deltas)

  # image ê²½ê³„ì— ë§ê²Œ anchorë¥¼ clip í•œë‹¤. 
  # [batch, N, (y1, x1, y2, x2)]
  height, width = config.IMAGE_SHAPE[:2]
  window = np.array([0,0,height,width]).astype(np.float32)
  boxes = clip_boxes(boxes, window)

  # ì‘ì€ bboxë“¤ì„ filter outí•´ì•¼í•˜ì§€ë§Œ, Xinlei Chenì˜ ë…¼ë¬¸ì— ì˜í•˜ë©´
  # filteringí•˜ëŠ” ê²ƒì´ ì‘ì€ ê°ì²´ íƒì§€ì— ì•…ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë–„ë¬¸ì— ìƒëµí•œë‹¤.

  # Non-maximum suppression
  keep = nms(torch.cat((boxes, scores.unsqueeze(1)),1).data, nms_threshold)
  keep = keep[:proposal_count]
  boxes = boxes[keep, :]

  # boxesì˜ ê° ì°¨ì›ì˜ ê°’ì„ 0~1ì‚¬ì´ë¡œ ì •ê·œí™”í•œë‹¤.
  norm = Variable(torch.from_numpy(np.array([height,width,height,width])).float(), requires_grad=False)
  if config.GPU_COUNT:
    norm = norm.cuda()
  normalized_boxes = boxes/norm

  # ë‹¤ì‹œ batch ì°¨ì›ì„ ì¶”ê°€í•´ì¤€ë‹¤.
  normalized_boxes = normalized_boxes.unsqueeze(0)

  return normalized_boxes
```

#### 3.2 RoI align layer

Mask R-CNN ëª¨ë¸ì—ì„œëŠ” ê³ ì •ëœ í¬ê¸°ì˜ feature mapì„ ì–»ê¸° ìœ„í•´ RoI pooling ëŒ€ì‹  RoI alignì„ ì‚¬ìš©í•œë‹¤. RoI alignì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [<U>Mask R-CNN ë…¼ë¬¸ ë¦¬ë·°</U>](https://hamin-chang.github.io/cv-objectdetection/maskrcnn/)ë¥¼ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤. 

Params : 
  - pool_size : ì¶œë ¥ pooled regionì˜ [height, width]. ë³´í†µ [7, 7] ì‚¬ì´ì¦ˆ
  - image_shape : ì…ë ¥ ì´ë¯¸ì§€ì˜ [height, width, channels]

Inputs :
  - boxes : ì •ê·œí™”ëœ ì¢Œí‘œì˜ [batch, num_boxes, (y1,x1,y2,x2)]
  - Feature maps : FPNì—ì„œ ì¶”ì¶œí•œ feature map pyramids, [batch, channels, height, width]

Outputs :
  - [num_boxes, height, width, channels]ì˜ shapeì˜ Pooled regions. heightê³¼ widthëŠ” pool_sizeë¡œ ê³ ì •

ê·¸ë¦¬ê³  ê° RoIë¥¼ ì ì ˆí•œ levelì— í• ë‹¹í•´ì•¼í•œë‹¤. ì´ëŠ” ë‹¤ìŒì˜ ê³µì‹ì„ ì´ìš©í•´ì„œ ì§„í–‰í•œë‹¤.

![1](https://user-images.githubusercontent.com/77332628/220282131-3a1f1158-e2a1-4db1-8b14-f8589349f0cf.png)




```python
def pyramid_roi_align(inputs, pool_size, image_shape):

  for i in range(len(inputs)):
    inputs[i] = inputs[i].unsqueeze(0)

  # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords
  boxes = inputs[0]

  # ê°ê¸° ë‹¤ë¥¸ levelì˜ feature map pyramids, [batch, height, width, channels]
  feature_maps = inputs[1:] 

  # RoIë¥¼ ì ì ˆí•œ pyramid levelì— í• ë‹¹
  y1, x1, y2, x2 = boxes.chunk(4, dim=1)
  h = y2 - y1
  w = x2 - x1

  # ìœ„ ì´ë¯¸ì§€ì˜ ê³µì‹ì„ ì´ìš©
  image_area = Variable(torch.FloatTensor([float(image_shape[0]*image_shape[1])]), requires_grad=False)
  if boxes.is_cuda:
    image_area = image_area.cuda()
  roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))
  roi_level = roi_level.round().int()
  roi_level = roi_level.clamp(2,5) # pyramid level P2~P5

  # ì´ì œ ê° pyramid level(P2~P5)ì—ì„œ RoIalign ìˆ˜í–‰
  pooled = []
  box_to_level = []
  for i, level in enumerate(range(2,6)):
    ix = roi_level == level
    if not ix.any():
      continue
    ix = torch.nonzero(ix)[:,0]
    level_boxes = boxes[ix.data, :]

    # ì–´ë–¤ boxê°€ ì–´ë–¤ levelì— mapping ë˜ì—ˆëŠ”ì§€ ì¶”ì 
    box_to_level.append(ix.data)

    # RoIì— ëŒ€í•œ gradient propagationëŠ” ë©ˆì¶˜ë‹¤.
    level_boxes = level.boxes.detach()

    # Crop and Resizeë¥¼ ìˆ˜í–‰í•´ì„œ RoI alignì„ êµ¬í˜„
    ind = Variable(torch.zeros(level_boxes.size()[0]), requires_grad=False).int()
    if level_boxes.is_cuda():
      ind = ind.cuda()
    feature_maps[i] = feature_maps[i].unsqueeze(0) # CropAndResizeFunctionê°€ batch ì°¨ì› í•„ìš”ë¡œ í•¨
    pooled_features = CropAndResizeFunction(pool_size, pool_size, 0)(feature_maps[i], level_boxes, ind)
    pooled.append(pooled_features)

  # ëª¨ë“  levelì˜ pooled featureë¥¼ í•©ì¹œë‹¤.
  pooled = torch.cat(pooled, dim=0)

  # box_to_levelë„ ëª¨ë“  levelì„ í•˜ë‚˜ë¡œ í•©ì¹œë‹¤.
  box_to_level = torch.cat(box_to_level, dim=0)

  # pooled featureê³¼ original boxë¥¼ ë§ì¶°ì„œ ë‹¤ì‹œ ì¬ë°°ì—´í•œë‹¤.
  _, box_to_level = torch.sort(box_to_level)
  pooled = pooled[box_to_level, :, :]
  
  return pooled
```

#### 3.3 Detection target layer 

Detection target layerëŠ” Fast RCNNì„ í•™ìŠµì‹œí‚¤ê¸° ìœ ìš©í•œ proposalsì„ subsampleí•˜ëŠ”ë°, ì´ëŠ” ground truth ê°’ê³¼ì˜ IoUë¥¼ ê³„ì‚°í•´ì„œ ê¸°ì¤€ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ëª‡ê°œì˜ proposalsë§Œ ì‚¬ìš©í•œë‹¤.




                   



```python
# ë‘ê°œì˜ boxì˜ IoUë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
# boxes1, boxes2 : [N, (y1,x1,y2,x2)]
def bbox_overlaps(boxes1, boxes2):
  # ë¨¼ì € ëª¨ë“  boxes1ì™€ boxes2ë¥¼ loop ì—†ì´ ë¹„êµí•˜ê¸° ìœ„í•´
  # tf.tile()ê³¼ ê°™ì€ ê¸°ëŠ¥ì¸ boxes1ì™€ boxes2 ê°ê°ì˜ ì¢Œí‘œë“¤ì„ Nê°œ ë§Œí¼ ë°˜ë³µí•´ì„œ ì´ì–´ë¶™ì¸ë‹¤.
  boxes1_repeat = boxes2.size()[0]
  boxes2_repeat = boxes1.size()[0]
  boxes1 = boxes1.repeat(boxes1_repeat, 1).view(-1,4)
  boxes2 = boxes2.repeat(boxes2_repeat, 1)

  # ì´ì œ intersectionì„ ê³„ì‚°í•œë‹¤.
  b1_y1, b1_x1, b1_y2, b1_x2 = boxes1.chunk(4, dim=1)
  b2_y1, b2_x1, b2_y2, b2_x2 = boxes2.chunk(4, dim=1)
  y1 = torch.max(b1_y1, b2_y1)[:,0]
  x1 = torch.max(b1_x1, b2_x2)[:,0]
  y2 = torch.min(b1_y2, b2_y2)[:,0]
  x2 = torch.min(b1_x2, b2_x2)[:,0]
  zeros = Variable(torch.zeros(y1.size()[0]), requires_grad=False)
  if y1.is_cuda:
    zeros = zeros.cuda()
  intersection = torch.max(x2-x1, zeros) * torch.max(y2-y1,zeros)

  # ë‘ boxì˜ union ì˜ì—­ì„ ê³„ì‚°í•œë‹¤.
  b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)
  b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)
  union = b1_area[:,0] + b2_area[:,0] - intersection

  # ë§ˆì§€ë§‰ìœ¼ë¡œ IoUë¥¼ ê³„ì‚°í•˜ê³  [boxes1, boxes2]ë¡œ reshape
  iou = intersection/union
  overlaps = iou.view(boxes2_repeat, boxes1_repeat)

  return overlaps
  
```

ê·¸ ë‹¤ìŒìœ¼ë¡œ Detection target layerë¥¼ êµ¬í˜„í•˜ëŠ”ë°, Detection  target layerì˜ ì…ë ¥ê°’ê³¼ ë°˜í™˜ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

Input :
  - proposals : ì •ê·œí™”ëœ ì¢Œí‘œì˜  [batch, N, (y1, x1, y2, x2)]ë¡œ ë§Œì•½ proposalì´ ë¶€ì¡±í•˜ë©´ zero paddingì´ ìˆ˜í–‰ë  ìˆ˜ ìˆë‹¤.
  - gt_class_id : [batch, MAX_GT_INSTANCES] Integer class ID.
  - gt_boxes: ì •ê·œí™”ëœ ì¢Œí‘œì˜ [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
  - gt_masks : boolean typeì˜ [batch, height, width, MAX_GT_INSTANCES]

Returns : 
  - (target) rois : ì •ê·œí™”ëœ ì¢Œí‘œì˜ [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)]
  - target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class ID.
  - target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (dy, dx, log(dh), log(dw), class_id)] bbox ìˆ˜ì • delta ê°’.
  - target_mask : [batch, TRAIN_ROIS_PER_IMAGE, height, width] 


```python
def detection_target_layer(proposals, gt_class_ids, gt_boxes, gt_masks, config):
  proposals = proposals.squeeze(0)
  gt_class_ids = gt_class_ids.squeeze(0)
  gt_boxes = gt_boxes.squeeze(0)
  gt_masks = gt_masks.squeeze(0)

  # COCO crowd boxëŠ” í›ˆë ¨ ë°ì´í„°ì—ì„œ ì œì™¸í•œë‹¤. COCO crowd boxëŠ” negative class IDë¥¼ ê°€ì§€ê³  ìˆë‹¤.
  # (ì´ ê³¼ì •ì„ ì§„í–‰í•˜ëŠ” ì´ìœ ëŠ” ëª¨ë¥´ê² ì§€ë§Œ, ì•„ë§ˆ positive class IDë§Œ ì‚¬ìš©í•˜ë ¤ëŠ”ê²Œ ì•„ë‹Œê°€ ì‹¶ë‹¤.)
  if torch.nonzero(gt_class_ids<0).size():
    crowd_ix = torch.nonzero(gt_class_ids < 0)[:,0]
    non_crowd_ix = torch.nonzero(gt_class_ids > 0)[:,0]
    crowd_boxes = gt_boxes[crowd_ix.data, :]
    crowd_masks = gt_masks[crowd_ix.data, :, :]
    gt_class_ids = gt_class_ids[non_crowd_ix.data]
    gt_boxes = gt_boxes[non_crowd_ix.data, :]
    gt_masks = gt_masks[non_crowd_ix.data, :,:]

    # propsalsì™€ crowd boxesì˜ overlap ê³„ì‚°
    crowd_overlaps = bbox_overlaps(proposals, crowd_boxes)
    crowd_iou_max = torch.max(crowd_overlaps, dim=1)[0]
    no_crowd_bool = crowd_iou_max < 0.001
  
  else: 
    no_crowd_bool = Variable(torch.ByteTensor(proposals.size()[0] * [True]), requires_grad=False)
    if config.GPU_COUNT:
      no_crowd_bool = no_crowd_bool.cuda()

  # proposalsì™€ gt_boxes overlap ê³„ì‚°í•˜ê³   positive/negative RoIs ê²°ì •
  overlaps = bbox_overlaps(proposals, gt_boxes)
  roi_iou_max = torch.max(overlaps, dim=1)[0]

  # positive RoIì˜ ê¸°ì¤€ì€ iou >= 0.5
  positive_roi_bool = roi_iou_max >= 0.5

  # Positive RoIs
  if torch.nonzero(positive_roi_bool).size():
    positive_indices = torch.nonzero(positive_roi_bool)[:,0]
    positive_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO) # positive ë¹„ìœ¨ì´ 33% ë˜ë„ë¡
    rand_idx = torch.randperm(positive_indices.size()[0])
    rand_idx = rand_idx[:positive_count]
    if config.GPU_COUNT:
      rand_idx = rand_idx.cuda()
    positive_indices = positive_indices[rand_idx]
    positive_count = positive_indices.size()[0]
    positive_rois = proposals[positive_indices.data, :]

    # positive RoIsë¥¼ GT boxesì— ë§¤ì¹˜ ì‹œí‚¨ë‹¤.
    positive_overlaps = overlaps[positive_indices.data, :]
    roi_gt_box_assignment = torch.max(positive_overlaps, dim=1)[1]
    roi_gt_boxes = gt_boxes[roi_gt_box_assignment.data,:]
    roi_gt_class_ids = gt_class_ids[roi_gt_box_assignment.data]

    # positive RoIsì— ëŒ€í•œ bbox refinement delta ê°’ ê³„ì‚°
    deltas = Variable(utils.box_refinement(positive_rois.data, roi_gt_boxes.data), requires_grad=False)
    std_dev = Variable(torch.from_numpy(config.BBOX_STD_DEV).float(), required_grad=False)
    if config.GPU_COUNT:
      std_dev = std_dev.cuda()
    deltas /= std_dev # ì •ê·œí™”

    # positive RoIsì— GT maskë¥¼ ë§¤ì¹˜ì‹œí‚¨ë‹¤.
    roi_masks = gt_masks[roi_gt_box_assignment.data,:,:]

    # mask target ê³„ì‚°
    boxes = positive_rois
    if config.USE_MINI_MASK:
      # RoI ì¢Œí‘œë¥¼ normalized image spaceì—ì„œ normalized mini-mask spaceë¡œ ë³€í™˜
      y1, x1, y2, x2 = positive_rois.chunk(4, dim=1)
      gt_y1, gt_x1, gt_y2, gt_x2 = roi_gt_boxes.chunk(4, dim=1)
      gt_h = gt_y2 - gt_y1
      gt_w = gt_y2 - gt_y1
      y1 = (y1 - gt_y1) / gt_h
      x1 = (x1 - gt_x1) / gt_w
      y2 = (y2 - gt_y1) / gt_h
      y2 = (y2 - gt_y1) / gt_h
      boxes = torch.cat([y1, x1, y2, x2], dim=1)
    box_ids = Variable(torch.arange(roi_masks.size()[0]), requires_grad=False).int()
    if config.GPU_COUNT:
      box_ids = box_ids.cuda()
    masks = Variable(CropAndResizeFunction(config.MASK_SHAPE[0], config.MASK_SHAPE[1],0)\
                     (roi_masks.unsqueeze(1), boxes, box_ids).data, requires_grad=False)
    masks = masks.squeeze(1)

    # binary cross entropy lossë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ 0.5 ì´ìƒì˜ mask pixel ê°€ì§€ëŠ” ê²ƒë“¤ì€ ë°˜ì˜¬ë¦¼.
    mask = torch.round(masks)
  else:
    positive_count = 0

  # Negative RoIsëŠ” GT boxì™€ì˜ iou < 0.5ì¸ proposalsì´ë‹¤. COCO crowdsëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
  negative_roi_bool = roi_iou_max < 0.5
  negative_roi_bool = negative_roi_bool & no_crowd_bool
  # positive/negative ë¹„ìœ¨ì´ ìœ ì§€ë˜ë„ë¡ ì¶©ë¶„í•œ Negative RoIsë¥¼ ì¶”ê°€í•´ì¤€ë‹¤.
  if torch.nonzero(negative_roi_bool).size() and positive_count > 0:
    negative_indices = torch.nonzero(negative_roi_bool)[:,0]
    r = 1.0 / config.ROI_POSITIVE_RATIO
    negative_count = int(r * positive_count - positive_count)
    rand_idx = torch.randperm(negative_indices.size()[0])
    rand_idx = rand_idx[:negative_count]
    if config.GPU_COUNT :
      radn_idx = rand_idx.cuda()
    negative_indices = negative_indices[rand_idx]
    negative_count = negative_indices.size()[0]
    negative_rois = proposals[negative_indices.data, :]
  else: 
    negative_count = 0
  
  # positive/negative RoIsë“¤ì„ í•©ì¹˜ê³ , negative RoIsì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” bbox delatê°’ê³¼ maskë¥¼ 0ìœ¼ë¡œ paddingí•œë‹¤.
  if positive_count > 0 and negative_count > 0 :
    rois = torch.cat((positive_rois, negative_rois), dim=0)
    zeros = Variable(torch.zeros(negative_count), requires_grad=False).int()
    roi_gt_class_ids = torch.cat([roi_gt_class_ids, zeros],dim=0)

    zeros = Variable(torch.zeros(negative_count, 4), requires_grad=False).int()
    deltas = torch.cat([deltas, zeros], dim=0)

    zeros = Variable(torch.zeros(negative_count, config.MASK_SHAPE[0], config.MASK_SHAPE[1]), requires_grad=False).int()
    masks = torch.cat([masks, zeros], dim=0)
  elif positive_count > 0:
    rois = positive_rois
  elif negative_count > 0:
    rois = negative_rois
    zeros = Variable(torch.zeros(negative_count), requires_grad=False)
    roi_gt_class_ids = zeros

    zeros = Variable(torch.zeros(negative_count,4), requires_grad=False).int()
    deltas = zeros

    zeros = Variable(torch.zeros(negative_count,config.MASK_SHAPE[0],config.MASK_SHAPE[1]), requires_grad=False)
    masks = zeros
  
  else:
    rois = Variable(torch.FloatTensor(), requires_grad=False)
    roi_gt_class_ids = Variable(torch.IntTensor(), requires_grad=False)
    deltas = Variable(torch.FloatTensor(), requires_grad=False)
    masks = Variable(torch.FloatTensor(), requires_grad=False)
  
  return rois, roi_gt_class_ids, deltas, masks
```

#### 3.4 Detection Layer
ìµœì¢… detection bboxë¥¼ ì¶œë ¥í•˜ëŠ” Detection layerë¥¼ êµ¬í˜„í•´ë³´ì.


```python
def clip_to_window(window, boxes):
  '''
  window : (y1, x1, y2, x2), The window in the image we want to clip to
  boxes : [N ,(y1, x1, y2, x2)]
  '''
  boxes[:, 0] = boxes[:, 0].clamp(float(window[0]), float(window[2]))
  boxes[:, 1] = boxes[:, 1].clamp(float(window[1]), float(window[3]))
  boxes[:, 2] = boxes[:, 2].clamp(float(window[0]), float(window[2]))
  boxes[:, 3] = boxes[:, 3].clamp(float(window[1]), float(window[3]))
   
  return boxes

def refine_detections(rois, probs, deltas, window, config):
  
  # Class IDs per RoI
  _, class_ids = torch.max(probs, dim=1)


  idx = torch.arange(class_ids.size()[0]).long()
  class_scores = probs[idx, class_ids.data]
  deltas_specific = deltas[idx, class_ids.data] # Class-specific bbox deltas

  # bbox deltaê°’ ì ìš©
  # Shape : [boxes, (y1, x1, y2, x2)] in normalized coordinates
  std_dev = Variable(torch.from_numpy(np.reshape(config.RPN_BBOX_STD_DEV, [1,4])).float(), requires_grad = False)
  refined_rois = apply_box_deltas(rois, deltas_specific * std_dev)

  # ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ì˜ ì˜ì—­ìœ¼ë¡œ ë³€í™˜
  height, width = config.IMAGE_SHAPE[:2]
  scale = Variable(torch.from_numpy(np.array([height, width, height, width])).float(), requires_grad=False)
  refined_rois *= scale

  # Clip boxes to image window
  refined_rois = clip_to_window(window, refined_rois)

  # í”½ì…€ ë‹¨ìœ„ë¥¼ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ì •ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
  refined_rois = torch.round(refined_rois)

  # ë°°ê²½ bbox Filter out
  keep_bool = class_ids > 0

  # ë‚®ì€ confidence scoreì˜ bbox Filter out
  if config.DETECTION_MIN_CONFIDENCE:
    keep_bool = keep_bool & (class_scores >= config.DETECTION_MIN_CONFIDENCE)
  keep = torch.nonzero(keep_bool)[:,0]

  # per-class NMS ì ìš©
  pre_nms_class_ids = class_ids[keep.data]
  pre_nms_scores = class_scores[keep.data]
  pre_nms_rois =refined_rois[keep.data]

  for i, class_id in enumerate(unique1d(pre_nms_class_ids)):
    # pick detections of this class
    ixs = torch.nonzero(pre_nms_class_ids == class_ids)[:,0]

    # Sort
    ix_rois = pre_nms_rois[ixs.data]
    ix_scores = pre_nms_scores[ixs]
    ix_scores, order = ix_scores.sort(descending=True)
    ix_rois = ix_rois[order.data, :]

    class_keep = nms(torch.cat((ix_rois, ix_scores.unsqueeze(1)), dim=1).data, config.DETECTION_NMS_THRESHOLD)

    class_keep = keep[ixs[order[class_keep].data].data]

    if i==0:
      nms_keep = class_keep
    else :
      nms_keep = unique1d(torch.cast((nms_keep, class_keep)))
  keep = intersect1d(keep, nms_keep)

  # Keep top detections
  roi_count = config.DETECTION_MAX_INSTANCES
  top_ids = class_scores[keep.data].sort(descending=True)[1][:roi_count]
  keep = keep[top_ids.data]
  
  # ì¶œë ¥ê°’ì„ [N, (y1, x1, y2, x2. class_id, score)]ì˜ í˜•íƒœë¡œ ë°°ì—´
  # ì¢Œí‘œê°’ì€ ì´ë¯¸ì§€ ì˜ì—­ì˜ ê°’
  result = torch.cat((refined_rois[keep.data], class_ids[keep.data].unsqueeze(1).float(),
                      class_scores[keep.data].unsqueeze(1)), dim=1)
  
  return result

def detection_layer(config, rois, mrcnn_class, mrcnn_bbox, image_meta):
  '''
  classified proposal boxesì™€ bbox deltaê°’ì„ ë°›ì•„ì„œ ìµœì¢… detection boxesë¥¼ ì¶œë ¥í•œë‹¤.
  '''

  rois = rois.squeeze(0)

  _, _, window , _ = parse_image_meta(image_meta)
  window = window[0]
  detections = refine_detections(rois, mrcnn_class, mrcnn_bbox, window, config)

  return detections
```

#### 3.5 Region Proposal Network (RPN)

ì…ë ¥ê°’ìœ¼ë¡œ

- anchors_per_location : feature mapì—ì„œ ê° pixelë§ˆë‹¤ anchorì˜ ê°œìˆ˜
- anchor_stride : anchor ìˆ˜ì˜ ë°€ë„ë¥¼ ì¡°ì •í•˜ëŠ” ê°’. (ë³´í†µ 1ë¡œ ì„¤ì •í•´ì„œ ë§¤ pixelë§ˆë‹¤ anchorê°€ ìˆê²Œ í•˜ê±°ë‚˜ 2ë¡œ í•´ì„œ pixelí•˜ë‚˜ ê±´ë„ˆë›°ì–´ì„œ anchorê°€ ìˆê²Œ ì„¤ì •í•œë‹¤.)

ë¥¼ ë°›ì•„ì„œ.

- rpn_logits : Anchor classifier logit (before softmax), [batch, H, W, 2]
- rpn_probs : Anchor classifier probabilites , [batch, H, W, 2]
- rpn_bbox : Anchorì— ì ìš©ë  deltas, [batch, H, W, (dy, dx, log(dh), log(dw))]

ë¥¼ ë°˜í™˜í•œë‹¤. ì°¸ê³ ë¡œ rpnì˜ probabilitesê°’ì´ 2ê°œì¸ ì´ìœ ëŠ” RPNì—ì„œëŠ” ê°ì²´ì˜ ìœ ë¬´ í™•ë¥ ë§Œ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤.



```python
class RPN(nn.Module):
  def __init__(self, anchors_per_location, anchor_stride, depth):
    super(RPN, self).__init__()
    self.anchors_per_location = anchors_per_location
    self.anchor_stride = anchor_stride
    self.depth = depth

    self.padding = SamePad2d(kernel_size=3, stride= self.anchor_stride)
    self.conv_shared = nn.Conv2d(self.depth, 512, kernel_size=3, stride=self.anchor_stride)
    self.relu = nn.ReLU(inplace=True)
    self.conv_class = nn.Conv2d(512, 2*anchors_per_location, kernel_size=1, stride=1)
    self.softmax = nn.Softmax(dim=2)
    self.conv_bbox = nn.Conv2d(512, 4 * anchors_per_location, kernel_size=1, stride=1)

  def forward(self, x):
    # shared convolutional base of the RPN
    x = self.relu(self.conv_shared(self.padding(x)))

    # Anchor score [batch, anchors_per_location * 2, height, width]
    rpn_class_logits = self.conv_class(x)

    # reshape to [batch, 2, anchors]
    rpn_class_logits = rpn_class_logits.permute(0,2,3,1)
    rpn_class_logits = rpn_class_logits.contiguous()
    rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)

    # softmax ì²˜ë¦¬
    rpn_probs = self.softmax(rpn_class_logits)

    # bbox refinement, [batch, H, W, anchors_per_location, depth]
    rpn_bbox = self.conv_bbox(x)

    # reshape to [batch, 4, anchors]
    rpn_bbox = rpn_bbox.permute(0,2,3,1)
    rpn_bbox = rpn_bbox.contiguous()
    rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)

    return [rpn_class_logits, rpn_probs, rpn_bbox]
    
```

#### 3.6 Classification, BBOX regressor, Mask segment Branch

ì´ì œ ìµœì¢… ê²°ê³¼ê°’ì„ ë„ì¶œí•˜ëŠ” Classificationê³¼ BBRì„ ìˆ˜í–‰í•˜ëŠ” classì™€ Mask segmentë¥¼ ìˆ˜í–‰í•˜ëŠ” classë¥¼ ì •ì˜í•œë‹¤.



```python
class Classifier(nn.Module):
  def __init__(self, depth, pool_size, image_shape, num_classes):
    super(Classifier, self).__init__()
    self.depth = depth
    self.pool_size = pool_size
    self.image_shape = image_shape
    self.num_classes = num_classes
    self.conv1 = nn.Conv2d(self.depth, 1024, kernel_size=self.pool_size, stride=1)
    self.bn1 = nn.BatchNorm2d(1024, eps=0.001, momentum=0.01)
    self.conv2 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)
    self.bn2 = nn.BatchNorm2d(1024, eps=0.001, momentum=0.01)
    self.relu = nn.ReLU(inplace=True)

    self.linear_class = nn.Linear(1024, num_classes)
    self.softmax = nn.Softmax(dim=1)

    self.linear_bbox = nn.Linear(1024, num_classes * 4)

  def forward(self, x, rois):
    x = pyramid_roi_align([rois]+x, self.pool_size, self.image_shape)
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)

    x = x.view(-1, 1024)

    # classification score
    mrcnn_class_logits = self.linear_class(x)
    mrcnn_probs = self.softmax(mrcnn_class_logits)

    # bbox regressor
    mrcnn_bbox = self.linear_bbox(x)
    mrcnn_bbox = mrcnn_bbox.view(mrcnn_bbox.size()[0], -1, 4)

    return [mrcnn_class_logits, mrcnn_probs, mrcnn_bbox]

class Mask(nn.Module):
  def __init__(self, depth, pool_size, image_shape, num_classes):
    super(Mask, self).__init__()
    self.depth = depth
    self.pool_size = pool_size
    self.image_shape = image_shape
    self.num_classes = num_classes
    self.padding = SamePad2d(kernel_size=3, stride=1)
    self.conv1 = nn.Conv2d(self.depth, 256, kernel_size=3, stride=1 )
    self.bn1 = nn.BatchNorm2d(256, eps=0.001)
    self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1)
    self.bn2 = nn.BatchNorm2d(256, eps=0.001)
    self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1)
    self.bn3 = nn.BatchNorm2d(256, eps=0.001)
    self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1)
    self.bn4 = nn.BatchNorm2d(256, eps=0.001)
    self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)
    self.conv5 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)
    self.sigmoid = nn.Sigmoid()
    self.relu = nn.ReLU(inplace=True)

  def forward(self, x , rois):
    x = pyramid_roi_align([rois]+x, self.pool_size, self.image_shape)
    x = self.conv1(self.padding(x))
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(self.padding(x))
    x = self.bn2(x)
    x = self.relu(x)
    x = self.conv3(self.padding(x))
    x = self.bn3(x)
    x = self.relu(x)
    x = self.conv4(self.padding(x))
    x = self.bn4(x)
    x = self.relu(x)
    x = self.deconv(x)
    x = self.relu(x)
    x = self.conv5(x)
    x = self.sigmoid(x)

    return x
```

ì´ê²ƒìœ¼ë¡œ Mask R-CNN ëª¨ë¸ì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì½”ë“œë¡œ ì§ì ‘ êµ¬í˜„í•´ë´¤ë‹¤. ì°¸ê³ í•œ repositoryì—ì„œëŠ” ì†ì‹¤í•¨ìˆ˜ì™€ ë°ì´í„° ì…‹ì„ ë‹¤ë£¨ëŠ” ì½”ë“œë„ ë‹¤ë£¨ì§€ë§Œ, ì´ë²ˆ ê¸€ì—ì„œëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì£¼ ëª©ì ì´ì—ˆê¸° ë•Œë¬¸ì— ì—¬ê¸°ê¹Œì§€ë§Œ ë‹¤ë£¨ê² ë‹¤. ì´ë²ˆ ì½”ë“œë¥¼ ê³µë¶€í•˜ë©´ì„œ ì°¸ê³ í•œ repositoryì˜ íŒŒì´ì¬, íŒŒì´í† ì¹˜ê°€ ì˜ˆì „ ë²„ì „ì´ê¸°ë„ í•˜ê³ , ë…¼ë¬¸ì—ì„œ ë°°ìš´ ê²ƒê³¼ ì›Œë”©ì´ ë‹¤ë¥¸ ê²ƒë“¤ë„ ìˆì–´ì„œ ì™„ë²½íˆ ì´í•´í•˜ì§€ëŠ” ëª»í–ˆì§€ë§Œ, ë¸”ë™ë°•ìŠ¤ ê°™ì•˜ë˜ ëª¨ë¸ì„ ì§ì ‘ ì½”ë“œë¡œ êµ¬í˜„í•´ë´¤ë‹¤ëŠ” ê²ƒì—ì„œ í° ì˜ë¯¸ê°€ ìˆë‹¤ê³  ìƒê°í•œë‹¤.

ë‹¤ìŒ ê¸€ì—ì„œëŠ” kaggleì—ì„œ ì‹¤ì œë¡œ Mask R-CNNìœ¼ë¡œ ê°ì²´ íƒì§€ë¥¼ ìˆ˜í–‰í•œ ê²ƒì„ ë‹¤ë¤„ë³´ê² ë‹¤.
